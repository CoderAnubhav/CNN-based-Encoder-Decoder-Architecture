{"cells":[{"cell_type":"markdown","metadata":{"id":"g9VYY53MMG-W"},"source":["\n","1.   **Horizontal Embedding  (batch_size,15,512)**\n","2.   **Image Embedding Finetuned during training**\n","3.   **Embedding Layer initialized with Normal distribution (finetuned during training)**\n","4.   **Embedding layer shifted inside main model because in trainer and in custom collate of dataloader it was not getting trained**\n","5.   **Embedding dim = 512**\n","6.   **The image embedding is concatenated with each word embedding**\n","7.   **The conv layers are not of equal dimensions anymore<br>\n","      first conv layer - (2x512->1024)<br>\n","      rest conv layers - (512->1024)**\n","8.   **By doing this the problem of residual connection for multiple  conv layers is resolved**\n","9. **Increasing the batch size to 10 because batch  size = 1 was leading to too much long training and higher batch sizes were leading to system crashes beacuse of ram overflow**\n","\n","****\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12905,"status":"ok","timestamp":1719813635726,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"hKHjTeMgvcgq","outputId":"3319bd4f-69b0-4bac-8ee0-534196f8e74f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/547.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/547.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m542.7/547.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"]}],"source":["!pip install datasets\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8677,"status":"ok","timestamp":1719813644385,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"i8XUmx2Vx_U7","outputId":"fd111e4e-4dac-410f-8c11-40067439501d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting positional-encodings\n","  Downloading positional_encodings-6.0.3-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from positional-encodings) (1.25.2)\n","Installing collected packages: positional-encodings\n","Successfully installed positional-encodings-6.0.3\n"]}],"source":["#package for positional encodings  github-> https://github.com/tatp22/multidim-positional-encoding\n","\n","!pip install positional-encodings"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Z7dekZWsw-yf","executionInfo":{"status":"ok","timestamp":1719813650849,"user_tz":-330,"elapsed":6490,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms as T\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader,Dataset\n","from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np\n","import math\n","import sys"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GUZpJE0Cw_g3","executionInfo":{"status":"ok","timestamp":1719813650849,"user_tz":-330,"elapsed":34,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1719813650850,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"JrwlD3t_JPBV","outputId":"a23b9434-8763-4347-a8ac-66028f2cba87"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]},{"output_type":"execute_result","data":{"text/plain":["['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"]},"metadata":{},"execution_count":5}],"source":["import torchtext\n","from torchtext.data import get_tokenizer\n","tokenizer = get_tokenizer(\"basic_english\")\n","tokens = tokenizer(\"You can now install TorchText using pip!\")\n","tokens"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1719813650850,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"W-P5AXaSyFG4","outputId":"6982d0a7-52ac-4f21-fdca-d01a75d67776"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["import torchtext as tt  # v0.9\n","import collections\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","toker = tt.data.utils.get_tokenizer(\"basic_english\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"P48ZZXlWP4V6","executionInfo":{"status":"ok","timestamp":1719813650850,"user_tz":-330,"elapsed":24,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["device = 'cpu'\n","if torch.cuda.is_available():  # take over whatever gpus are on the system\n","    device = torch.cuda.current_device()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1065,"status":"ok","timestamp":1719813651892,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"ck025EYGyOSm","outputId":"6e25509a-fd7c-41dd-e2e7-da2e2022d08e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[True, True, True, True, True, True, True, True, True, True],\n","         [True, True, True, True, True, True, True, True, True, True],\n","         [True, True, True, True, True, True, True, True, True, True],\n","         [True, True, True, True, True, True, True, True, True, True],\n","         [True, True, True, True, True, True, True, True, True, True],\n","         [True, True, True, True, True, True, True, True, True, True]]])\n","tensor([[[ 0.8978,  1.6674,  0.4369,  1.5310,  0.6636,  1.4759,  0.2497,\n","           1.8114,  0.8589,  1.7893],\n","         [ 1.7806,  1.5039,  0.9146,  1.0378,  0.3441,  1.1626,  0.3453,\n","           1.5274,  0.8561,  1.8458],\n","         [ 1.3889, -0.2808,  1.2052,  1.7904,  0.4677,  1.8966,  0.9021,\n","           1.5786,  0.0330,  1.8321],\n","         [ 0.3276, -0.7224,  0.9838,  1.2865,  0.3442,  1.2991,  0.3403,\n","           1.6190,  0.3557,  1.4113],\n","         [-0.4644,  0.3412,  1.5117,  0.8608,  0.1259,  1.1676,  0.6610,\n","           1.7779,  0.5159,  1.9044],\n","         [-0.8039,  1.1394,  1.6402,  1.2114,  0.7012,  1.2818,  0.7905,\n","           1.3245,  0.4436,  1.2634]]])\n"]}],"source":["# for positional encodings\n","\n","from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n","\n","# Returns the position encoding only\n","p_enc_1d_model = PositionalEncoding1D(10)\n","\n","# Return the inputs with the position encoding added\n","p_enc_1d_model_sum = Summer(PositionalEncoding1D(10))\n","\n","x = torch.rand(1,6,10)\n","penc_no_sum = p_enc_1d_model(x) # penc_no_sum.shape == (1, 6, 10)\n","penc_sum = p_enc_1d_model_sum(x)\n","print(penc_no_sum + x == penc_sum) # True\n","\n","print(penc_sum)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467,"referenced_widgets":["54c3e1507dfb437585f6b9bb185f264d","27d64080a6a0406a88cd0b93bd28087d","7d26abdba299455ab0c2d8dc9eb92e5d","e2360a52c68543a195280b5da9b73200","6f295d966e04429d9365c3fc519c7723","45aaa6b842f94385b5d3ebf9af38dc0e","e12b1bd5b7b845c2a380e384b67cdf39","6a051dbeba3f4d3a855abe1ded56c91e","55673a517a4a4d87af46164c02999bbd","cf08f5ab11ca419eb67eb592f0de69f7","4b77de891a314cada47563f129e6f8f5","e9e5ec6035d342c3a94d8a807fb98dae","653ffd5badab448aafef1948d95fb626","1736721f2a294f58bbe5073259bc057a","19cf98e3dc2b4fc889e413251da9e8c4","e5e5f024d981437e80376e634068bdf7","6d00857d23f844289232334fcfcd4726","c9f765a7839f4a93977c8fefe6964c88","98ccee25ecf74aa981b692620b263bdc","a3202cb5bbd948728f15b99abe572e15","33f8337e51b347a99a0f9be6bc5c5499","48481a825e83439aa3a4716357a6390d","123f1ae57ef74aa9a7d0e613781d64e9","ac9b9d2910a84e6fb76b80f53c42aca8","6982f6ba098149be9d66d11030e85ce1","698a9ff981164758bdd0c22565cdcc49","be3d00a555344caba16d05ee68318453","f7da6f2617b8454ab4a1f6a5b6c0edee","6ad13eed3ad54551a119502f9aff603d","f3eb3c459c5f4959a45197edd5598ae9","8072d3184e8d4bdd8b9cc9c27d4d91b9","d58e7af5613c495b8c46cc6c29465acb","e60aeb212ee64005af93cbc6de40d7de","e45f5832a27b48fa9b4356ed052066f0","39f1e6dc36fd4619bea38070d090b5f8","bcfd1ea33a5e41b39e8dd4d75799ff2a","5ff9b6879a3c48f1a4ad86e2a56608be","9230bb8090fe43fe988a8efee9017176","0af34994dadb4abcb36b6d2d332934a2","08b0df96d6eb4256a096feb99e953a14","264949875e554ec38dc217e7ebb359ce","764a094175764ba889c2af27d9032570","080f56a029d2415b84fd3f6a5ef34743","e43c0c784ef04a128d0a1db458a0fbf5","3654c115e84e47f8a83918fb6d1a9537","a100f487bcc14a53860db238b17edcb5","923988799bce4a4f9fc375b71b08f318","04ccedc6f8b64e4285b821f73645dd91","4b3956775892485a91eee8214a58ea9c","383e79bbb44643dab47ee0c6be929535","522ed2e25a514843b586409f55b6bde3","bf4a02ad17f04af4aaa94dea9b7619c9","37817c3a0408424293b8668c8eee222a","0b6341b492b14895a7a0ad1a0fb9c76c","7bb9d9a0a0524e6b80ad86d5d56511af","e9275a21b3124ccd9a82d5248c015748","112a72d73fa54c6999a4b757a3328626","7ea122e5638d455e8db88c44c6915370","d4a73b9fbda74563acbb86e7db7bb463","da47c5d7c6174cd3b61e4c75cdcb3d89","d2bc964a3dad4f1687b99746fc1796a9","ceb946485b594b678d0986f5d08a068e","108023bb4a7a44b2882078edd9a4b76e","9b991a2e947046c28e98cf3b43839826","8b3ec580cc0f43db8955663d6ce26a95","9fdc3f9fa5e548d9bb45426d3f93c389","d8b6743f9215483cbbdf2421867e95e8","495cc8efc5a24083bff1608c29f8665a","18673a64563e42769959a693b6dde4b9","7300d50e78914565b5098612074e3dfb","4518a2197d2144978b376dc7041d42c5","e7f4047a01da4ad88f03c8d07b4ba2ed","993945a097ab40e482f9dc6ff821a82d","6befc9b2d534403f822f4668dc8166e0","6ffb9e1cffa948799f9042bec7e5756f","cae388041b5d490682e603589fc888a1","334a3b95c852403ca97c88c9c3d7056c","a8f0273e015f4abb96c4bb5653514023","d4b64095aa7d44038b304e9a116ee093","f7d874b62f6c4bc390807b80001715c2","e9007caf9f50436686fbd4754dc56c87","a1cdcdf6502d4247b55533845adc83a2","eeac42398fb64a6b9a3e957f39e2fed2","fa55347d18de494b8054603968270163","8a42f894e67947d9a203dc17346c8ff4","275864e232a9491484971be25a7efa94","394f4f63399349a8ad80dcf71e5bfdd4","7fa5674d2aa14037bd881048a80f535e"]},"id":"fiM71zTlyHxJ","outputId":"4bcd196e-d17d-4b41-ee53-6bf87b5b0a17","executionInfo":{"status":"ok","timestamp":1719814399651,"user_tz":-330,"elapsed":747766,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/9.47k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c3e1507dfb437585f6b9bb185f264d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e5ec6035d342c3a94d8a807fb98dae"}},"metadata":{}},{"name":"stdout","output_type":"stream","text":["The repository for HuggingFaceM4/COCO contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/COCO.\n","You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n","\n","Do you wish to run the custom code? [y/N] y\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"123f1ae57ef74aa9a7d0e613781d64e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45f5832a27b48fa9b4356ed052066f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/6.65G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3654c115e84e47f8a83918fb6d1a9537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9275a21b3124ccd9a82d5248c015748"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b6743f9215483cbbdf2421867e95e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f0273e015f4abb96c4bb5653514023"}},"metadata":{}}],"source":["from datasets import load_dataset\n","\n","train_dataset = load_dataset(\"HuggingFaceM4/COCO\",split='train')\n","test_dataset =  load_dataset(\"HuggingFaceM4/COCO\",split='test')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeGGcN8wyazH","outputId":"ccc4e2c6-7d1b-4740-d6ed-0b3f1c7ebada","executionInfo":{"status":"ok","timestamp":1719814399652,"user_tz":-330,"elapsed":12,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["500\n"]}],"source":["sliced_train_dataset =  train_dataset.select(range(500))\n","print(len(sliced_train_dataset))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9W01CLzyc7C","outputId":"96daffe9-b99e-4d25-cb78-5d5186370428","executionInfo":{"status":"ok","timestamp":1719814558758,"user_tz":-330,"elapsed":159114,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-01 06:10:05--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2024-07-01 06:10:05--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2024-07-01 06:10:05--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n","\n","2024-07-01 06:12:44 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHoo1N4Fyezf","outputId":"8d9ae319-c6b0-44aa-ae4a-6f182fe2fba1","executionInfo":{"status":"ok","timestamp":1719814590429,"user_tz":-330,"elapsed":31681,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"]}],"source":["!unzip glove.6B.zip"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIwMnQEsndqX","outputId":"99c804d5-abaf-40c4-f803-ba23e9d6c97d","executionInfo":{"status":"ok","timestamp":1719814601421,"user_tz":-330,"elapsed":11025,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:07<00:00, 71.2MB/s]\n"]}],"source":["#VGG embeddings which will be finetuned later ----> from convcap vggfeats.py\n","pretrained_model = models.vgg16(pretrained=True)\n","\n","class Vgg16Feats(nn.Module):\n","\n","    def __init__(self):\n","        super(Vgg16Feats, self).__init__()\n","        self.features_nopool = nn.Sequential(*list(pretrained_model.features.children())[:-1])\n","        #print(self.features_nopool)\n","        self.features_pool = list(pretrained_model.features.children())[-1]\n","        #print(f\"self.features_pool : {self.features_pool}\")\n","        self.classifier = nn.Sequential(*list(pretrained_model.classifier.children())[:-1])\n","        #print(f\"self.classifier : {self.classifier}\")\n","\n","        #self.embed = nn.Linear(input_size, embed_size)   # for turning down the feature vector to embedding dimensions ... transfering it to  the trainer\n","\n","    def forward(self, x):                                             # y is 4096 dimensional vgg embedding\n","        x = self.features_nopool(x)\n","        x_pool = self.features_pool(x)\n","        x_feat = x_pool.view(x_pool.size(0), -1)\n","        y = self.classifier(x_feat)\n","        #y = self.embed(y)\n","\n","        #print(f\"type  of y: {type(y)}\")\n","        return  y"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVx3ImrjygmC","outputId":"f2fd9f6e-a04c-4b37-d986-8dcf87ce2227","executionInfo":{"status":"ok","timestamp":1719814699160,"user_tz":-330,"elapsed":2992,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0 a\n","1 .\n","2 of\n","3 in\n","4 with\n","5 and\n","6 the\n","7 on\n","8 bathroom\n","9 is\n","10 to\n","11 toilet\n","12 kitchen\n","13 white\n","14 street\n","15 ,\n","16 man\n","17 sink\n","18 sitting\n","19 an\n","20 next\n","21 shower\n","22 large\n","23 small\n","24 motorcycle\n","25 two\n","26 people\n","27 front\n","28 room\n","29 parked\n","30 building\n","31 are\n","32 mirror\n","33 riding\n","34 at\n","35 motorcycles\n","36 standing\n","37 up\n","38 has\n","39 wall\n","40 woman\n","41 bike\n","42 city\n","43 picture\n","44 some\n","45 that\n","46 cars\n","47 down\n","48 it\n","49 computer\n","50 this\n","51 his\n","52 person\n","53 skateboard\n","54 top\n","55 view\n","56 bicycle\n","57 blue\n","58 desk\n","59 road\n","60 traffic\n","61 car\n","62 horses\n","63 there\n","64 tub\n","65 very\n","66 area\n","67 bowl\n","68 by\n","69 laptop\n","70 looking\n","71 lot\n","72 many\n","73 other\n","74 table\n","75 bathtub\n","76 cat\n","77 motor\n","78 near\n","79 office\n","80 over\n","81 side\n","82 wearing\n","83 working\n","84 black\n","85 boy\n","86 driving\n","87 floor\n","88 food\n","89 full\n","90 green\n","91 group\n","92 men\n","93 orange\n","94 red\n","95 several\n","96 sinks\n","97 young\n","98 appliances\n","99 bunch\n","100 cabinets\n","101 close\n","102 computers\n","103 curb\n","104 door\n","105 filled\n","106 old\n","107 park\n","108 parking\n","109 police\n","110 row\n","111 stove\n","112 three\n","113 under\n","114 walk\n","115 as\n","116 back\n","117 bananas\n","118 bikes\n","119 bus\n","120 busy\n","121 clock\n","122 cooking\n","123 corner\n","124 curtain\n","125 display\n","126 eating\n","127 glass\n","128 holding\n","129 house\n","130 image\n","131 into\n","132 metal\n","133 officer\n","134 outside\n","135 plant\n","136 sidewalk\n","137 sits\n","138 taking\n","139 toilets\n","140 vanity\n","141 window\n","142 '\n","143 all\n","144 along\n","145 around\n","146 baby\n","147 bear\n","148 behind\n","149 being\n","150 beside\n","151 boat\n","152 brown\n","153 cake\n","154 carriage\n","155 closet\n","156 dining\n","157 dogs\n","158 doors\n","159 elephant\n","160 field\n","161 for\n","162 grass\n","163 hanging\n","164 her\n","165 home\n","166 inside\n","167 monitor\n","168 one\n","169 pan\n","170 pitcher\n","171 pot\n","172 s\n","173 shot\n","174 teddy\n","175 tower\n","176 trees\n","177 truck\n","178 urinal\n","179 vegetables\n","180 walls\n","181 while\n","182 wood\n","183 wooden\n","184 bath\n","185 beige\n","186 big\n","187 cabinet\n","188 cinnamon\n","189 counter\n","190 crosses\n","191 cutting\n","192 dark\n","193 donut\n","194 drawn\n","195 from\n","196 grazing\n","197 ground\n","198 head\n","199 highway\n","200 homeless\n","201 horse\n","202 intersection\n","203 lights\n","204 lined\n","205 little\n","206 locked\n","207 long\n","208 lots\n","209 microwave\n","210 middle\n","211 mirrors\n","212 open\n","213 oven\n","214 plate\n","215 reflection\n","216 scooters\n","217 shiny\n","218 shown\n","219 slices\n","220 space\n","221 stall\n","222 stand\n","223 sun\n","224 tile\n","225 together\n","226 trick\n","227 using\n","228 various\n","229 walking\n","230 wheel\n","231 above\n","232 airplane\n","233 antenna\n","234 background\n","235 been\n","236 bicycles\n","237 bikers\n","238 brick\n","239 broken\n","240 bucket\n","241 buckets\n","242 buildings\n","243 can\n","244 carrying\n","245 cart\n","246 chef\n","247 child\n","248 church\n","249 clean\n","250 cluttered\n","251 commercial\n","252 cooks\n","253 couple\n","254 decorated\n","255 dirty\n","256 doing\n","257 fixtures\n","258 guy\n","259 headphones\n","260 high\n","261 him\n","262 kid\n","263 laying\n","264 license\n","265 line\n","266 modern\n","267 multiple\n","268 no\n","269 off\n","270 opens\n","271 out\n","272 pair\n","273 pans\n","274 pigeon\n","275 potted\n","276 professional\n","277 put\n","278 ready\n","279 refrigerator\n","280 restroom\n","281 riders\n","282 rides\n","283 rows\n","284 scene\n","285 screen\n","286 seat\n","287 shop\n","288 shopping\n","289 silver\n","290 skate\n","291 stainless\n","292 steel\n","293 stone\n","294 sugar\n","295 television\n","296 through\n","297 tiled\n","298 tongs\n","299 track\n","300 trash\n","301 use\n","302 water\n","303 wheels\n","304 air\n","305 approaching\n","306 asia\n","307 backpack\n","308 bag\n","309 bedroom\n","310 beer\n","311 beneath\n","312 bottle\n","313 bowls\n","314 bundles\n","315 center\n","316 chained\n","317 chefs\n","318 close-up\n","319 closed\n","320 coming\n","321 commode\n","322 cop\n","323 counters\n","324 crowded\n","325 cup\n","326 different\n","327 dim\n","328 dish\n","329 displayed\n","330 double\n","331 dust\n","332 each\n","333 equipment\n","334 few\n","335 flat\n","336 flies\n","337 flowers\n","338 flying\n","339 fruit\n","340 girl\n","341 gray\n","342 hallway\n","343 hand\n","344 he\n","345 heavy\n","346 hiding\n","347 interior\n","348 items\n","349 jumps\n","350 keyboard\n","351 lap\n","352 lawn\n","353 left\n","354 lid\n","355 like\n","356 listening\n","357 living\n","358 loaded\n","359 low\n","360 lying\n","361 made\n","362 medicine\n","363 mop\n","364 mopeds\n","365 motorbikes\n","366 net\n","367 new\n","368 nice\n","369 oatmeal\n","370 object\n","371 older\n","372 onto\n","373 paper\n","374 pass\n","375 passing\n","376 past\n","377 pavement\n","378 performing\n","379 performs\n","380 pickup\n","381 piece\n","382 plumbing\n","383 prepare\n","384 preparing\n","385 printer\n","386 race\n","387 restaurant\n","388 river\n","389 section\n","390 see\n","391 sheet\n","392 shelf\n","393 shows\n","394 skateboarder\n","395 sky\n","396 snow\n","397 something\n","398 stares\n","399 stopped\n","400 store\n","401 style\n","402 takes\n","403 tall\n","404 tan\n","405 their\n","406 ticket\n","407 tiles\n","408 toliet\n","409 tops\n","410 trailer\n","411 tv\n","412 vehicle\n","413 vehicles\n","414 walk-in\n","415 walled\n","416 way\n","417 windows\n","418 wires\n","419 women\n","420 works\n","421 writing\n","422 yellow\n","423 aboard\n","424 about\n","425 across\n","426 additional\n","427 adjoining\n","428 adjust\n","429 advanced\n","430 against\n","431 ahead\n","432 amid\n","433 amount\n","434 another\n","435 apartment\n","436 apron\n","437 aprons\n","438 assortment\n","439 athletes\n","440 attached\n","441 attending\n","442 awaits\n","443 baker\n","444 bakeware\n","445 banana\n","446 bare\n","447 base\n","448 beautiful\n","449 bedding\n","450 begging\n","451 begs\n","452 beverage\n","453 bicyclists\n","454 bin\n","455 bird\n","456 blur\n","457 blurry\n","458 bmx\n","459 board\n","460 book\n","461 bottles\n","462 brand\n","463 break\n","464 brightly\n","465 broom\n","466 brush\n","467 budget\n","468 bunches\n","469 cabinetry\n","470 cabs\n","471 camera\n","472 cargo\n","473 carpeted\n","474 carries\n","475 cash\n","476 cell\n","477 cement\n","478 cereal\n","479 children\n","480 cleaned\n","481 cleaning\n","482 clear\n","483 cold\n","484 colored\n","485 colorful\n","486 columns\n","487 combination\n","488 comes\n","489 commercial-style\n","490 compact\n","491 consisting\n","492 construction\n","493 container\n","494 containing\n","495 contains\n","496 control\n","497 controls\n","498 cops\n","499 country\n","500 covered\n","501 coworker\n","502 cramped\n","503 creek\n","504 crib\n","505 crowd\n","506 currently\n","507 customer\n","508 cycle\n","509 decal\n","510 decor\n","511 decoration\n","512 decorative\n","513 depicting\n","514 desert\n","515 designed\n","516 diamond\n","517 dinner\n","518 direction\n","519 dirt\n","520 discarded\n","521 dough\n","522 driveway\n","523 dustpan\n","524 dwellers\n","525 earphones\n","526 eat\n","527 eats\n","528 efficiency\n","529 electric\n","530 employee\n","531 empty\n","532 enclosed\n","533 end\n","534 estate\n","535 evening\n","536 examines\n","537 external\n","538 extra\n","539 falls\n","540 faucet\n","541 features\n","542 fender\n","543 filed\n","544 fixture\n","545 flipping\n","546 floored\n","547 flooring\n","548 floors\n","549 flower\n","550 fluffy\n","551 focal\n","552 forest\n","553 forks\n","554 fountain\n","555 framed\n","556 fresh\n","557 furnishings\n","558 furniture\n","559 gathered\n","560 geoup\n","561 glazed\n","562 gloves\n","563 glow\n","564 going\n","565 grabbing\n","566 granite\n","567 graze\n","568 great\n","569 greets\n","570 grouped\n","571 guests\n","572 hair\n","573 heading\n","574 hedge\n","575 held\n","576 helmets\n","577 helping\n","578 himself\n","579 holder\n","580 holds\n","581 hope\n","582 huge\n","583 i\n","584 images\n","585 includes\n","586 including\n","587 indicator\n","588 industrial\n","589 installed\n","590 intently\n","591 interesting\n","592 island\n","593 jacket\n","594 jacuzzi\n","595 janitors\n","596 jetliner\n","597 juice\n","598 just\n","599 kettle\n","600 kickflip\n","601 kitchenette\n","602 knife\n","603 lady\n","604 lamp\n","605 land\n","606 lanes\n","607 late\n","608 lays\n","609 leading\n","610 leaning\n","611 letter\n","612 lies\n","613 lighting\n","614 liquid\n","615 lit\n","616 looks\n","617 lush\n","618 luxurious\n","619 machines\n","620 major\n","621 makes\n","622 making\n","623 males\n","624 marking\n","625 mashed\n","626 material\n","627 may\n","628 mean\n","629 melting\n","630 messy\n","631 metallic\n","632 metro\n","633 milk\n","634 mirrored\n","635 mixed\n","636 mostly\n","637 motorbike\n","638 motorcyclists\n","639 motorists\n","640 motorized\n","641 mounted\n","642 mouse\n","643 mouth\n","644 mug\n","645 nearby\n","646 nearing\n","647 neck\n","648 neutrals\n","649 non\n","650 not\n","651 number\n","652 objects\n","653 one-way\n","654 opened\n","655 oranges\n","656 ordered\n","657 outdoors\n","658 overloaded\n","659 overlooking\n","660 padlocked\n","661 pale\n","662 palette\n","663 panel\n","664 parks\n","665 pasture\n","666 path\n","667 patterned\n","668 performance\n","669 perspective\n","670 phone\n","671 phones\n","672 photo\n","673 photograph\n","674 photos\n","675 pick\n","676 picking\n","677 pies\n","678 pile\n","679 pizzas\n","680 placed\n","681 plain\n","682 plane\n","683 planters\n","684 plaque\n","685 plaster\n","686 plates\n","687 playground\n","688 playing\n","689 plays\n","690 point\n","691 polie\n","692 porcelain\n","693 posed\n","694 position\n","695 possibly\n","696 post\n","697 pots\n","698 printers\n","699 process\n","700 public\n","701 purple\n","702 racer\n","703 racetrack\n","704 reading\n","705 red-light\n","706 religious\n","707 remodeling\n","708 remolded\n","709 renovations\n","710 repair\n","711 revealing\n","712 ride\n","713 rider\n","714 roadway\n","715 rod\n","716 rolled\n","717 rolling\n","718 rough\n","719 run\n","720 scanner\n","721 school\n","722 scooter\n","723 self\n","724 selfie\n","725 series\n","726 set\n","727 setting\n","728 setup\n","729 shapped\n","730 she\n","731 shelves\n","732 shirt\n","733 shirts\n","734 shots\n","735 show\n","736 showing\n","737 shut\n","738 sign\n","739 signs\n","740 sit\n","741 skateboarding\n","742 skirt\n","743 sliced\n","744 smiles\n","745 smiling\n","746 soap\n","747 soft\n","748 softening\n","749 someone\n","750 someones\n","751 sort\n","752 sound\n","753 spacious\n","754 sport\n","755 spot\n","756 stack\n","757 stands\n","758 standup\n","759 station\n","760 stations\n","761 stepping\n","762 stop\n","763 storage\n","764 stores\n","765 straight\n","766 streets\n","767 striped\n","768 stuck\n","769 students\n","770 stuffed\n","771 surround\n","772 surrounded\n","773 swiftly\n","774 tabby\n","775 tablet\n","776 tail\n","777 talking\n","778 tank\n","779 tasty\n","780 taxi\n","781 tea\n","782 they\n","783 thing\n","784 things\n","785 tightly\n","786 tire\n","787 tissue\n","788 toilette\n","789 tolet\n","790 toward\n","791 towering\n","792 toy\n","793 transparent\n","794 treat\n","795 tree\n","796 tricks\n","797 unattended\n","798 undergoing\n","799 unfinished\n","800 us\n","801 used\n","802 uses\n","803 vane\n","804 varying\n","805 vases\n","806 waits\n","807 was\n","808 washing\n","809 washroom\n","810 waste\n","811 wears\n","812 weather\n","813 well\n","814 where\n","815 which\n","816 who\n","817 wide\n","818 wind\n","819 without\n","820 workstation\n","821 wrong\n","822 pad\n","823 start\n","824 unk\n"]}],"source":["def yield_tokens():\n","  for i in range(len(sliced_train_dataset)):\n","    line=sliced_train_dataset[i]['sentences']['raw']\n","    tokens=toker(line)\n","    yield tokens\n","\n","token_generator=yield_tokens()\n","\n","vocab=build_vocab_from_iterator(token_generator)\n","vocab.append_token('pad')\n","vocab.append_token('start')\n","vocab.append_token('unk')                      #end already exist when taking dataset as 500\n","\n","for i in range(len(vocab)):\n","  print(i,vocab.get_itos()[i])"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MZOL_LnD0T7k","executionInfo":{"status":"ok","timestamp":1719814704655,"user_tz":-330,"elapsed":5500,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["vocab_size=len(vocab)\n","embedding_dim=100\n","filepath='/content/glove.6B.100d.txt'\n","\n","embedding_matrix_vocab = np.zeros((vocab_size,                                            # embedding_matrix_vocab contains the Glove embeddings of the vocabulary words\n","                                       embedding_dim))\n","\n","with open(filepath, encoding=\"utf8\") as f:\n","    for line in f:\n","        word, *vector = line.split()\n","        if word=='unk':\n","            continue\n","        if vocab.__contains__(word):\n","            idx = vocab.__getitem__(word)\n","            embedding_matrix_vocab[idx] = np.array(\n","                vector, dtype=np.float32)[:embedding_dim]"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ghgfy-oi0Wmy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814704655,"user_tz":-330,"elapsed":21,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"918b4204-f378-4a2c-c03b-9cd2d35414ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0.]\n"]}],"source":["np.shape(embedding_matrix_vocab)\n","print(embedding_matrix_vocab[vocab.__getitem__('unk')])"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"rbkiaIzz0Ykl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814704655,"user_tz":-330,"elapsed":18,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"084e29c7-c01d-41cf-f855-7479c952b657"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.54649255e-01  2.54874021e-01  1.26416009e-01 -1.54341416e-01\n","  8.67788775e-02  2.32674117e-01 -8.28174200e-02  2.90328873e-01\n"," -1.80766942e-02  3.97891219e-02  1.24876897e-01 -6.10143749e-02\n","  2.41499169e-01  1.59347787e-01  1.52479749e-01 -2.60556587e-02\n","  1.11346901e-01  1.22791192e-01 -2.19469810e-02 -1.69757675e-01\n","  1.29030495e-01  1.07934019e-01  1.16207433e-01 -1.81458675e-01\n","  3.08597568e-01 -2.53275897e-02 -2.46480045e-01 -2.31270120e-01\n"," -1.07880605e-01 -1.16530845e-01  2.81921137e-02  1.45322104e-01\n","  1.82588274e-02  3.53022597e-02  1.27797537e-01  2.40640482e-01\n"," -5.19883196e-02  3.46041328e-02  1.85193833e-01 -9.36370613e-02\n"," -2.96097846e-02 -3.36390224e-01  4.00645060e-02 -2.21827674e-01\n"," -2.96574838e-02  9.08368885e-02 -8.10369464e-02  2.47024141e-02\n","  5.55540035e-02 -3.68664015e-01 -1.35916899e-02 -8.66048856e-02\n","  1.12531815e-03  7.76724569e-01 -1.24784086e-01 -1.46725461e+00\n"," -1.10084591e-01  6.67930249e-02  1.05995432e+00  2.16526406e-01\n"," -5.23929982e-02  5.99500939e-01 -3.52947808e-02  2.75181093e-01\n","  3.42373033e-01 -3.52889921e-03  3.97836547e-01 -6.65848886e-02\n","  4.45310785e-02 -1.71157572e-01 -8.82523671e-02 -5.79998169e-02\n","  1.34156740e-01 -1.28672492e-01  9.78184229e-02  1.53477042e-01\n","  1.22252275e-01  6.93403692e-02 -2.10980149e-01 -2.19979986e-03\n","  3.48805832e-01  2.85506439e-02 -2.57393319e-01  1.40974600e-01\n"," -6.47006991e-01 -1.98268512e-01  5.40496095e-02 -1.15297008e-01\n"," -7.04640262e-02 -6.84916532e-02  5.43817385e-02 -8.48668568e-02\n","  2.21753633e-02 -5.44003200e-02 -3.18162473e-01 -1.33202043e-01\n"," -2.23503184e-01 -1.18401242e-01  5.38665917e-01  9.22526369e-02]\n"]}],"source":["#pad_emb_npa = np.zeros((1,embedding_matrix_vocab.shape[1]))   #embedding for '<pad>' token.\n","unk_emb_npa = np.mean(embedding_matrix_vocab,axis=0,keepdims=True)    #embedding for '<unk>' token.\n","\n","embedding_matrix_vocab[vocab.__getitem__('unk')]=unk_emb_npa\n","print(embedding_matrix_vocab[vocab.__getitem__('unk')])"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"NPvi8YHd0aiZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814704656,"user_tz":-330,"elapsed":16,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"ec93f0a7-bd4c-4ee0-b7f0-dca5e0c0c3a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(825, 100)"]},"metadata":{},"execution_count":18}],"source":["np.shape(embedding_matrix_vocab)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"1EUhbDyE0dYu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814704656,"user_tz":-330,"elapsed":13,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"5e9a1997-d520-4888-8a62-89e0de76f8d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([825, 100])\n"]}],"source":["import torch\n","my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix_vocab).float())\n","print(my_embedding_layer.weight.shape)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"wSPshAIhLiRm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814704657,"user_tz":-330,"elapsed":12,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"3e17c196-6285-4317-b6a4-e6cc87795c08"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-1.5465e-01,  2.5487e-01,  1.2642e-01, -1.5434e-01,  8.6779e-02,\n","         2.3267e-01, -8.2817e-02,  2.9033e-01, -1.8077e-02,  3.9789e-02,\n","         1.2488e-01, -6.1014e-02,  2.4150e-01,  1.5935e-01,  1.5248e-01,\n","        -2.6056e-02,  1.1135e-01,  1.2279e-01, -2.1947e-02, -1.6976e-01,\n","         1.2903e-01,  1.0793e-01,  1.1621e-01, -1.8146e-01,  3.0860e-01,\n","        -2.5328e-02, -2.4648e-01, -2.3127e-01, -1.0788e-01, -1.1653e-01,\n","         2.8192e-02,  1.4532e-01,  1.8259e-02,  3.5302e-02,  1.2780e-01,\n","         2.4064e-01, -5.1988e-02,  3.4604e-02,  1.8519e-01, -9.3637e-02,\n","        -2.9610e-02, -3.3639e-01,  4.0065e-02, -2.2183e-01, -2.9657e-02,\n","         9.0837e-02, -8.1037e-02,  2.4702e-02,  5.5554e-02, -3.6866e-01,\n","        -1.3592e-02, -8.6605e-02,  1.1253e-03,  7.7672e-01, -1.2478e-01,\n","        -1.4673e+00, -1.1008e-01,  6.6793e-02,  1.0600e+00,  2.1653e-01,\n","        -5.2393e-02,  5.9950e-01, -3.5295e-02,  2.7518e-01,  3.4237e-01,\n","        -3.5289e-03,  3.9784e-01, -6.6585e-02,  4.4531e-02, -1.7116e-01,\n","        -8.8252e-02, -5.8000e-02,  1.3416e-01, -1.2867e-01,  9.7818e-02,\n","         1.5348e-01,  1.2225e-01,  6.9340e-02, -2.1098e-01, -2.1998e-03,\n","         3.4881e-01,  2.8551e-02, -2.5739e-01,  1.4097e-01, -6.4701e-01,\n","        -1.9827e-01,  5.4050e-02, -1.1530e-01, -7.0464e-02, -6.8492e-02,\n","         5.4382e-02, -8.4867e-02,  2.2175e-02, -5.4400e-02, -3.1816e-01,\n","        -1.3320e-01, -2.2350e-01, -1.1840e-01,  5.3867e-01,  9.2253e-02])\n"]}],"source":["print(my_embedding_layer(torch.tensor(vocab['unk'])))"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"dTWpt-TK0flg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814707219,"user_tz":-330,"elapsed":2571,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"678cd74c-9421-4eb7-febe-3972d57b4e8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["sliced_train_dataset size: 500\n","10.426\n","[12, 8, 11, 10, 13, 10, 11, 9, 9, 11, 13, 9, 9, 9, 9, 8, 12, 9, 9, 21, 8, 14, 9, 11, 9, 11, 10, 9, 14, 11, 9, 9, 8, 11, 10, 9, 11, 14, 8, 11, 12, 11, 11, 9, 11, 12, 12, 8, 9, 8, 8, 9, 10, 8, 13, 9, 20, 13, 10, 10, 9, 12, 10, 12, 8, 12, 9, 11, 10, 8, 8, 9, 9, 9, 9, 13, 9, 10, 8, 11, 14, 12, 12, 11, 12, 8, 10, 8, 12, 11, 10, 25, 11, 8, 14, 11, 9, 8, 12, 8, 9, 9, 10, 9, 8, 10, 10, 11, 9, 10, 9, 11, 10, 18, 13, 11, 11, 11, 16, 10, 13, 9, 11, 10, 14, 11, 12, 12, 10, 10, 9, 11, 10, 8, 9, 9, 8, 10, 8, 9, 8, 11, 9, 9, 11, 10, 12, 9, 13, 9, 10, 10, 8, 18, 8, 10, 9, 8, 9, 14, 15, 11, 10, 13, 17, 11, 10, 10, 10, 9, 12, 8, 11, 9, 13, 11, 12, 9, 11, 9, 14, 11, 10, 8, 8, 10, 8, 9, 12, 13, 10, 8, 9, 10, 10, 10, 10, 10, 11, 9, 8, 10, 8, 8, 9, 8, 12, 11, 10, 12, 11, 11, 10, 10, 12, 10, 9, 9, 10, 8, 11, 11, 11, 8, 8, 9, 8, 10, 10, 9, 9, 9, 8, 8, 12, 12, 9, 11, 11, 10, 8, 9, 8, 8, 10, 9, 11, 11, 11, 8, 10, 8, 11, 8, 9, 10, 10, 11, 9, 8, 9, 14, 10, 14, 11, 9, 11, 12, 9, 9, 10, 8, 13, 12, 19, 11, 9, 14, 13, 10, 11, 8, 10, 9, 9, 10, 8, 10, 9, 11, 10, 8, 9, 8, 10, 8, 13, 15, 9, 11, 11, 9, 12, 9, 12, 9, 11, 9, 14, 13, 9, 9, 9, 9, 9, 11, 10, 17, 18, 11, 16, 11, 11, 14, 12, 10, 10, 8, 9, 11, 9, 8, 8, 8, 9, 11, 12, 11, 12, 11, 9, 8, 11, 9, 10, 10, 11, 10, 8, 10, 13, 10, 8, 12, 11, 8, 10, 9, 13, 16, 8, 11, 9, 19, 10, 9, 8, 10, 11, 9, 9, 8, 13, 10, 13, 11, 9, 12, 13, 8, 11, 11, 13, 15, 10, 9, 9, 8, 8, 12, 9, 10, 9, 8, 9, 10, 10, 9, 11, 10, 10, 10, 11, 11, 8, 9, 11, 9, 11, 23, 10, 11, 11, 11, 8, 10, 10, 14, 11, 9, 9, 9, 11, 11, 17, 11, 13, 8, 10, 18, 13, 9, 10, 10, 10, 11, 8, 8, 9, 8, 9, 14, 8, 10, 10, 8, 10, 8, 8, 8, 12, 8, 8, 10, 10, 9, 21, 8, 10, 12, 10, 10, 13, 9, 10, 12, 11, 11, 11, 10, 11, 13, 12, 8, 9, 10, 10, 8, 10, 9, 12, 13, 17, 12, 13, 8, 9, 9, 9, 10, 8, 14, 12, 14, 12, 13, 11, 11, 10, 11]\n"]}],"source":["len_list=list()\n","sum=0\n","print(f'sliced_train_dataset size: {len(sliced_train_dataset)}')\n","\n","for i in range(len(sliced_train_dataset)):\n","  sentence=sliced_train_dataset[i]['sentences']['raw']\n","  split_sentence=sentence.split(\" \")\n","  len_list.append(len(split_sentence))\n","  sum+=len(split_sentence)\n","\n","avg_length=sum/len(sliced_train_dataset)\n","\n","print(avg_length)\n","print(len_list)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNc3B6ar0hSX","outputId":"808a191d-447b-4d71-e016-12654834c228","executionInfo":{"status":"ok","timestamp":1719814707219,"user_tz":-330,"elapsed":19,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["10\n"]}],"source":["avg_length=round(avg_length)\n","print(avg_length)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"xbDm4bTV0i4n","executionInfo":{"status":"ok","timestamp":1719814707220,"user_tz":-330,"elapsed":16,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["standardise_len=13"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"XQzdwy030kUS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719814710240,"user_tz":-330,"elapsed":3036,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"56fa99c3-a1fa-4c41-92a5-213bc6c75d14"},"outputs":[{"output_type":"stream","name":"stdout","text":["0   11   A woman wearing a net on her head cutting a cake. \n","A woman wearing a net on her head cutting a cake. \n","1   8   A woman cutting a large white sheet cake.\n","A woman cutting a large white sheet cake.\n","2   11   A woman wearing a hair net cutting a large sheet cake.\n","A woman wearing a hair net cutting a large sheet cake.\n","3   10   there is a woman that is cutting a white cake\n","there is a woman that is cutting a white cake\n","4   12   A woman marking a cake with the back of a chef's knife. \n","A woman marking a cake with the back of a chef's knife. \n","5   10   A young boy standing in front of a computer keyboard.\n","A young boy standing in front of a computer keyboard.\n","6   11   a little boy wearing headphones and looking at a computer monitor\n","a little boy wearing headphones and looking at a computer monitor\n","7   9   He is listening intently to the computer at school.\n","He is listening intently to the computer at school.\n","8   9   A young boy stares up at the computer monitor.\n","A young boy stares up at the computer monitor.\n","9   10   a young kid with head phones on using a computer \n","a young kid with head phones on using a computer \n","10   13   a boy wearing headphones using one computer in a long row of computers\n","a boy wearing headphones using one computer in a long row of computers\n","11   9   A little boy with earphones on listening to something.\n","A little boy with earphones on listening to something.\n","12   9   A group of people sitting at desk using computers.\n","A group of people sitting at desk using computers.\n","13   9   Children sitting at computer stations on a long table.\n","Children sitting at computer stations on a long table.\n","14   9   A small child wearing headphones plays on the computer.\n","A small child wearing headphones plays on the computer.\n","15   8   A man is in a kitchen making pizzas.\n","A man is in a kitchen making pizzas.\n","16   12   Man in apron standing on front of oven with pans and bakeware\n","Man in apron standing on front of oven with pans and bakeware\n","17   9   A baker is working in the kitchen rolling dough.\n","A baker is working in the kitchen rolling dough.\n","18   9   A person standing by a stove in a kitchen.\n","A person standing by a stove in a kitchen.\n","19   21   A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.\n","A table with pies being made and a person standing near a wall\n","20   8   A woman in a room with a cat.\n","A woman in a room with a cat.\n","21   14   A girl smiles as she holds a cat and wears a brightly colored skirt.\n","A girl smiles as she holds a cat and wears a brightly colored\n","22   9   a woman is holding a cat in her kitchen\n","a woman is holding a cat in her kitchen\n","23   11   A woman is working in a kitchen carrying a soft toy.\n","A woman is working in a kitchen carrying a soft toy.\n","24   9   A woman is holding a cat in her kitchen.\n","A woman is holding a cat in her kitchen.\n","25   10   A commercial stainless kitchen with a pot of food cooking. \n","A commercial stainless kitchen with a pot of food cooking. \n","26   9   Some food sits in a pot in a kitchen. \n","Some food sits in a pot in a kitchen. \n","27   9   A kitchen has all stainless steel appliances and counters.\n","A kitchen has all stainless steel appliances and counters.\n","28   14   a kitchen with a sink and many cooking machines and a pot of food\n","a kitchen with a sink and many cooking machines and a pot of\n","29   11   Food cooks in a pot on a stove in a kitchen.\n","Food cooks in a pot on a stove in a kitchen.\n","30   9   Two men wearing aprons working in a commercial-style kitchen.\n","Two men wearing aprons working in a commercial-style kitchen.\n","31   9   Chefs preparing food in a professional metallic style kitchen.\n","Chefs preparing food in a professional metallic style kitchen.\n","32   8   Two people standing around in a large kitchen.\n","Two people standing around in a large kitchen.\n","33   11   A commercial kitchen with two men working to prepare several plates.\n","A commercial kitchen with two men working to prepare several plates.\n","34   10   two men in white shirts in a large steel kitchen\n","two men in white shirts in a large steel kitchen\n","35   8   Two chefs in a restaurant kitchen preparing food. \n","Two chefs in a restaurant kitchen preparing food. \n","36   11   Two cooks are cooking the food someone ordered at this restaurant\n","Two cooks are cooking the food someone ordered at this restaurant\n","37   13   The chef is cooking with pans on the stove next to an oven. \n","The chef is cooking with pans on the stove next to an oven.\n","38   8   Two men that are standing in a kitchen.\n","Two men that are standing in a kitchen.\n","39   11   Two cooks are near the stove in a stainless steel kitchen.\n","Two cooks are near the stove in a stainless steel kitchen.\n","40   12   this is a very dark picture of a room with a shelf\n","this is a very dark picture of a room with a shelf\n","41   11   a cluttered room with a table and shelf on the wall.\n","a cluttered room with a table and shelf on the wall.\n","42   11   A view of a messy room, with shelves on the wall.\n","A view of a messy room, with shelves on the wall.\n","43   9   A dark and cluttered storage area with wood walls.\n","A dark and cluttered storage area with wood walls.\n","44   10   A dim lit room consisting of many objects put together. \n","A dim lit room consisting of many objects put together. \n","45   12   A kitchen filled with black appliances and lots of counter top space.\n","A kitchen filled with black appliances and lots of counter top space.\n","46   12   some brown cabinets a black oven a tea kettle and a microwave\n","some brown cabinets a black oven a tea kettle and a microwave\n","47   8   A small kitchen with glass and wooden cabinets.\n","A small kitchen with glass and wooden cabinets.\n","48   9   A modern style kitchen filled with may different items.\n","A modern style kitchen filled with may different items.\n","49   8   A kitchen with wooden cabinets and black appliances.\n","A kitchen with wooden cabinets and black appliances.\n","50   8   A professional kitchen filled with sinks and appliances.\n","A professional kitchen filled with sinks and appliances.\n","51   9   A kitchen area with toilet and various cleaning appliances.\n","A kitchen area with toilet and various cleaning appliances.\n","52   10   A commercial dish washing station with a toilet in it.\n","A commercial dish washing station with a toilet in it.\n","53   8   A toilet and mop bucket in a kitchen.\n","A toilet and mop bucket in a kitchen.\n","54   13   A cluttered room with a sink, a toilet and in industrial mop bucket.\n","A cluttered room with a sink, a toilet and in industrial mop bucket.\n","55   9   A kitchen with wood floors and lots of furniture.\n","A kitchen with wood floors and lots of furniture.\n","56   20   A beautiful, open kitchen and dining room area features an island in the center and wood cabinets and large windows.\n","A beautiful, open kitchen and dining room area features an island in the\n","57   13   A kitchen made of mostly wood with a small desk with a laptop.\n","A kitchen made of mostly wood with a small desk with a laptop.\n","58   10   A very spacious room with a kitchen and dining area.\n","A very spacious room with a kitchen and dining area.\n","59   10   A full view of an open kitchen and dining area.\n","A full view of an open kitchen and dining area.\n","60   9   A woman eating vegetables in front of a stove.\n","A woman eating vegetables in front of a stove.\n","61   11   A woman forks vegetables out of a bowl into her mouth. \n","A woman forks vegetables out of a bowl into her mouth. \n","62   10   Woman eating an assortment of mixed vegetables in a bowl.\n","Woman eating an assortment of mixed vegetables in a bowl.\n","63   12   A young woman standing in a kitchen eats a plate of vegetables.\n","A young woman standing in a kitchen eats a plate of vegetables.\n","64   8   A woman eating fresh vegetables from a bowl.\n","A woman eating fresh vegetables from a bowl.\n","65   12   A boy performing a kickflip on his skateboard on a city street.\n","A boy performing a kickflip on his skateboard on a city street.\n","66   9   A man is doing a trick on a skateboard\n","A man is doing a trick on a skateboard\n","67   11   A guy jumps in the air with his skateboard beneath him.\n","A guy jumps in the air with his skateboard beneath him.\n","68   10   Man in all black doing a trick on his skateboard.\n","Man in all black doing a trick on his skateboard.\n","69   8   A skateboarder flipping his board on a street.\n","A skateboarder flipping his board on a street.\n","70   8   A kitchen with a stove, microwave and refrigerator.\n","A kitchen with a stove, microwave and refrigerator.\n","71   9   A refrigerator, oven and microwave sitting in a kitchen.\n","A refrigerator, oven and microwave sitting in a kitchen.\n","72   8   The kitchenette uses  small space to great efficiency.\n","The kitchenette uses  small space to great efficiency.\n","73   9   an image of a kitchen setting with black appliances\n","an image of a kitchen setting with black appliances\n","74   9   A kitchen with cabinets, a stove, microwave and refrigerator.\n","A kitchen with cabinets, a stove, microwave and refrigerator.\n","75   13   The dining table near the kitchen has a bowl of fruit on it.\n","The dining table near the kitchen has a bowl of fruit on it.\n","76   9   A small kitchen has various appliances and a table.\n","A small kitchen has various appliances and a table.\n","77   10   The kitchen is clean and ready for us to see.\n","The kitchen is clean and ready for us to see.\n","78   8   A kitchen and dining area decorated in white.\n","A kitchen and dining area decorated in white.\n","79   11   A kitchen that has a bowl of fruit on the table.\n","A kitchen that has a bowl of fruit on the table.\n","80   14   A group of people riding on the back of a loaded red pickup truck.\n","A group of people riding on the back of a loaded red pickup\n","81   12   A truck with a number of people and things in the back\n","A truck with a number of people and things in the back\n","82   12   Men are crowded on the back of a small overloaded pickup truck.\n","Men are crowded on the back of a small overloaded pickup truck.\n","83   11   An old pick up truck over loaded with people and cargo.\n","An old pick up truck over loaded with people and cargo.\n","84   12   A truck carries a large amount of items and a few people.\n","A truck carries a large amount of items and a few people.\n","85   8   A large boat filled with mean on wheels.\n","A large boat filled with mean on wheels.\n","86   9   A bunch of people aboard a boat with wheels. \n","A bunch of people aboard a boat with wheels. \n","87   8   A boat is being rolled on a trailer\n","A boat is being rolled on a trailer\n","88   11   A large boat full of men is sitting on a cart \n","A large boat full of men is sitting on a cart \n","89   11   A boat full of people is on a trailer with wheels.\n","A boat full of people is on a trailer with wheels.\n","90   10   several young students working at a desk with multiple computers\n","several young students working at a desk with multiple computers\n","91   25   A young man at his workstation examines the monitor of a lap top, with one hand on the keyboard and the other on the mouse.\n","A young man at his workstation examines the monitor of a lap top,\n","92   11   A man is working on a laptop next to other computers.\n","A man is working on a laptop next to other computers.\n","93   8   People in a large room, use multiple computers.\n","People in a large room, use multiple computers.\n","94   14   A young professional is working at his laptop while his coworker is reading material.\n","A young professional is working at his laptop while his coworker is reading\n","95   11   A man riding an elephant into some water of a creek.\n","A man riding an elephant into some water of a creek.\n","96   9   man riding an elephant into water surrounded by forest\n","man riding an elephant into water surrounded by forest\n","97   8   A man riding an elephant in a river.\n","A man riding an elephant in a river.\n","98   12   A man in a brown shirt rides an elephant into the water.\n","A man in a brown shirt rides an elephant into the water.\n","99   8   A man rides an elephant into a river.\n","A man rides an elephant into a river.\n","100   8   a couple of women  are in a kitchen\n","a couple of women  are in a kitchen\n","101   9   Two people standing in the kitchen of a home.\n","Two people standing in the kitchen of a home.\n","102   10   A group of people who are around a kitchen counter.\n","A group of people who are around a kitchen counter.\n","103   9   A group of people prepare dinner in the kitchen.\n","A group of people prepare dinner in the kitchen.\n","104   8   two women in a kitchen bottles and lights\n","two women in a kitchen bottles and lights\n","105   10   The woman in the kitchen is holding a huge pan.\n","The woman in the kitchen is holding a huge pan.\n","106   10   A chef carrying a large pan inside of a kitchen.\n","A chef carrying a large pan inside of a kitchen.\n","107   10   A woman is holding a large pan in a kitchen. \n","A woman is holding a large pan in a kitchen. \n","108   9   A woman cooking in a kitchen with granite counters.\n","A woman cooking in a kitchen with granite counters.\n","109   10   A woman cooking in her kitchen with a black pan.\n","A woman cooking in her kitchen with a black pan.\n","110   9   A baby is laying down with a teddy bear.\n","A baby is laying down with a teddy bear.\n","111   11   A baby laying in a crib with a stuffed teddy bear.\n","A baby laying in a crib with a stuffed teddy bear.\n","112   10   A baby wearing gloves, lying next to a teddy bear\n","A baby wearing gloves, lying next to a teddy bear\n","113   18   A baby stares to the left while taking a picture with a teddy bear lays beside it .\n","A baby stares to the left while taking a picture with a teddy\n","114   13   A baby lies on blue and green bedding next to a teddy bear.\n","A baby lies on blue and green bedding next to a teddy bear.\n","115   10   a person with a shopping cart on a city street \n","a person with a shopping cart on a city street \n","116   11   City dwellers walk by as a homeless man begs for cash.\n","City dwellers walk by as a homeless man begs for cash.\n","117   11   People walking past a homeless man begging on a city street\n","People walking past a homeless man begging on a city street\n","118   16   a homeless man holding a cup and standing next to a shopping cart on a street\n","a homeless man holding a cup and standing next to a shopping cart\n","119   10   People are walking on the street by a homeless person.\n","People are walking on the street by a homeless person.\n","120   13   Two bikers, one in front of a building, the other in the city.\n","Two bikers, one in front of a building, the other in the city.\n","121   9   Two shots of men riding bicycles down city streets.\n","Two shots of men riding bicycles down city streets.\n","122   11   A man riding a bike in front of a tall building.\n","A man riding a bike in front of a tall building.\n","123   10   There are two different people riding bikes down the street.\n","There are two different people riding bikes down the street.\n","124   14   Two photos of a person riding a bicycle on the side of the road.\n","Two photos of a person riding a bicycle on the side of the\n","125   11   A person on a skateboard and bike at a skate park.\n","A person on a skateboard and bike at a skate park.\n","126   12   A man on a skateboard performs a trick at the skate park\n","A man on a skateboard performs a trick at the skate park\n","127   12   A skateboarder jumps into the air as he performs a skateboard trick.\n","A skateboarder jumps into the air as he performs a skateboard trick.\n","128   10   Athletes performing tricks on a BMX bicycle and a skateboard.\n","Athletes performing tricks on a BMX bicycle and a skateboard.\n","129   10   a man falls off his skateboard in a skate park.\n","a man falls off his skateboard in a skate park.\n","130   8   a blue bike parked on a side walk \n","a blue bike parked on a side walk \n","131   11   A bicycle is chained to a fixture on a city street\n","A bicycle is chained to a fixture on a city street\n","132   10   A blue bicycle sits on a sidewalk near a street.\n","A blue bicycle sits on a sidewalk near a street.\n","133   8   A bicycle is locked up to a post\n","A bicycle is locked up to a post\n","134   8   a bike sits parked next to a street \n","a bike sits parked next to a street \n","135   9   People riding bicycles down the road approaching a bird.\n","People riding bicycles down the road approaching a bird.\n","136   8   three bicycle riders some trees and a pigeon\n","three bicycle riders some trees and a pigeon\n","137   10   A geoup of people on bicycles coming down a street.\n","A geoup of people on bicycles coming down a street.\n","138   8   Several smiling bicycle riders approaching a colorful pigeon.\n","Several smiling bicycle riders approaching a colorful pigeon.\n","139   9   A pigeon greets three bicyclists on a park path\n","A pigeon greets three bicyclists on a park path\n","140   8   A toilet and a sink in small bathroom.\n","A toilet and a sink in small bathroom.\n","141   11   a bathroom with just a toliet and a sink in it\n","a bathroom with just a toliet and a sink in it\n","142   9   A small section of a bathroom with dim lighting.\n","A small section of a bathroom with dim lighting.\n","143   9   a bathroom with a toilet, sink, cabinet and mirror\n","a bathroom with a toilet, sink, cabinet and mirror\n","144   10   The bathroom has been cleaned and is ready to use. \n","The bathroom has been cleaned and is ready to use. \n","145   10   A bicycle store shows two males leaning toward a bike.\n","A bicycle store shows two males leaning toward a bike.\n","146   12   A man adjust a bicycle in a bike shop with a child.\n","A man adjust a bicycle in a bike shop with a child.\n","147   8   The bike shop employee is helping a customer. \n","The bike shop employee is helping a customer. \n","148   13   A man and a boy are talking about a bicycle in a store.\n","A man and a boy are talking about a bicycle in a store.\n","149   9   Two people in a shop looking at a bike.\n","Two people in a shop looking at a bike.\n","150   10   A shower stall with interesting tile is the focal point.\n","A shower stall with interesting tile is the focal point.\n","151   9   A full perspective of a washroom with a sink. \n","\n","A full perspective of a washroom with a sink. \n","\n","152   8   A white bathroom sink sitting under a mirror.\n","A white bathroom sink sitting under a mirror.\n","153   18   I picture of a bathroom with a stand up shower stall and a person's reflection in the mirror.\n","I picture of a bathroom with a stand up shower stall and a\n","154   8   A small shower behind a small bathroom sink.\n","A small shower behind a small bathroom sink.\n","155   10   A view of a very large bathroom with mirrored walls.\n","A view of a very large bathroom with mirrored walls.\n","156   9   a corner bathroom with two sinks and a bathtub\n","a corner bathroom with two sinks and a bathtub\n","157   8   A bathroom with a sink, shower and bathtub.\n","A bathroom with a sink, shower and bathtub.\n","158   9   A large bathroom with cabinets, mirrors and a tub.\n","A large bathroom with cabinets, mirrors and a tub.\n","159   14   This bathroom has mirrors on the doors, cabinets, and a bathtub in the corner.\n","This bathroom has mirrors on the doors, cabinets, and a bathtub in the\n","160   15   a long beige bathroom with a door to a bedroom and another to the hallway\n","a long beige bathroom with a door to a bedroom and another to\n","161   11   A view of a bathroom with double sinks and carpeted floor.\n","A view of a bathroom with double sinks and carpeted floor.\n","162   10   white vanity that opens up to a bathroom with shower\n","white vanity that opens up to a bathroom with shower\n","163   13   a hallway with diamond patterned walls leading to a bathroom and a bedroom\n","a hallway with diamond patterned walls leading to a bathroom and a bedroom\n","164   17   A bathroom with a large sink and mirror opens onto a room with the toilet and bathtub.\n","A bathroom with a large sink and mirror opens onto a room with\n","165   11   The view of a large bathroom with a walk in closet.\n","The view of a large bathroom with a walk in closet.\n","166   10   A bathroom with a walk-in closet and a jacuzzi tub.\n","A bathroom with a walk-in closet and a jacuzzi tub.\n","167   10   A white and beige tiled bathroom and adjoining walk-in closet.\n","A white and beige tiled bathroom and adjoining walk-in closet.\n","168   10   White tiled bathroom with a vanity tub and white flowers.\n","White tiled bathroom with a vanity tub and white flowers.\n","169   9   Long shot of a bathroom includes closet and tub.\n","Long shot of a bathroom includes closet and tub.\n","170   12   A white toilet sitting next to a green wall under a picture.\n","A white toilet sitting next to a green wall under a picture.\n","171   8   The letter A framed in a green bathroom.\n","The letter A framed in a green bathroom.\n","172   11   There is a potted plant on the back of a toilet\n","There is a potted plant on the back of a toilet\n","173   9   A white toilet and some tissue in a room.\n","A white toilet and some tissue in a room.\n","174   13   A commode with a plant on it under a picture on the wall.\n","A commode with a plant on it under a picture on the wall.\n","175   11   This is an image of the inside of a nice bathroom.\n","This is an image of the inside of a nice bathroom.\n","176   12   A white toilet next to a walk in shower and a sink.\n","A white toilet next to a walk in shower and a sink.\n","177   9   A potted plant is being displayed in a bathroom.\n","A potted plant is being displayed in a bathroom.\n","178   11   A tiled bathroom with a potted plant as a center piece.\n","A tiled bathroom with a potted plant as a center piece.\n","179   9   Interior bathroom scene with modern furnishings including a plant.\n","Interior bathroom scene with modern furnishings including a plant.\n","180   14   Two dogs are looking up while they stand near the toilet in the bathroom.\n","Two dogs are looking up while they stand near the toilet in the\n","181   11   Two small dogs standing in a restroom next to a toilet.\n","Two small dogs standing in a restroom next to a toilet.\n","182   10   Two dogs looking up at a camera in a bathroom.\n","Two dogs looking up at a camera in a bathroom.\n","183   8   Two small lap dogs in a small bathroom.\n","Two small lap dogs in a small bathroom.\n","184   8   Two small dogs stand together in a bathroom.\n","Two small dogs stand together in a bathroom.\n","185   10   A bathroom with a walk in shower currently under repair.\n","A bathroom with a walk in shower currently under repair.\n","186   8   A photograph of a bathroom undergoing major renovations.\n","A photograph of a bathroom undergoing major renovations.\n","187   9   Interior shot of bathroom in the process of remodeling.\n","Interior shot of bathroom in the process of remodeling.\n","188   11   Picture of bathroom being remolded where sink has not been installed. \n","Picture of bathroom being remolded where sink has not been installed. \n","189   13   A room under construction with an unfinished shower and plumbing for the sink.\n","A room under construction with an unfinished shower and plumbing for the sink.\n","190   9   The bathroom is clean for the guests to use. \n","The bathroom is clean for the guests to use. \n","191   8   A bath and a sink in a room.\n","A bath and a sink in a room.\n","192   9   A bathroom area with tub, sink and standup shower.\n","A bathroom area with tub, sink and standup shower.\n","193   10   a view of a bathroom which is covered in tiles.\n","a view of a bathroom which is covered in tiles.\n","194   10   A kitchen is shown with a tub and a sink.\n","A kitchen is shown with a tub and a sink.\n","195   10   A white bath tub sitting next to a bathroom sink.\n","A white bath tub sitting next to a bathroom sink.\n","196   10   There is a bathtub and a counter in a bathroom.\n","There is a bathtub and a counter in a bathroom.\n","197   10   A bathroom with a tub and shower and a sink.\n","A bathroom with a tub and shower and a sink.\n","198   10   This is a shower and bathtub without a shower curtain. \n","This is a shower and bathtub without a shower curtain. \n","199   9   A bathroom that has a mirror and a bathtub.\n","A bathroom that has a mirror and a bathtub.\n","200   8   A full view of a shower with glass.\n","A full view of a shower with glass.\n","201   10   A walk in shower with a hand held shower head.\n","A walk in shower with a hand held shower head.\n","202   8   A bathroom shower stall with a shower head.\n","A bathroom shower stall with a shower head.\n","203   8   a glass walled shower in a home bathroom\n","a glass walled shower in a home bathroom\n","204   8   a see through glass shower in a bathroom \n","a see through glass shower in a bathroom \n","205   8   a couple of buckets in a white room\n","a couple of buckets in a white room\n","206   11   A bathroom with no toilets and a red and green bucket. \n","A bathroom with no toilets and a red and green bucket. \n","207   11   a shower room with two buckets, tolet paper holder and soap.\n","a shower room with two buckets, tolet paper holder and soap.\n","208   10   A standing toilet in a bathroom next to a window.\n","A standing toilet in a bathroom next to a window.\n","209   12   This picture looks like a janitors closet with buckets on the floor.\n","This picture looks like a janitors closet with buckets on the floor.\n","210   11   Blue dust pan and brush on floor next to white commode.\n","Blue dust pan and brush on floor next to white commode.\n","211   11   A toilet in a low budget bathroom with a rough floor.\n","A toilet in a low budget bathroom with a rough floor.\n","212   10   a close up of a toilet and a dust pale\n","a close up of a toilet and a dust pale\n","213   10   A clean toilet in a bathroom with a cement floor.\n","A clean toilet in a bathroom with a cement floor.\n","214   11   a bathroom with a blue dustpan and broom on the floor \n","a bathroom with a blue dustpan and broom on the floor \n","215   10   A bathroom with a sink and toilet and a window.\n","A bathroom with a sink and toilet and a window.\n","216   9   A white bathroom with white fixtures and tile floor\n","A white bathroom with white fixtures and tile floor\n","217   9   A small white toilet sitting next to a sink.\n","A small white toilet sitting next to a sink.\n","218   9   Small bathroom with a toilet, shower, and small mirror. \n","Small bathroom with a toilet, shower, and small mirror. \n","219   8   A bathroom contains a toilet and a sink.\n","A bathroom contains a toilet and a sink.\n","220   10   A large porcelain toilet posed with a tan flower pot. \n","A large porcelain toilet posed with a tan flower pot. \n","221   11   A toilet is sitting on the ground next to a plant.\n","A toilet is sitting on the ground next to a plant.\n","222   11   A toliet sitting on a curb in front of a house.\n","A toliet sitting on a curb in front of a house.\n","223   8   A white toilet sits on the front lawn.\n","A white toilet sits on the front lawn.\n","224   8   A white toilet sitting on a sidewalk outside.\n","A white toilet sitting on a sidewalk outside.\n","225   9   a gray bicycle is locked to some metal doors\n","a gray bicycle is locked to some metal doors\n","226   8   A bike that is padlocked to a wall.\n","A bike that is padlocked to a wall.\n","227   9   The bicycle is locked on the green, metal wall. \n","The bicycle is locked on the green, metal wall. \n","228   9   A bike chained to the doors of a building \n","A bike chained to the doors of a building \n","229   9   A bike is locked and hanging from a door.\n","A bike is locked and hanging from a door.\n","230   9   A man takes a picture in the bathroom mirror\n","\n","A man takes a picture in the bathroom mirror\n","\n","231   8   a person taking a photo in a mirror \n","a person taking a photo in a mirror \n","232   8   A man taking a picture of a bathroom.\n","A man taking a picture of a bathroom.\n","233   8   A bathroom sink sitting under a bathroom mirror.\n","A bathroom sink sitting under a bathroom mirror.\n","234   11   A man takes a picture of his reflection in a mirror. \n","A man takes a picture of his reflection in a mirror. \n","235   12   A bathroom with a white toilet sitting next to a bathroom sink.\n","A bathroom with a white toilet sitting next to a bathroom sink.\n","236   9   A bathroom vanity and toilet in a public restroom.\n","A bathroom vanity and toilet in a public restroom.\n","237   11   A view of a bathroom shows a toilet, sink and mirror.\n","A view of a bathroom shows a toilet, sink and mirror.\n","238   10   A modern looking bathroom has a toilet and a sink. \n","A modern looking bathroom has a toilet and a sink. \n","239   10   This bathroom has brown counter tops and a white toilet\n","This bathroom has brown counter tops and a white toilet\n","240   8   A bathroom with two sinks and a shower.\n","A bathroom with two sinks and a shower.\n","241   9   a big bathroom that has some sinks in it\n","a big bathroom that has some sinks in it\n","242   8   Bathroom vanity with double sinks and large mirrors.\n","Bathroom vanity with double sinks and large mirrors.\n","243   8   A bathroom with sinks mirrors and tile flooring.\n","A bathroom with sinks mirrors and tile flooring.\n","244   10   A bathroom that has two sinks next to each other.\n","A bathroom that has two sinks next to each other.\n","245   9   an image of a cars driving on the highway\n","an image of a cars driving on the highway\n","246   11   A section of traffic coming to a stop at an intersection.\n","A section of traffic coming to a stop at an intersection.\n","247   11   A bunch of cars sit at the intersection of a street.\n","A bunch of cars sit at the intersection of a street.\n","248   11   This is a picture of traffic on a very busy street.\n","This is a picture of traffic on a very busy street.\n","249   8   A busy intersection filled with cars in asia\n","A busy intersection filled with cars in asia\n","250   10   This shot is of a crowded highway full of traffic\n","This shot is of a crowded highway full of traffic\n","251   8   there are many taxi cabs on the road\n","there are many taxi cabs on the road\n","252   11   A city street with lots of traffic and lined with buildings.\n","A city street with lots of traffic and lined with buildings.\n","253   8   Heavy city traffic all going in one direction\n","Heavy city traffic all going in one direction\n","254   9   Many cars stuck in traffic on a high way\n","Many cars stuck in traffic on a high way\n","255   10   Mirror view of a bathroom with a sink and tub.\n","Mirror view of a bathroom with a sink and tub.\n","256   10   mirror reflection of sink and the tub next to it.\n","mirror reflection of sink and the tub next to it.\n","257   11   A bathroom mirror has the reflection of the sink and bathtub.\n","A bathroom mirror has the reflection of the sink and bathtub.\n","258   8   The bathroom is very small with white fixtures. \n","The bathroom is very small with white fixtures. \n","259   8   A sink is shown in a small bathroom\n","A sink is shown in a small bathroom\n","260   9   A city street filled with traffic and parking lights.\n","A city street filled with traffic and parking lights.\n","261   14   A full view of a late evening with many cars parked on the street.\n","A full view of a late evening with many cars parked on the\n","262   10   A very dark street with cars and many wires above.\n","A very dark street with cars and many wires above.\n","263   14   A car's break lights glow as it waits at a red-light at an intersection.\n","A car's break lights glow as it waits at a red-light at an\n","264   11   A street scene with cars, street signs, and many high wires.\n","A street scene with cars, street signs, and many high wires.\n","265   9   A white toilet and a sink in a room.\n","A white toilet and a sink in a room.\n","266   10   White bathroom area with a blue and white shower curtain. \n","White bathroom area with a blue and white shower curtain. \n","267   11   Small bathroom area with a blue and white shower curtain hanging. \n","Small bathroom area with a blue and white shower curtain hanging. \n","268   9   A bath tub shower sitting next to a toilet.\n","A bath tub shower sitting next to a toilet.\n","269   9   A toilet with a sink and the door opened\n","A toilet with a sink and the door opened\n","270   10   a small white bathroom with a curtain for the shower\n","a small white bathroom with a curtain for the shower\n","271   8   A bathroom sink shapped like a glass bowl.\n","A bathroom sink shapped like a glass bowl.\n","272   13   A man that is standing with a mug in front of a mirror.\n","A man that is standing with a mug in front of a mirror.\n","273   11   a man  taking a picture of himself in a bathroom mirror\n","a man  taking a picture of himself in a bathroom mirror\n","274   19   A man standing in a bathroom taking a picture of his self in the mirror with his cell phone\n","A man standing in a bathroom taking a picture of his self in\n","275   10   a man taking a selfie in a little bathroom mirror \n","a man taking a selfie in a little bathroom mirror \n","276   8   A door opens to a bare, white bathroom. \n","A door opens to a bare, white bathroom. \n","277   14   A varying palette of neutrals in a bathroom awaits the softening of the cabinetry.\n","A varying palette of neutrals in a bathroom awaits the softening of the\n","278   13   A bathroom with a tile floor, bathtub and shower and no shower curtain.\n","A bathroom with a tile floor, bathtub and shower and no shower curtain.\n","279   10   A bathroom is shown with a shower and a toilet.\n","A bathroom is shown with a shower and a toilet.\n","280   11   A beige and white three piece bathroom with no shower curtain.\n","A beige and white three piece bathroom with no shower curtain.\n","281   8   A bathroom sink sitting under a large mirror.\n","A bathroom sink sitting under a large mirror.\n","282   10   A bathroom with a vanity cabinet on the wrong wall\n","A bathroom with a vanity cabinet on the wrong wall\n","283   9   A small beige bathroom with an additional medicine cabinet.\n","A small beige bathroom with an additional medicine cabinet.\n","284   9   A bathroom sink with a mirror and medicine cabinet.\n","A bathroom sink with a mirror and medicine cabinet.\n","285   10   THERE IS A BATHROOM WITH A SINK AND A MIRROR\n","THERE IS A BATHROOM WITH A SINK AND A MIRROR\n","286   8   A white urinal mounted to a bathroom wall.\n","A white urinal mounted to a bathroom wall.\n","287   10   A pair of kitchen tongs on top of a urinal.\n","A pair of kitchen tongs on top of a urinal.\n","288   9   Bathroom urinal with kitchen tongs on top behind plumbing.\n","Bathroom urinal with kitchen tongs on top behind plumbing.\n","289   11   A urinal in a bathroom with tongs on top of it.\n","A urinal in a bathroom with tongs on top of it.\n","290   10   a metal thing is sitting on top of a urinal\n","a metal thing is sitting on top of a urinal\n","291   8   A glazed donut hanging from a metal rod.\n","A glazed donut hanging from a metal rod.\n","292   9   A tasty looking treat hanging on a car antenna.\n","A tasty looking treat hanging on a car antenna.\n","293   8   A donut on the antenna of a car.\n","A donut on the antenna of a car.\n","294   9   A donut put around an antenna on a car \n","A donut put around an antenna on a car \n","295   8   There is a donut sitting on the car.\n","There is a donut sitting on the car.\n","296   13   An orange striped tabby cat laying on top of a red vehicle's wheel.\n","An orange striped tabby cat laying on top of a red vehicle's wheel.\n","297   14   a cat with a big fluffy tail sitting on top of a car tire \n","a cat with a big fluffy tail sitting on top of a car\n","298   9   A cat sitting on a wheel of a vehicle.\n","A cat sitting on a wheel of a vehicle.\n","299   10   A cat hiding in a fender well of a car. \n","A cat hiding in a fender well of a car. \n","300   11   An orange cat hiding on the wheel of a red car.\n","An orange cat hiding on the wheel of a red car.\n","301   9   A large jetliner flying over a traffic filled street.\n","A large jetliner flying over a traffic filled street.\n","302   11   An airplane flies low in the sky over a city street. \n","An airplane flies low in the sky over a city street. \n","303   9   An airplane flies over a street with many cars.\n","An airplane flies over a street with many cars.\n","304   12   An airplane comes in to land over a road full of cars\n","An airplane comes in to land over a road full of cars\n","305   9   The plane is flying over top of the cars\n","The plane is flying over top of the cars\n","306   11   A silver and red bus in parking lot next to cars.\n","A silver and red bus in parking lot next to cars.\n","307   9   A silver bus that is parked in a lot.\n","A silver bus that is parked in a lot.\n","308   13   An older picture of a bus and other vehicles in a parking lot. \n","An older picture of a bus and other vehicles in a parking lot.\n","309   13   An old bus parks in a parking lot with other cars and bus.\n","An old bus parks in a parking lot with other cars and bus.\n","310   9   An older silver metro bus in a parking lot.\n","An older silver metro bus in a parking lot.\n","311   9   a little kid riding a skateboard at a park\n","a little kid riding a skateboard at a park\n","312   9   A child playing on a skateboard in a park.\n","A child playing on a skateboard in a park.\n","313   9   A girl stepping onto a skateboard in the playground\n","A girl stepping onto a skateboard in the playground\n","314   9   A young lady riding a skateboard across a street.\n","A young lady riding a skateboard across a street.\n","315   11   The kid is skateboarding on the street while wearing a jacket.\n","The kid is skateboarding on the street while wearing a jacket.\n","316   10   A person on a street next to a motor bike.\n","A person on a street next to a motor bike.\n","317   16   A cop standing next to a police bike next to a man sitting on a  curb.\n","A cop standing next to a police bike next to a man sitting\n","318   18   A man sits on the sidewalk next to a police motorcycle with a police officer in the background.\n","A man sits on the sidewalk next to a police motorcycle with a\n","319   11   A man that is sitting down next to a cops motorcycle.\n","A man that is sitting down next to a cops motorcycle.\n","320   16   A man is sitting on the sidewalk while a police officer is doing something behind him.\n","A man is sitting on the sidewalk while a police officer is doing\n","321   11   A man looking down with a backpack on in a house.\n","A man looking down with a backpack on in a house.\n","322   10   THERE IS A MAN WITH BOOK BAG ON HIS BAG \n","THERE IS A MAN WITH BOOK BAG ON HIS BAG \n","323   13   a person standing in a stone walled and floored room wearing a backpack \n","a person standing in a stone walled and floored room wearing a backpack\n","324   12   A guy standing in a very dark room with a small window.\n","A guy standing in a very dark room with a small window.\n","325   10   A man in a stone room possibly using the bathroom\n","A man in a stone room possibly using the bathroom\n","326   10   A toilet sitting in a bathroom next to a mirror.\n","A toilet sitting in a bathroom next to a mirror.\n","327   8   A small bathroom with the toilet seat up.\n","A small bathroom with the toilet seat up.\n","328   9   The toilet seat was left in the up position.\n","The toilet seat was left in the up position.\n","329   11   A door open revealing a small bathroom with sink and toilet.\n","A door open revealing a small bathroom with sink and toilet.\n","330   9   A toilet and a sink in a small bathroom.\n","A toilet and a sink in a small bathroom.\n","331   8   The shiny motorcycle has been put on display.\n","The shiny motorcycle has been put on display.\n","332   8   The new motorcycle on display is very shiny.\n","The new motorcycle on display is very shiny.\n","333   8   A motorcycle is parked inside of a building.\n","A motorcycle is parked inside of a building.\n","334   9   A brand new motorcycle on display at a show.\n","A brand new motorcycle on display at a show.\n","335   11   The front end of a red motorcycle that is on display.\n","The front end of a red motorcycle that is on display.\n","336   12   A row of white toilets sitting on top of a dirt ground.\n","A row of white toilets sitting on top of a dirt ground.\n","337   11   A bunch of dirty looking white toilets in a row outside.\n","A bunch of dirty looking white toilets in a row outside.\n","338   12   There is a row of used and broken toilets in a field.\n","There is a row of used and broken toilets in a field.\n","339   11   A row of toilets with broken seat tops on the ground.\n","A row of toilets with broken seat tops on the ground.\n","340   9   LOTS OF BROKEN TOILETS SITTING OUT ON A LAWN\n","LOTS OF BROKEN TOILETS SITTING OUT ON A LAWN\n","341   8   Bathroom with white fixtures, tiles and corner shower.\n","Bathroom with white fixtures, tiles and corner shower.\n","342   11   A white toilet sitting in a bathroom next to a shower.\n","A white toilet sitting in a bathroom next to a shower.\n","343   9   Bathroom shot of non transparent shower glass and toilet.\n","Bathroom shot of non transparent shower glass and toilet.\n","344   10   An all white bathroom with a shower, toilet and sink.\n","An all white bathroom with a shower, toilet and sink.\n","345   10   A plain white bathroom with a sink, shower, and toilet.\n","A plain white bathroom with a sink, shower, and toilet.\n","346   10   a motorcycle in a driveway with a vanity license plate \n","a motorcycle in a driveway with a vanity license plate \n","347   10   Two luxurious motor bikes on the road near a hedge.\n","Two luxurious motor bikes on the road near a hedge.\n","348   8   a close up of a motorcycle license plate\n","a close up of a motorcycle license plate\n","349   10   A motor bike parked on the side of the road.\n","A motor bike parked on the side of the road.\n","350   12   The back of a motorcycle showing the wheel, license plate and lights. \n","The back of a motorcycle showing the wheel, license plate and lights. \n","351   10   A picture of a wooden container with crosses on it.\n","A picture of a wooden container with crosses on it.\n","352   8   a close up of wood decorated with crosses\n","a close up of wood decorated with crosses\n","353   12   close up of a wooden base with crosses designed on the neck\n","close up of a wooden base with crosses designed on the neck\n","354   10   A close up shut off some sort of religious object. \n","A close up shut off some sort of religious object. \n","355   8   A shiny wooden object is decorated with crosses.\n","A shiny wooden object is decorated with crosses.\n","356   10   There is a man picking bananas next to a street.\n","There is a man picking bananas next to a street.\n","357   9   A man standing over several bunches of green bananas.\n","A man standing over several bunches of green bananas.\n","358   13   A man attending to several bundles of banana's as a motorcycle rides by.\n","A man attending to several bundles of banana's as a motorcycle rides by.\n","359   16   A guy grabbing a stack of bananas from a pile on the side of the road.\n","A guy grabbing a stack of bananas from a pile on the side\n","360   8   A man standing over some bundles of bananas\n","A man standing over some bundles of bananas\n","361   11   A restroom has a toilet and a decorative sun wall plaque.\n","A restroom has a toilet and a decorative sun wall plaque.\n","362   9   A picture of a very nice and white toilet.\n","A picture of a very nice and white toilet.\n","363   19   A bathroom with a white toilet in the middle of the wall and a sun wall decor above it.\n","A bathroom with a white toilet in the middle of the wall and\n","364   10   A bath room with a sun decoration above the toilet\n","A bath room with a sun decoration above the toilet\n","365   9   A bathroom has a sun decal on the wall.\n","A bathroom has a sun decal on the wall.\n","366   8   A small closed toilet in a cramped space.\n","A small closed toilet in a cramped space.\n","367   10   A tan toilet and sink combination in a small room.\n","A tan toilet and sink combination in a small room.\n","368   11   This is an advanced toilet with a sink and control panel.\n","This is an advanced toilet with a sink and control panel.\n","369   9   A close-up picture of a toilet with a fountain.\n","A close-up picture of a toilet with a fountain.\n","370   8   Off white toilet with a faucet and controls. \n","Off white toilet with a faucet and controls. \n","371   8   some horses grazing in front of a church\n","some horses grazing in front of a church\n","372   13   Three horses on a green pasture with an old building in the background.\n","Three horses on a green pasture with an old building in the background.\n","373   10   A filed with brown horses standing next to a church.\n","A filed with brown horses standing next to a church.\n","374   13   Three horses are grazing in a field in front of an old church.\n","Three horses are grazing in a field in front of an old church.\n","375   11   horses eating grass in a field with trees in the background\n","horses eating grass in a field with trees in the background\n","376   9   A horse drawn carriage is parked along the curb.\n","A horse drawn carriage is parked along the curb.\n","377   12   A horse drawn carriage is in front of an old large building.\n","A horse drawn carriage is in front of an old large building.\n","378   13   A pair of horses carrying a carriage that is parked by a street.\n","A pair of horses carrying a carriage that is parked by a street.\n","379   8   A horse drawn carriage parked on the street.\n","A horse drawn carriage parked on the street.\n","380   11   People walking pass a horse drawn carriage sitting at the curb\n","People walking pass a horse drawn carriage sitting at the curb\n","381   11   a toilet on the ground outdoors in front of a house\n","a toilet on the ground outdoors in front of a house\n","382   12   A toilet sitting in the middle of the road beside a home. \n","A toilet sitting in the middle of the road beside a home. \n","383   15   A toilet on the pavement in front of a house, the tank lid lying discarded.\n","A toilet on the pavement in front of a house, the tank lid\n","384   10   An old looking toilet sitting out on the side walk.\n","An old looking toilet sitting out on the side walk.\n","385   8   A toilet sitting in front of someones house. \n","A toilet sitting in front of someones house. \n","386   9   The shiny motorcycle is being shown on a display.\n","The shiny motorcycle is being shown on a display.\n","387   8   A motor bike parked inside of a building.\n","A motor bike parked inside of a building.\n","388   8   A parked motorcycle sitting next to a crowd.\n","A parked motorcycle sitting next to a crowd.\n","389   11   White motorcycle on display inside a building with people looking on. \n","White motorcycle on display inside a building with people looking on. \n","390   9   A motorcycle sits displayed in a large shopping area.\n","A motorcycle sits displayed in a large shopping area.\n","391   10   Horses graze in front of a large building amid snow.\n","Horses graze in front of a large building amid snow.\n","392   9   Several horses eating grass around melting a snow spot\n","Several horses eating grass around melting a snow spot\n","393   8   some horses eating grass by a big house\n","some horses eating grass by a big house\n","394   9   Horses grazing in a field by a large home.\n","Horses grazing in a field by a large home.\n","395   10   A couple of horses grazing in front of an estate.\n","A couple of horses grazing in front of an estate.\n","396   10   A large clock tower with a wind indicator on top.\n","A large clock tower with a wind indicator on top.\n","397   9   A very tall clock tower towering over a city.\n","A very tall clock tower towering over a city.\n","398   11   A clock tower has a weather vane on top of it.\n","A clock tower has a weather vane on top of it.\n","399   10   Blue and orange stone clock tower with a small clock.\n","Blue and orange stone clock tower with a small clock.\n","400   10   A large clock tower in front of a clear sky\n","A large clock tower in front of a clear sky\n","401   10   A motorcycle sitting on the side of a street corner.\n","A motorcycle sitting on the side of a street corner.\n","402   10   Black motorcycle parked on the side of a busy street. \n","Black motorcycle parked on the side of a busy street. \n","403   11   A motorcycle is sitting outside on the street near a tree.\n","A motorcycle is sitting outside on the street near a tree.\n","404   8   a black motorcycle is parked on a sidewalk\n","a black motorcycle is parked on a sidewalk\n","405   9   An unattended motorcycle parked along a curb on roadway.\n","An unattended motorcycle parked along a curb on roadway.\n","406   11   A small white toilet sitting next to a metal trash can.\n","A small white toilet sitting next to a metal trash can.\n","407   9   A compact toilet and shower of a small bathroom\n","A compact toilet and shower of a small bathroom\n","408   11   A close up shot of a white toilet and waste bin.\n","A close up shot of a white toilet and waste bin.\n","409   23   A small white toilette with the lid closed has a metal trash can on one side and a shower stall on the other.\n","A small white toilette with the lid closed has a metal trash can\n","410   10   a toilet bowl and a trash can in a bathroom\n","a toilet bowl and a trash can in a bathroom\n","411   11   A purple motorcycle parked in front of a red brick building.\n","A purple motorcycle parked in front of a red brick building.\n","412   11   A motorcycle that is parked in front of a brick building.\n","A motorcycle that is parked in front of a brick building.\n","413   11   A sport motorbike parked on a pavement close to a building\n","A sport motorbike parked on a pavement close to a building\n","414   8   A blur motorcycle against a red brick wall.\n","A blur motorcycle against a red brick wall.\n","415   10   A motor bike parked on the side of the street.\n","A motor bike parked on the side of the street.\n","416   10   A few people working on various computers in an office.\n","A few people working on various computers in an office.\n","417   14   A man works on a laptop placed on a table in a busy office.\n","A man works on a laptop placed on a table in a busy\n","418   11   A man is working on a laptop computer at a desk.\n","A man is working on a laptop computer at a desk.\n","419   9   A man in blue works on a laptop computer.\n","A man in blue works on a laptop computer.\n","420   9   A man sitting in front of a laptop computer.\n","A man sitting in front of a laptop computer.\n","421   9   People are walking and riding motorcycles on the street\n","People are walking and riding motorcycles on the street\n","422   10   A group of motorists pass very large buildings in asia. \n","A group of motorists pass very large buildings in asia. \n","423   10   A bunch of bikers are gathered on a city street. \n","A bunch of bikers are gathered on a city street. \n","424   17   people ride their motorcycles beside some cars, passing by an empty street with stores and apartment buildings\n","people ride their motorcycles beside some cars, passing by an empty street with\n","425   11   A view of motorcyclists riding their bikes through heavy city traffic.\n","A view of motorcyclists riding their bikes through heavy city traffic.\n","426   13   Two blue bowls of food next to a bottle of cinnamon and sugar.\n","Two blue bowls of food next to a bottle of cinnamon and sugar.\n","427   8   this is an image of cereal and milk\n","this is an image of cereal and milk\n","428   10   A desert of bananas and cinnamon in two blue bowls.\n","A desert of bananas and cinnamon in two blue bowls.\n","429   18   A blue bowl holding mashed bananas and oatmeal with a bottle of sugar and cinnamon sitting beside it.\n","A blue bowl holding mashed bananas and oatmeal with a bottle of sugar\n","430   13   A bowl of oatmeal is ready to eat with some cinnamon and sugar.\n","A bowl of oatmeal is ready to eat with some cinnamon and sugar.\n","431   9   Rows of motor bikes and helmets in a city\n","Rows of motor bikes and helmets in a city\n","432   10   A lot of motorbikes line up down a busy street.\n","A lot of motorbikes line up down a busy street.\n","433   10   Large set of motorcycles all lined up down a street.\n","Large set of motorcycles all lined up down a street.\n","434   10   A large group of motorcycles lined up on the street.\n","A large group of motorcycles lined up on the street.\n","435   11   A series of motorbikes parked in a row on a street\n","A series of motorbikes parked in a row on a street\n","436   8   Two rows of various makes of parked motorcycles.\n","Two rows of various makes of parked motorcycles.\n","437   8   a bunch of motorcycles are parked tightly together\n","a bunch of motorcycles are parked tightly together\n","438   9   Rows and columns of motorcycles in middle of street.\n","Rows and columns of motorcycles in middle of street.\n","439   8   A bunch of motorcycles that are grouped together.\n","A bunch of motorcycles that are grouped together.\n","440   8   Motorcycles are lined up outside along the street. \n","Motorcycles are lined up outside along the street. \n","441   14   A row of motorcycles parked next to each other on a lush green field.\n","A row of motorcycles parked next to each other on a lush green\n","442   8   Several motorcycles parked beneath trees in a park.\n","Several motorcycles parked beneath trees in a park.\n","443   10   THERE ARE A LOT OF MOTOR BIKES ON THE GRASS\n","THERE ARE A LOT OF MOTOR BIKES ON THE GRASS\n","444   10   a line of parked motorcycles on some grass and trees\n","a line of parked motorcycles on some grass and trees\n","445   8   A bunch of motorcycles line up near trees.\n","A bunch of motorcycles line up near trees.\n","446   10   Two people on mopeds passing in front of a building.\n","Two people on mopeds passing in front of a building.\n","447   8   two people riding scooter on a city street\n","two people riding scooter on a city street\n","448   8   People on mopeds in front of a building.\n","People on mopeds in front of a building.\n","449   8   People riding motorcycles in front of a building\n","People riding motorcycles in front of a building\n","450   12   an image of a boy on his bike riding past a building\n","an image of a boy on his bike riding past a building\n","451   8   Office space with office equipment on desk top.\n","Office space with office equipment on desk top.\n","452   8   A hope office setup of computers and printers.\n","A hope office setup of computers and printers.\n","453   10   a home office with laptop, printer, scanner, and extra monitor\n","a home office with laptop, printer, scanner, and extra monitor\n","454   10   The computer desk in the corner is by a window.\n","The computer desk in the corner is by a window.\n","455   9   A laptop, monitor, printer and tablet on a desk.\n","A laptop, monitor, printer and tablet on a desk.\n","456   21   A building wall and pair of doors that are open, along with vases of flowers on the outside of the building.\n","A building wall and pair of doors that are open, along with vases\n","457   8   a building with dirty walls and dirty doors\n","a building with dirty walls and dirty doors\n","458   10   a run down building with two planters outside the door\n","a run down building with two planters outside the door\n","459   11   a yellow and brown wall a gray door and a sign \n","a yellow and brown wall a gray door and a sign \n","460   10   A plaster external wall with multiple old paper images attached.\n","A plaster external wall with multiple old paper images attached.\n","461   10   A computer that is on a desk near a window.\n","A computer that is on a desk near a window.\n","462   13   An office desk in front of a large window overlooking a parking lot\n","An office desk in front of a large window overlooking a parking lot\n","463   9   this is a desk with a computer on it\n","this is a desk with a computer on it\n","464   10   A corner desk with a laptop and other office equipment\n","A corner desk with a laptop and other office equipment\n","465   10   A office desk,  with a view of a parking lot. \n","A office desk,  with a view of a parking lot. \n","466   11   A close-up of a one-way street with many cars and motorcycles.\n","A close-up of a one-way street with many cars and motorcycles.\n","467   11   Straight ahead view of several lanes of traffic heading this way\n","Straight ahead view of several lanes of traffic heading this way\n","468   11   Cars, motorized scooters, and other vehicles stopped on a busy road.\n","Cars, motorized scooters, and other vehicles stopped on a busy road.\n","469   10   Bikers on bikes in front of a lot of traffic\n","\n","Bikers on bikes in front of a lot of traffic\n","\n","470   11   a group of motorcycle riders stopped in traffic next to cars\n","a group of motorcycle riders stopped in traffic next to cars\n","471   13   this living room has a small tv and a lamp next to it\n","this living room has a small tv and a lamp next to it\n","472   12   A big screen television is sitting on a stand in a room.\n","A big screen television is sitting on a stand in a room.\n","473   8   A large television and table in a room.\n","A large television and table in a room.\n","474   9   Living room with flat screen TV and surround sound.\n","Living room with flat screen TV and surround sound.\n","475   10   A flat screen television in a room with yellow walls.\n","A flat screen television in a room with yellow walls.\n","476   10   A group of men riding motorcycle down a country road.\n","A group of men riding motorcycle down a country road.\n","477   8   a street with some motorcycles driving through it\n","a street with some motorcycles driving through it\n","478   10   A white car driving down the highway with three motorcycles.\n","A white car driving down the highway with three motorcycles.\n","479   9   Three motorcycles are driving swiftly behind a white car.\n","Three motorcycles are driving swiftly behind a white car.\n","480   12   a blurry image of motorcycles and a car driving along a highway\n","a blurry image of motorcycles and a car driving along a highway\n","481   12   A polie officer standing next to a  motorcycle parked on the street.\n","A polie officer standing next to a  motorcycle parked on the street.\n","482   17   A scene depicting a man sitting down on a curb and a motor cop writing a ticket\n","A scene depicting a man sitting down on a curb and a motor\n","483   12   A man and police officer on the sidewalk next to a motorcycle.\n","A man and police officer on the sidewalk next to a motorcycle.\n","484   12   A man sitting on a curb with a police officer behind him. \n","A man sitting on a curb with a police officer behind him. \n","485   8   A motorcycle police officer is writing a ticket.\n","A motorcycle police officer is writing a ticket.\n","486   9   a person riding a motorcycle on an enclosed road\n","a person riding a motorcycle on an enclosed road\n","487   9   A motorcycle and a rider on a race track\n","A motorcycle and a rider on a race track\n","488   9   A person on a motorcycle driving around a racetrack.\n","A person on a motorcycle driving around a racetrack.\n","489   10   A person riding a motor cycle down a race track.\n","A person riding a motor cycle down a race track.\n","490   8   Racer driving a high performance motorcycle on track\n","Racer driving a high performance motorcycle on track\n","491   14   A large pitcher of some beverage is on the table next to orange slices.\n","A large pitcher of some beverage is on the table next to orange\n","492   12   A cold pitcher of orange juice beside a bowl of orange slices.\n","A cold pitcher of orange juice beside a bowl of orange slices.\n","493   13   A pitcher of beer is sitting next to a cup of orange slices. \n","A pitcher of beer is sitting next to a cup of orange slices.\n","494   12   A pitcher of beer stands next to a dish containing orange slices.\n","A pitcher of beer stands next to a dish containing orange slices.\n","495   13   a pitcher of liquid sitting on a table next to some sliced oranges\n","a pitcher of liquid sitting on a table next to some sliced oranges\n","496   11   Motorcycles driving down a wide city street nearing a white building.\n","Motorcycles driving down a wide city street nearing a white building.\n","497   11   A large building with many windows and people riding scooters nearby\n","A large building with many windows and people riding scooters nearby\n","498   10   People are riding scooters down the middle of the road.\n","People are riding scooters down the middle of the road.\n","499   11   People are riding electric scooters in front of a large building.\n","People are riding electric scooters in front of a large building.\n"]}],"source":["for i in range(len(sliced_train_dataset)):\n","  sentence=sliced_train_dataset[i]['sentences']['raw']\n","  sentence_split=sentence.split()\n","  print(i,\" \",len(sentence_split),\" \",sentence)\n","  if(len(sentence_split)<standardise_len):\n","    print(sentence)\n","\n","  else:\n","    print(\" \".join(sentence_split[0:standardise_len]))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Bbd7Aobp1lUp","executionInfo":{"status":"ok","timestamp":1719814710243,"user_tz":-330,"elapsed":27,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["def standardiseAvgLength(example):\n","  sentence=example['sentences']['raw']\n","  sentence=sentence.replace('.','')\n","  sentence=sentence.replace(',','')\n","  sentence=sentence.replace(\"'\",\"\")\n","  sentence=sentence.lower()\n","  split_sentence=sentence.split()\n","  n=standardise_len-len(split_sentence)\n","  if(n>0):\n","    sentence='start '+sentence+' end'\n","    sentence=sentence + ' pad '*(n)\n","  else:\n","    sentence=\" \".join(split_sentence[0:standardise_len])\n","    sentence='start '+sentence+' end'\n","  #print(sentence)\n","  example['sentences']['raw']=sentence\n","  print(example['sentences']['raw'])\n","  return example"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ZSxQgVsb1ps1","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cb942e154e5943c39ad1040d4530c19f","20b667f856b94f929eaa60c3a85a2690","1b1dab78a9154eb5803cd6e43bdad044","ffe62f6a0831453fb080fb9d36f13eae","14fd1d662e7b4541a65aac153bf7ffdd","3e87bafc9a6d49a78ba9c661cb84240a","90bca603b626480986cfded8984fc2ab","9e4f2ef12db14629823da5eaa59ec0fd","c059b9608f2b413ab52a3370246d0543","b3df069769d7496a8e28a214be67901f","27cff1d7716d456796f052fc38df5a02"]},"executionInfo":{"status":"ok","timestamp":1719814710243,"user_tz":-330,"elapsed":26,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"af81c833-805d-4a0a-a6d5-f8e9223c4900"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb942e154e5943c39ad1040d4530c19f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["start a woman wearing a net on her head cutting a cake  end pad  pad \n","start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","start there is a woman that is cutting a white cake end pad  pad  pad \n","start a woman marking a cake with the back of a chefs knife  end pad \n","start a young boy standing in front of a computer keyboard end pad  pad  pad \n","start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","start he is listening intently to the computer at school end pad  pad  pad  pad \n","start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","start a young kid with head phones on using a computer  end pad  pad  pad \n","start a boy wearing headphones using one computer in a long row of computers end\n","start a little boy with earphones on listening to something end pad  pad  pad  pad \n","start a group of people sitting at desk using computers end pad  pad  pad  pad \n","start children sitting at computer stations on a long table end pad  pad  pad  pad \n","start a small child wearing headphones plays on the computer end pad  pad  pad  pad \n","start a man is in a kitchen making pizzas end pad  pad  pad  pad  pad \n","start man in apron standing on front of oven with pans and bakeware end pad \n","start a baker is working in the kitchen rolling dough end pad  pad  pad  pad \n","start a person standing by a stove in a kitchen end pad  pad  pad  pad \n","start a table with pies being made and a person standing near a wall end\n","start a woman in a room with a cat end pad  pad  pad  pad  pad \n","start a girl smiles as she holds a cat and wears a brightly colored end\n","start a woman is holding a cat in her kitchen end pad  pad  pad  pad \n","start a woman is working in a kitchen carrying a soft toy end pad  pad \n","start a woman is holding a cat in her kitchen end pad  pad  pad  pad \n","start a commercial stainless kitchen with a pot of food cooking  end pad  pad  pad \n","start some food sits in a pot in a kitchen  end pad  pad  pad  pad \n","start a kitchen has all stainless steel appliances and counters end pad  pad  pad  pad \n","start a kitchen with a sink and many cooking machines and a pot of end\n","start food cooks in a pot on a stove in a kitchen end pad  pad \n","start two men wearing aprons working in a commercial-style kitchen end pad  pad  pad  pad \n","start chefs preparing food in a professional metallic style kitchen end pad  pad  pad  pad \n","start two people standing around in a large kitchen end pad  pad  pad  pad  pad \n","start a commercial kitchen with two men working to prepare several plates end pad  pad \n","start two men in white shirts in a large steel kitchen end pad  pad  pad \n","start two chefs in a restaurant kitchen preparing food  end pad  pad  pad  pad  pad \n","start two cooks are cooking the food someone ordered at this restaurant end pad  pad \n","start the chef is cooking with pans on the stove next to an oven end\n","start two men that are standing in a kitchen end pad  pad  pad  pad  pad \n","start two cooks are near the stove in a stainless steel kitchen end pad  pad \n","start this is a very dark picture of a room with a shelf end pad \n","start a cluttered room with a table and shelf on the wall end pad  pad \n","start a view of a messy room with shelves on the wall end pad  pad \n","start a dark and cluttered storage area with wood walls end pad  pad  pad  pad \n","start a dim lit room consisting of many objects put together  end pad  pad  pad \n","start a kitchen filled with black appliances and lots of counter top space end pad \n","start some brown cabinets a black oven a tea kettle and a microwave end pad \n","start a small kitchen with glass and wooden cabinets end pad  pad  pad  pad  pad \n","start a modern style kitchen filled with may different items end pad  pad  pad  pad \n","start a kitchen with wooden cabinets and black appliances end pad  pad  pad  pad  pad \n","start a professional kitchen filled with sinks and appliances end pad  pad  pad  pad  pad \n","start a kitchen area with toilet and various cleaning appliances end pad  pad  pad  pad \n","start a commercial dish washing station with a toilet in it end pad  pad  pad \n","start a toilet and mop bucket in a kitchen end pad  pad  pad  pad  pad \n","start a cluttered room with a sink a toilet and in industrial mop bucket end\n","start a kitchen with wood floors and lots of furniture end pad  pad  pad  pad \n","start a beautiful open kitchen and dining room area features an island in the end\n","start a kitchen made of mostly wood with a small desk with a laptop end\n","start a very spacious room with a kitchen and dining area end pad  pad  pad \n","start a full view of an open kitchen and dining area end pad  pad  pad \n","start a woman eating vegetables in front of a stove end pad  pad  pad  pad \n","start a woman forks vegetables out of a bowl into her mouth  end pad  pad \n","start woman eating an assortment of mixed vegetables in a bowl end pad  pad  pad \n","start a young woman standing in a kitchen eats a plate of vegetables end pad \n","start a woman eating fresh vegetables from a bowl end pad  pad  pad  pad  pad \n","start a boy performing a kickflip on his skateboard on a city street end pad \n","start a man is doing a trick on a skateboard end pad  pad  pad  pad \n","start a guy jumps in the air with his skateboard beneath him end pad  pad \n","start man in all black doing a trick on his skateboard end pad  pad  pad \n","start a skateboarder flipping his board on a street end pad  pad  pad  pad  pad \n","start a kitchen with a stove microwave and refrigerator end pad  pad  pad  pad  pad \n","start a refrigerator oven and microwave sitting in a kitchen end pad  pad  pad  pad \n","start the kitchenette uses  small space to great efficiency end pad  pad  pad  pad  pad \n","start an image of a kitchen setting with black appliances end pad  pad  pad  pad \n","start a kitchen with cabinets a stove microwave and refrigerator end pad  pad  pad  pad \n","start the dining table near the kitchen has a bowl of fruit on it end\n","start a small kitchen has various appliances and a table end pad  pad  pad  pad \n","start the kitchen is clean and ready for us to see end pad  pad  pad \n","start a kitchen and dining area decorated in white end pad  pad  pad  pad  pad \n","start a kitchen that has a bowl of fruit on the table end pad  pad \n","start a group of people riding on the back of a loaded red pickup end\n","start a truck with a number of people and things in the back end pad \n","start men are crowded on the back of a small overloaded pickup truck end pad \n","start an old pick up truck over loaded with people and cargo end pad  pad \n","start a truck carries a large amount of items and a few people end pad \n","start a large boat filled with mean on wheels end pad  pad  pad  pad  pad \n","start a bunch of people aboard a boat with wheels  end pad  pad  pad  pad \n","start a boat is being rolled on a trailer end pad  pad  pad  pad  pad \n","start a large boat full of men is sitting on a cart  end pad  pad \n","start a boat full of people is on a trailer with wheels end pad  pad \n","start several young students working at a desk with multiple computers end pad  pad  pad \n","start a young man at his workstation examines the monitor of a lap top end\n","start a man is working on a laptop next to other computers end pad  pad \n","start people in a large room use multiple computers end pad  pad  pad  pad  pad \n","start a young professional is working at his laptop while his coworker is reading end\n","start a man riding an elephant into some water of a creek end pad  pad \n","start man riding an elephant into water surrounded by forest end pad  pad  pad  pad \n","start a man riding an elephant in a river end pad  pad  pad  pad  pad \n","start a man in a brown shirt rides an elephant into the water end pad \n","start a man rides an elephant into a river end pad  pad  pad  pad  pad \n","start a couple of women  are in a kitchen end pad  pad  pad  pad  pad \n","start two people standing in the kitchen of a home end pad  pad  pad  pad \n","start a group of people who are around a kitchen counter end pad  pad  pad \n","start a group of people prepare dinner in the kitchen end pad  pad  pad  pad \n","start two women in a kitchen bottles and lights end pad  pad  pad  pad  pad \n","start the woman in the kitchen is holding a huge pan end pad  pad  pad \n","start a chef carrying a large pan inside of a kitchen end pad  pad  pad \n","start a woman is holding a large pan in a kitchen  end pad  pad  pad \n","start a woman cooking in a kitchen with granite counters end pad  pad  pad  pad \n","start a woman cooking in her kitchen with a black pan end pad  pad  pad \n","start a baby is laying down with a teddy bear end pad  pad  pad  pad \n","start a baby laying in a crib with a stuffed teddy bear end pad  pad \n","start a baby wearing gloves lying next to a teddy bear end pad  pad  pad \n","start a baby stares to the left while taking a picture with a teddy end\n","start a baby lies on blue and green bedding next to a teddy bear end\n","start a person with a shopping cart on a city street  end pad  pad  pad \n","start city dwellers walk by as a homeless man begs for cash end pad  pad \n","start people walking past a homeless man begging on a city street end pad  pad \n","start a homeless man holding a cup and standing next to a shopping cart end\n","start people are walking on the street by a homeless person end pad  pad  pad \n","start two bikers one in front of a building the other in the city end\n","start two shots of men riding bicycles down city streets end pad  pad  pad  pad \n","start a man riding a bike in front of a tall building end pad  pad \n","start there are two different people riding bikes down the street end pad  pad  pad \n","start two photos of a person riding a bicycle on the side of the end\n","start a person on a skateboard and bike at a skate park end pad  pad \n","start a man on a skateboard performs a trick at the skate park end pad \n","start a skateboarder jumps into the air as he performs a skateboard trick end pad \n","start athletes performing tricks on a bmx bicycle and a skateboard end pad  pad  pad \n","start a man falls off his skateboard in a skate park end pad  pad  pad \n","start a blue bike parked on a side walk  end pad  pad  pad  pad  pad \n","start a bicycle is chained to a fixture on a city street end pad  pad \n","start a blue bicycle sits on a sidewalk near a street end pad  pad  pad \n","start a bicycle is locked up to a post end pad  pad  pad  pad  pad \n","start a bike sits parked next to a street  end pad  pad  pad  pad  pad \n","start people riding bicycles down the road approaching a bird end pad  pad  pad  pad \n","start three bicycle riders some trees and a pigeon end pad  pad  pad  pad  pad \n","start a geoup of people on bicycles coming down a street end pad  pad  pad \n","start several smiling bicycle riders approaching a colorful pigeon end pad  pad  pad  pad  pad \n","start a pigeon greets three bicyclists on a park path end pad  pad  pad  pad \n","start a toilet and a sink in small bathroom end pad  pad  pad  pad  pad \n","start a bathroom with just a toliet and a sink in it end pad  pad \n","start a small section of a bathroom with dim lighting end pad  pad  pad  pad \n","start a bathroom with a toilet sink cabinet and mirror end pad  pad  pad  pad \n","start the bathroom has been cleaned and is ready to use  end pad  pad  pad \n","start a bicycle store shows two males leaning toward a bike end pad  pad  pad \n","start a man adjust a bicycle in a bike shop with a child end pad \n","start the bike shop employee is helping a customer  end pad  pad  pad  pad  pad \n","start a man and a boy are talking about a bicycle in a store end\n","start two people in a shop looking at a bike end pad  pad  pad  pad \n","start a shower stall with interesting tile is the focal point end pad  pad  pad \n","start a full perspective of a washroom with a sink \n"," end pad  pad  pad  pad \n","start a white bathroom sink sitting under a mirror end pad  pad  pad  pad  pad \n","start i picture of a bathroom with a stand up shower stall and a end\n","start a small shower behind a small bathroom sink end pad  pad  pad  pad  pad \n","start a view of a very large bathroom with mirrored walls end pad  pad  pad \n","start a corner bathroom with two sinks and a bathtub end pad  pad  pad  pad \n","start a bathroom with a sink shower and bathtub end pad  pad  pad  pad  pad \n","start a large bathroom with cabinets mirrors and a tub end pad  pad  pad  pad \n","start this bathroom has mirrors on the doors cabinets and a bathtub in the end\n","start a long beige bathroom with a door to a bedroom and another to end\n","start a view of a bathroom with double sinks and carpeted floor end pad  pad \n","start white vanity that opens up to a bathroom with shower end pad  pad  pad \n","start a hallway with diamond patterned walls leading to a bathroom and a bedroom end\n","start a bathroom with a large sink and mirror opens onto a room with end\n","start the view of a large bathroom with a walk in closet end pad  pad \n","start a bathroom with a walk-in closet and a jacuzzi tub end pad  pad  pad \n","start a white and beige tiled bathroom and adjoining walk-in closet end pad  pad  pad \n","start white tiled bathroom with a vanity tub and white flowers end pad  pad  pad \n","start long shot of a bathroom includes closet and tub end pad  pad  pad  pad \n","start a white toilet sitting next to a green wall under a picture end pad \n","start the letter a framed in a green bathroom end pad  pad  pad  pad  pad \n","start there is a potted plant on the back of a toilet end pad  pad \n","start a white toilet and some tissue in a room end pad  pad  pad  pad \n","start a commode with a plant on it under a picture on the wall end\n","start this is an image of the inside of a nice bathroom end pad  pad \n","start a white toilet next to a walk in shower and a sink end pad \n","start a potted plant is being displayed in a bathroom end pad  pad  pad  pad \n","start a tiled bathroom with a potted plant as a center piece end pad  pad \n","start interior bathroom scene with modern furnishings including a plant end pad  pad  pad  pad \n","start two dogs are looking up while they stand near the toilet in the end\n","start two small dogs standing in a restroom next to a toilet end pad  pad \n","start two dogs looking up at a camera in a bathroom end pad  pad  pad \n","start two small lap dogs in a small bathroom end pad  pad  pad  pad  pad \n","start two small dogs stand together in a bathroom end pad  pad  pad  pad  pad \n","start a bathroom with a walk in shower currently under repair end pad  pad  pad \n","start a photograph of a bathroom undergoing major renovations end pad  pad  pad  pad  pad \n","start interior shot of bathroom in the process of remodeling end pad  pad  pad  pad \n","start picture of bathroom being remolded where sink has not been installed  end pad  pad \n","start a room under construction with an unfinished shower and plumbing for the sink end\n","start the bathroom is clean for the guests to use  end pad  pad  pad  pad \n","start a bath and a sink in a room end pad  pad  pad  pad  pad \n","start a bathroom area with tub sink and standup shower end pad  pad  pad  pad \n","start a view of a bathroom which is covered in tiles end pad  pad  pad \n","start a kitchen is shown with a tub and a sink end pad  pad  pad \n","start a white bath tub sitting next to a bathroom sink end pad  pad  pad \n","start there is a bathtub and a counter in a bathroom end pad  pad  pad \n","start a bathroom with a tub and shower and a sink end pad  pad  pad \n","start this is a shower and bathtub without a shower curtain  end pad  pad  pad \n","start a bathroom that has a mirror and a bathtub end pad  pad  pad  pad \n","start a full view of a shower with glass end pad  pad  pad  pad  pad \n","start a walk in shower with a hand held shower head end pad  pad  pad \n","start a bathroom shower stall with a shower head end pad  pad  pad  pad  pad \n","start a glass walled shower in a home bathroom end pad  pad  pad  pad  pad \n","start a see through glass shower in a bathroom  end pad  pad  pad  pad  pad \n","start a couple of buckets in a white room end pad  pad  pad  pad  pad \n","start a bathroom with no toilets and a red and green bucket  end pad  pad \n","start a shower room with two buckets tolet paper holder and soap end pad  pad \n","start a standing toilet in a bathroom next to a window end pad  pad  pad \n","start this picture looks like a janitors closet with buckets on the floor end pad \n","start blue dust pan and brush on floor next to white commode end pad  pad \n","start a toilet in a low budget bathroom with a rough floor end pad  pad \n","start a close up of a toilet and a dust pale end pad  pad  pad \n","start a clean toilet in a bathroom with a cement floor end pad  pad  pad \n","start a bathroom with a blue dustpan and broom on the floor  end pad  pad \n","start a bathroom with a sink and toilet and a window end pad  pad  pad \n","start a white bathroom with white fixtures and tile floor end pad  pad  pad  pad \n","start a small white toilet sitting next to a sink end pad  pad  pad  pad \n","start small bathroom with a toilet shower and small mirror  end pad  pad  pad  pad \n","start a bathroom contains a toilet and a sink end pad  pad  pad  pad  pad \n","start a large porcelain toilet posed with a tan flower pot  end pad  pad  pad \n","start a toilet is sitting on the ground next to a plant end pad  pad \n","start a toliet sitting on a curb in front of a house end pad  pad \n","start a white toilet sits on the front lawn end pad  pad  pad  pad  pad \n","start a white toilet sitting on a sidewalk outside end pad  pad  pad  pad  pad \n","start a gray bicycle is locked to some metal doors end pad  pad  pad  pad \n","start a bike that is padlocked to a wall end pad  pad  pad  pad  pad \n","start the bicycle is locked on the green metal wall  end pad  pad  pad  pad \n","start a bike chained to the doors of a building  end pad  pad  pad  pad \n","start a bike is locked and hanging from a door end pad  pad  pad  pad \n","start a man takes a picture in the bathroom mirror\n"," end pad  pad  pad  pad \n","start a person taking a photo in a mirror  end pad  pad  pad  pad  pad \n","start a man taking a picture of a bathroom end pad  pad  pad  pad  pad \n","start a bathroom sink sitting under a bathroom mirror end pad  pad  pad  pad  pad \n","start a man takes a picture of his reflection in a mirror  end pad  pad \n","start a bathroom with a white toilet sitting next to a bathroom sink end pad \n","start a bathroom vanity and toilet in a public restroom end pad  pad  pad  pad \n","start a view of a bathroom shows a toilet sink and mirror end pad  pad \n","start a modern looking bathroom has a toilet and a sink  end pad  pad  pad \n","start this bathroom has brown counter tops and a white toilet end pad  pad  pad \n","start a bathroom with two sinks and a shower end pad  pad  pad  pad  pad \n","start a big bathroom that has some sinks in it end pad  pad  pad  pad \n","start bathroom vanity with double sinks and large mirrors end pad  pad  pad  pad  pad \n","start a bathroom with sinks mirrors and tile flooring end pad  pad  pad  pad  pad \n","start a bathroom that has two sinks next to each other end pad  pad  pad \n","start an image of a cars driving on the highway end pad  pad  pad  pad \n","start a section of traffic coming to a stop at an intersection end pad  pad \n","start a bunch of cars sit at the intersection of a street end pad  pad \n","start this is a picture of traffic on a very busy street end pad  pad \n","start a busy intersection filled with cars in asia end pad  pad  pad  pad  pad \n","start this shot is of a crowded highway full of traffic end pad  pad  pad \n","start there are many taxi cabs on the road end pad  pad  pad  pad  pad \n","start a city street with lots of traffic and lined with buildings end pad  pad \n","start heavy city traffic all going in one direction end pad  pad  pad  pad  pad \n","start many cars stuck in traffic on a high way end pad  pad  pad  pad \n","start mirror view of a bathroom with a sink and tub end pad  pad  pad \n","start mirror reflection of sink and the tub next to it end pad  pad  pad \n","start a bathroom mirror has the reflection of the sink and bathtub end pad  pad \n","start the bathroom is very small with white fixtures  end pad  pad  pad  pad  pad \n","start a sink is shown in a small bathroom end pad  pad  pad  pad  pad \n","start a city street filled with traffic and parking lights end pad  pad  pad  pad \n","start a full view of a late evening with many cars parked on the end\n","start a very dark street with cars and many wires above end pad  pad  pad \n","start a cars break lights glow as it waits at a red-light at an end\n","start a street scene with cars street signs and many high wires end pad  pad \n","start a white toilet and a sink in a room end pad  pad  pad  pad \n","start white bathroom area with a blue and white shower curtain  end pad  pad  pad \n","start small bathroom area with a blue and white shower curtain hanging  end pad  pad \n","start a bath tub shower sitting next to a toilet end pad  pad  pad  pad \n","start a toilet with a sink and the door opened end pad  pad  pad  pad \n","start a small white bathroom with a curtain for the shower end pad  pad  pad \n","start a bathroom sink shapped like a glass bowl end pad  pad  pad  pad  pad \n","start a man that is standing with a mug in front of a mirror end\n","start a man  taking a picture of himself in a bathroom mirror end pad  pad \n","start a man standing in a bathroom taking a picture of his self in end\n","start a man taking a selfie in a little bathroom mirror  end pad  pad  pad \n","start a door opens to a bare white bathroom  end pad  pad  pad  pad  pad \n","start a varying palette of neutrals in a bathroom awaits the softening of the end\n","start a bathroom with a tile floor bathtub and shower and no shower curtain end\n","start a bathroom is shown with a shower and a toilet end pad  pad  pad \n","start a beige and white three piece bathroom with no shower curtain end pad  pad \n","start a bathroom sink sitting under a large mirror end pad  pad  pad  pad  pad \n","start a bathroom with a vanity cabinet on the wrong wall end pad  pad  pad \n","start a small beige bathroom with an additional medicine cabinet end pad  pad  pad  pad \n","start a bathroom sink with a mirror and medicine cabinet end pad  pad  pad  pad \n","start there is a bathroom with a sink and a mirror end pad  pad  pad \n","start a white urinal mounted to a bathroom wall end pad  pad  pad  pad  pad \n","start a pair of kitchen tongs on top of a urinal end pad  pad  pad \n","start bathroom urinal with kitchen tongs on top behind plumbing end pad  pad  pad  pad \n","start a urinal in a bathroom with tongs on top of it end pad  pad \n","start a metal thing is sitting on top of a urinal end pad  pad  pad \n","start a glazed donut hanging from a metal rod end pad  pad  pad  pad  pad \n","start a tasty looking treat hanging on a car antenna end pad  pad  pad  pad \n","start a donut on the antenna of a car end pad  pad  pad  pad  pad \n","start a donut put around an antenna on a car  end pad  pad  pad  pad \n","start there is a donut sitting on the car end pad  pad  pad  pad  pad \n","start an orange striped tabby cat laying on top of a red vehicles wheel end\n","start a cat with a big fluffy tail sitting on top of a car end\n","start a cat sitting on a wheel of a vehicle end pad  pad  pad  pad \n","start a cat hiding in a fender well of a car  end pad  pad  pad \n","start an orange cat hiding on the wheel of a red car end pad  pad \n","start a large jetliner flying over a traffic filled street end pad  pad  pad  pad \n","start an airplane flies low in the sky over a city street  end pad  pad \n","start an airplane flies over a street with many cars end pad  pad  pad  pad \n","start an airplane comes in to land over a road full of cars end pad \n","start the plane is flying over top of the cars end pad  pad  pad  pad \n","start a silver and red bus in parking lot next to cars end pad  pad \n","start a silver bus that is parked in a lot end pad  pad  pad  pad \n","start an older picture of a bus and other vehicles in a parking lot end\n","start an old bus parks in a parking lot with other cars and bus end\n","start an older silver metro bus in a parking lot end pad  pad  pad  pad \n","start a little kid riding a skateboard at a park end pad  pad  pad  pad \n","start a child playing on a skateboard in a park end pad  pad  pad  pad \n","start a girl stepping onto a skateboard in the playground end pad  pad  pad  pad \n","start a young lady riding a skateboard across a street end pad  pad  pad  pad \n","start the kid is skateboarding on the street while wearing a jacket end pad  pad \n","start a person on a street next to a motor bike end pad  pad  pad \n","start a cop standing next to a police bike next to a man sitting end\n","start a man sits on the sidewalk next to a police motorcycle with a end\n","start a man that is sitting down next to a cops motorcycle end pad  pad \n","start a man is sitting on the sidewalk while a police officer is doing end\n","start a man looking down with a backpack on in a house end pad  pad \n","start there is a man with book bag on his bag  end pad  pad  pad \n","start a person standing in a stone walled and floored room wearing a backpack end\n","start a guy standing in a very dark room with a small window end pad \n","start a man in a stone room possibly using the bathroom end pad  pad  pad \n","start a toilet sitting in a bathroom next to a mirror end pad  pad  pad \n","start a small bathroom with the toilet seat up end pad  pad  pad  pad  pad \n","start the toilet seat was left in the up position end pad  pad  pad  pad \n","start a door open revealing a small bathroom with sink and toilet end pad  pad \n","start a toilet and a sink in a small bathroom end pad  pad  pad  pad \n","start the shiny motorcycle has been put on display end pad  pad  pad  pad  pad \n","start the new motorcycle on display is very shiny end pad  pad  pad  pad  pad \n","start a motorcycle is parked inside of a building end pad  pad  pad  pad  pad \n","start a brand new motorcycle on display at a show end pad  pad  pad  pad \n","start the front end of a red motorcycle that is on display end pad  pad \n","start a row of white toilets sitting on top of a dirt ground end pad \n","start a bunch of dirty looking white toilets in a row outside end pad  pad \n","start there is a row of used and broken toilets in a field end pad \n","start a row of toilets with broken seat tops on the ground end pad  pad \n","start lots of broken toilets sitting out on a lawn end pad  pad  pad  pad \n","start bathroom with white fixtures tiles and corner shower end pad  pad  pad  pad  pad \n","start a white toilet sitting in a bathroom next to a shower end pad  pad \n","start bathroom shot of non transparent shower glass and toilet end pad  pad  pad  pad \n","start an all white bathroom with a shower toilet and sink end pad  pad  pad \n","start a plain white bathroom with a sink shower and toilet end pad  pad  pad \n","start a motorcycle in a driveway with a vanity license plate  end pad  pad  pad \n","start two luxurious motor bikes on the road near a hedge end pad  pad  pad \n","start a close up of a motorcycle license plate end pad  pad  pad  pad  pad \n","start a motor bike parked on the side of the road end pad  pad  pad \n","start the back of a motorcycle showing the wheel license plate and lights  end pad \n","start a picture of a wooden container with crosses on it end pad  pad  pad \n","start a close up of wood decorated with crosses end pad  pad  pad  pad  pad \n","start close up of a wooden base with crosses designed on the neck end pad \n","start a close up shut off some sort of religious object  end pad  pad  pad \n","start a shiny wooden object is decorated with crosses end pad  pad  pad  pad  pad \n","start there is a man picking bananas next to a street end pad  pad  pad \n","start a man standing over several bunches of green bananas end pad  pad  pad  pad \n","start a man attending to several bundles of bananas as a motorcycle rides by end\n","start a guy grabbing a stack of bananas from a pile on the side end\n","start a man standing over some bundles of bananas end pad  pad  pad  pad  pad \n","start a restroom has a toilet and a decorative sun wall plaque end pad  pad \n","start a picture of a very nice and white toilet end pad  pad  pad  pad \n","start a bathroom with a white toilet in the middle of the wall and end\n","start a bath room with a sun decoration above the toilet end pad  pad  pad \n","start a bathroom has a sun decal on the wall end pad  pad  pad  pad \n","start a small closed toilet in a cramped space end pad  pad  pad  pad  pad \n","start a tan toilet and sink combination in a small room end pad  pad  pad \n","start this is an advanced toilet with a sink and control panel end pad  pad \n","start a close-up picture of a toilet with a fountain end pad  pad  pad  pad \n","start off white toilet with a faucet and controls  end pad  pad  pad  pad  pad \n","start some horses grazing in front of a church end pad  pad  pad  pad  pad \n","start three horses on a green pasture with an old building in the background end\n","start a filed with brown horses standing next to a church end pad  pad  pad \n","start three horses are grazing in a field in front of an old church end\n","start horses eating grass in a field with trees in the background end pad  pad \n","start a horse drawn carriage is parked along the curb end pad  pad  pad  pad \n","start a horse drawn carriage is in front of an old large building end pad \n","start a pair of horses carrying a carriage that is parked by a street end\n","start a horse drawn carriage parked on the street end pad  pad  pad  pad  pad \n","start people walking pass a horse drawn carriage sitting at the curb end pad  pad \n","start a toilet on the ground outdoors in front of a house end pad  pad \n","start a toilet sitting in the middle of the road beside a home  end pad \n","start a toilet on the pavement in front of a house the tank lid end\n","start an old looking toilet sitting out on the side walk end pad  pad  pad \n","start a toilet sitting in front of someones house  end pad  pad  pad  pad  pad \n","start the shiny motorcycle is being shown on a display end pad  pad  pad  pad \n","start a motor bike parked inside of a building end pad  pad  pad  pad  pad \n","start a parked motorcycle sitting next to a crowd end pad  pad  pad  pad  pad \n","start white motorcycle on display inside a building with people looking on  end pad  pad \n","start a motorcycle sits displayed in a large shopping area end pad  pad  pad  pad \n","start horses graze in front of a large building amid snow end pad  pad  pad \n","start several horses eating grass around melting a snow spot end pad  pad  pad  pad \n","start some horses eating grass by a big house end pad  pad  pad  pad  pad \n","start horses grazing in a field by a large home end pad  pad  pad  pad \n","start a couple of horses grazing in front of an estate end pad  pad  pad \n","start a large clock tower with a wind indicator on top end pad  pad  pad \n","start a very tall clock tower towering over a city end pad  pad  pad  pad \n","start a clock tower has a weather vane on top of it end pad  pad \n","start blue and orange stone clock tower with a small clock end pad  pad  pad \n","start a large clock tower in front of a clear sky end pad  pad  pad \n","start a motorcycle sitting on the side of a street corner end pad  pad  pad \n","start black motorcycle parked on the side of a busy street  end pad  pad  pad \n","start a motorcycle is sitting outside on the street near a tree end pad  pad \n","start a black motorcycle is parked on a sidewalk end pad  pad  pad  pad  pad \n","start an unattended motorcycle parked along a curb on roadway end pad  pad  pad  pad \n","start a small white toilet sitting next to a metal trash can end pad  pad \n","start a compact toilet and shower of a small bathroom end pad  pad  pad  pad \n","start a close up shot of a white toilet and waste bin end pad  pad \n","start a small white toilette with the lid closed has a metal trash can end\n","start a toilet bowl and a trash can in a bathroom end pad  pad  pad \n","start a purple motorcycle parked in front of a red brick building end pad  pad \n","start a motorcycle that is parked in front of a brick building end pad  pad \n","start a sport motorbike parked on a pavement close to a building end pad  pad \n","start a blur motorcycle against a red brick wall end pad  pad  pad  pad  pad \n","start a motor bike parked on the side of the street end pad  pad  pad \n","start a few people working on various computers in an office end pad  pad  pad \n","start a man works on a laptop placed on a table in a busy end\n","start a man is working on a laptop computer at a desk end pad  pad \n","start a man in blue works on a laptop computer end pad  pad  pad  pad \n","start a man sitting in front of a laptop computer end pad  pad  pad  pad \n","start people are walking and riding motorcycles on the street end pad  pad  pad  pad \n","start a group of motorists pass very large buildings in asia  end pad  pad  pad \n","start a bunch of bikers are gathered on a city street  end pad  pad  pad \n","start people ride their motorcycles beside some cars passing by an empty street with end\n","start a view of motorcyclists riding their bikes through heavy city traffic end pad  pad \n","start two blue bowls of food next to a bottle of cinnamon and sugar end\n","start this is an image of cereal and milk end pad  pad  pad  pad  pad \n","start a desert of bananas and cinnamon in two blue bowls end pad  pad  pad \n","start a blue bowl holding mashed bananas and oatmeal with a bottle of sugar end\n","start a bowl of oatmeal is ready to eat with some cinnamon and sugar end\n","start rows of motor bikes and helmets in a city end pad  pad  pad  pad \n","start a lot of motorbikes line up down a busy street end pad  pad  pad \n","start large set of motorcycles all lined up down a street end pad  pad  pad \n","start a large group of motorcycles lined up on the street end pad  pad  pad \n","start a series of motorbikes parked in a row on a street end pad  pad \n","start two rows of various makes of parked motorcycles end pad  pad  pad  pad  pad \n","start a bunch of motorcycles are parked tightly together end pad  pad  pad  pad  pad \n","start rows and columns of motorcycles in middle of street end pad  pad  pad  pad \n","start a bunch of motorcycles that are grouped together end pad  pad  pad  pad  pad \n","start motorcycles are lined up outside along the street  end pad  pad  pad  pad  pad \n","start a row of motorcycles parked next to each other on a lush green end\n","start several motorcycles parked beneath trees in a park end pad  pad  pad  pad  pad \n","start there are a lot of motor bikes on the grass end pad  pad  pad \n","start a line of parked motorcycles on some grass and trees end pad  pad  pad \n","start a bunch of motorcycles line up near trees end pad  pad  pad  pad  pad \n","start two people on mopeds passing in front of a building end pad  pad  pad \n","start two people riding scooter on a city street end pad  pad  pad  pad  pad \n","start people on mopeds in front of a building end pad  pad  pad  pad  pad \n","start people riding motorcycles in front of a building end pad  pad  pad  pad  pad \n","start an image of a boy on his bike riding past a building end pad \n","start office space with office equipment on desk top end pad  pad  pad  pad  pad \n","start a hope office setup of computers and printers end pad  pad  pad  pad  pad \n","start a home office with laptop printer scanner and extra monitor end pad  pad  pad \n","start the computer desk in the corner is by a window end pad  pad  pad \n","start a laptop monitor printer and tablet on a desk end pad  pad  pad  pad \n","start a building wall and pair of doors that are open along with vases end\n","start a building with dirty walls and dirty doors end pad  pad  pad  pad  pad \n","start a run down building with two planters outside the door end pad  pad  pad \n","start a yellow and brown wall a gray door and a sign  end pad  pad \n","start a plaster external wall with multiple old paper images attached end pad  pad  pad \n","start a computer that is on a desk near a window end pad  pad  pad \n","start an office desk in front of a large window overlooking a parking lot end\n","start this is a desk with a computer on it end pad  pad  pad  pad \n","start a corner desk with a laptop and other office equipment end pad  pad  pad \n","start a office desk  with a view of a parking lot  end pad  pad  pad \n","start a close-up of a one-way street with many cars and motorcycles end pad  pad \n","start straight ahead view of several lanes of traffic heading this way end pad  pad \n","start cars motorized scooters and other vehicles stopped on a busy road end pad  pad \n","start bikers on bikes in front of a lot of traffic\n"," end pad  pad  pad \n","start a group of motorcycle riders stopped in traffic next to cars end pad  pad \n","start this living room has a small tv and a lamp next to it end\n","start a big screen television is sitting on a stand in a room end pad \n","start a large television and table in a room end pad  pad  pad  pad  pad \n","start living room with flat screen tv and surround sound end pad  pad  pad  pad \n","start a flat screen television in a room with yellow walls end pad  pad  pad \n","start a group of men riding motorcycle down a country road end pad  pad  pad \n","start a street with some motorcycles driving through it end pad  pad  pad  pad  pad \n","start a white car driving down the highway with three motorcycles end pad  pad  pad \n","start three motorcycles are driving swiftly behind a white car end pad  pad  pad  pad \n","start a blurry image of motorcycles and a car driving along a highway end pad \n","start a polie officer standing next to a  motorcycle parked on the street end pad \n","start a scene depicting a man sitting down on a curb and a motor end\n","start a man and police officer on the sidewalk next to a motorcycle end pad \n","start a man sitting on a curb with a police officer behind him  end pad \n","start a motorcycle police officer is writing a ticket end pad  pad  pad  pad  pad \n","start a person riding a motorcycle on an enclosed road end pad  pad  pad  pad \n","start a motorcycle and a rider on a race track end pad  pad  pad  pad \n","start a person on a motorcycle driving around a racetrack end pad  pad  pad  pad \n","start a person riding a motor cycle down a race track end pad  pad  pad \n","start racer driving a high performance motorcycle on track end pad  pad  pad  pad  pad \n","start a large pitcher of some beverage is on the table next to orange end\n","start a cold pitcher of orange juice beside a bowl of orange slices end pad \n","start a pitcher of beer is sitting next to a cup of orange slices end\n","start a pitcher of beer stands next to a dish containing orange slices end pad \n","start a pitcher of liquid sitting on a table next to some sliced oranges end\n","start motorcycles driving down a wide city street nearing a white building end pad  pad \n","start a large building with many windows and people riding scooters nearby end pad  pad \n","start people are riding scooters down the middle of the road end pad  pad  pad \n","start people are riding electric scooters in front of a large building end pad  pad \n"]}],"source":["processed_train_dataset=sliced_train_dataset.map(standardiseAvgLength)\n","#test_dataset=test_dataset.map(standardiseAvgLength)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2613,"status":"ok","timestamp":1719814712840,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"qaf4Wh5X1uMA","outputId":"a0641325-399e-4bd1-a692-03edec334863"},"outputs":[{"output_type":"stream","name":"stdout","text":["start a woman wearing a net on her head cutting a cake  end pad  pad  15\n","start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad  15\n","start a woman wearing a hair net cutting a large sheet cake end pad  pad  15\n","start there is a woman that is cutting a white cake end pad  pad  pad  15\n","start a woman marking a cake with the back of a chefs knife  end pad  15\n","start a young boy standing in front of a computer keyboard end pad  pad  pad  15\n","start a little boy wearing headphones and looking at a computer monitor end pad  pad  15\n","start he is listening intently to the computer at school end pad  pad  pad  pad  15\n","start a young boy stares up at the computer monitor end pad  pad  pad  pad  15\n","start a young kid with head phones on using a computer  end pad  pad  pad  15\n","start a boy wearing headphones using one computer in a long row of computers end 15\n","start a little boy with earphones on listening to something end pad  pad  pad  pad  15\n","start a group of people sitting at desk using computers end pad  pad  pad  pad  15\n","start children sitting at computer stations on a long table end pad  pad  pad  pad  15\n","start a small child wearing headphones plays on the computer end pad  pad  pad  pad  15\n","start a man is in a kitchen making pizzas end pad  pad  pad  pad  pad  15\n","start man in apron standing on front of oven with pans and bakeware end pad  15\n","start a baker is working in the kitchen rolling dough end pad  pad  pad  pad  15\n","start a person standing by a stove in a kitchen end pad  pad  pad  pad  15\n","start a table with pies being made and a person standing near a wall end 15\n","start a woman in a room with a cat end pad  pad  pad  pad  pad  15\n","start a girl smiles as she holds a cat and wears a brightly colored end 15\n","start a woman is holding a cat in her kitchen end pad  pad  pad  pad  15\n","start a woman is working in a kitchen carrying a soft toy end pad  pad  15\n","start a woman is holding a cat in her kitchen end pad  pad  pad  pad  15\n","start a commercial stainless kitchen with a pot of food cooking  end pad  pad  pad  15\n","start some food sits in a pot in a kitchen  end pad  pad  pad  pad  15\n","start a kitchen has all stainless steel appliances and counters end pad  pad  pad  pad  15\n","start a kitchen with a sink and many cooking machines and a pot of end 15\n","start food cooks in a pot on a stove in a kitchen end pad  pad  15\n","start two men wearing aprons working in a commercial-style kitchen end pad  pad  pad  pad  15\n","start chefs preparing food in a professional metallic style kitchen end pad  pad  pad  pad  15\n","start two people standing around in a large kitchen end pad  pad  pad  pad  pad  15\n","start a commercial kitchen with two men working to prepare several plates end pad  pad  15\n","start two men in white shirts in a large steel kitchen end pad  pad  pad  15\n","start two chefs in a restaurant kitchen preparing food  end pad  pad  pad  pad  pad  15\n","start two cooks are cooking the food someone ordered at this restaurant end pad  pad  15\n","start the chef is cooking with pans on the stove next to an oven end 15\n","start two men that are standing in a kitchen end pad  pad  pad  pad  pad  15\n","start two cooks are near the stove in a stainless steel kitchen end pad  pad  15\n","start this is a very dark picture of a room with a shelf end pad  15\n","start a cluttered room with a table and shelf on the wall end pad  pad  15\n","start a view of a messy room with shelves on the wall end pad  pad  15\n","start a dark and cluttered storage area with wood walls end pad  pad  pad  pad  15\n","start a dim lit room consisting of many objects put together  end pad  pad  pad  15\n","start a kitchen filled with black appliances and lots of counter top space end pad  15\n","start some brown cabinets a black oven a tea kettle and a microwave end pad  15\n","start a small kitchen with glass and wooden cabinets end pad  pad  pad  pad  pad  15\n","start a modern style kitchen filled with may different items end pad  pad  pad  pad  15\n","start a kitchen with wooden cabinets and black appliances end pad  pad  pad  pad  pad  15\n","start a professional kitchen filled with sinks and appliances end pad  pad  pad  pad  pad  15\n","start a kitchen area with toilet and various cleaning appliances end pad  pad  pad  pad  15\n","start a commercial dish washing station with a toilet in it end pad  pad  pad  15\n","start a toilet and mop bucket in a kitchen end pad  pad  pad  pad  pad  15\n","start a cluttered room with a sink a toilet and in industrial mop bucket end 15\n","start a kitchen with wood floors and lots of furniture end pad  pad  pad  pad  15\n","start a beautiful open kitchen and dining room area features an island in the end 15\n","start a kitchen made of mostly wood with a small desk with a laptop end 15\n","start a very spacious room with a kitchen and dining area end pad  pad  pad  15\n","start a full view of an open kitchen and dining area end pad  pad  pad  15\n","start a woman eating vegetables in front of a stove end pad  pad  pad  pad  15\n","start a woman forks vegetables out of a bowl into her mouth  end pad  pad  15\n","start woman eating an assortment of mixed vegetables in a bowl end pad  pad  pad  15\n","start a young woman standing in a kitchen eats a plate of vegetables end pad  15\n","start a woman eating fresh vegetables from a bowl end pad  pad  pad  pad  pad  15\n","start a boy performing a kickflip on his skateboard on a city street end pad  15\n","start a man is doing a trick on a skateboard end pad  pad  pad  pad  15\n","start a guy jumps in the air with his skateboard beneath him end pad  pad  15\n","start man in all black doing a trick on his skateboard end pad  pad  pad  15\n","start a skateboarder flipping his board on a street end pad  pad  pad  pad  pad  15\n","start a kitchen with a stove microwave and refrigerator end pad  pad  pad  pad  pad  15\n","start a refrigerator oven and microwave sitting in a kitchen end pad  pad  pad  pad  15\n","start the kitchenette uses  small space to great efficiency end pad  pad  pad  pad  pad  15\n","start an image of a kitchen setting with black appliances end pad  pad  pad  pad  15\n","start a kitchen with cabinets a stove microwave and refrigerator end pad  pad  pad  pad  15\n","start the dining table near the kitchen has a bowl of fruit on it end 15\n","start a small kitchen has various appliances and a table end pad  pad  pad  pad  15\n","start the kitchen is clean and ready for us to see end pad  pad  pad  15\n","start a kitchen and dining area decorated in white end pad  pad  pad  pad  pad  15\n","start a kitchen that has a bowl of fruit on the table end pad  pad  15\n","start a group of people riding on the back of a loaded red pickup end 15\n","start a truck with a number of people and things in the back end pad  15\n","start men are crowded on the back of a small overloaded pickup truck end pad  15\n","start an old pick up truck over loaded with people and cargo end pad  pad  15\n","start a truck carries a large amount of items and a few people end pad  15\n","start a large boat filled with mean on wheels end pad  pad  pad  pad  pad  15\n","start a bunch of people aboard a boat with wheels  end pad  pad  pad  pad  15\n","start a boat is being rolled on a trailer end pad  pad  pad  pad  pad  15\n","start a large boat full of men is sitting on a cart  end pad  pad  15\n","start a boat full of people is on a trailer with wheels end pad  pad  15\n","start several young students working at a desk with multiple computers end pad  pad  pad  15\n","start a young man at his workstation examines the monitor of a lap top end 15\n","start a man is working on a laptop next to other computers end pad  pad  15\n","start people in a large room use multiple computers end pad  pad  pad  pad  pad  15\n","start a young professional is working at his laptop while his coworker is reading end 15\n","start a man riding an elephant into some water of a creek end pad  pad  15\n","start man riding an elephant into water surrounded by forest end pad  pad  pad  pad  15\n","start a man riding an elephant in a river end pad  pad  pad  pad  pad  15\n","start a man in a brown shirt rides an elephant into the water end pad  15\n","start a man rides an elephant into a river end pad  pad  pad  pad  pad  15\n","start a couple of women  are in a kitchen end pad  pad  pad  pad  pad  15\n","start two people standing in the kitchen of a home end pad  pad  pad  pad  15\n","start a group of people who are around a kitchen counter end pad  pad  pad  15\n","start a group of people prepare dinner in the kitchen end pad  pad  pad  pad  15\n","start two women in a kitchen bottles and lights end pad  pad  pad  pad  pad  15\n","start the woman in the kitchen is holding a huge pan end pad  pad  pad  15\n","start a chef carrying a large pan inside of a kitchen end pad  pad  pad  15\n","start a woman is holding a large pan in a kitchen  end pad  pad  pad  15\n","start a woman cooking in a kitchen with granite counters end pad  pad  pad  pad  15\n","start a woman cooking in her kitchen with a black pan end pad  pad  pad  15\n","start a baby is laying down with a teddy bear end pad  pad  pad  pad  15\n","start a baby laying in a crib with a stuffed teddy bear end pad  pad  15\n","start a baby wearing gloves lying next to a teddy bear end pad  pad  pad  15\n","start a baby stares to the left while taking a picture with a teddy end 15\n","start a baby lies on blue and green bedding next to a teddy bear end 15\n","start a person with a shopping cart on a city street  end pad  pad  pad  15\n","start city dwellers walk by as a homeless man begs for cash end pad  pad  15\n","start people walking past a homeless man begging on a city street end pad  pad  15\n","start a homeless man holding a cup and standing next to a shopping cart end 15\n","start people are walking on the street by a homeless person end pad  pad  pad  15\n","start two bikers one in front of a building the other in the city end 15\n","start two shots of men riding bicycles down city streets end pad  pad  pad  pad  15\n","start a man riding a bike in front of a tall building end pad  pad  15\n","start there are two different people riding bikes down the street end pad  pad  pad  15\n","start two photos of a person riding a bicycle on the side of the end 15\n","start a person on a skateboard and bike at a skate park end pad  pad  15\n","start a man on a skateboard performs a trick at the skate park end pad  15\n","start a skateboarder jumps into the air as he performs a skateboard trick end pad  15\n","start athletes performing tricks on a bmx bicycle and a skateboard end pad  pad  pad  15\n","start a man falls off his skateboard in a skate park end pad  pad  pad  15\n","start a blue bike parked on a side walk  end pad  pad  pad  pad  pad  15\n","start a bicycle is chained to a fixture on a city street end pad  pad  15\n","start a blue bicycle sits on a sidewalk near a street end pad  pad  pad  15\n","start a bicycle is locked up to a post end pad  pad  pad  pad  pad  15\n","start a bike sits parked next to a street  end pad  pad  pad  pad  pad  15\n","start people riding bicycles down the road approaching a bird end pad  pad  pad  pad  15\n","start three bicycle riders some trees and a pigeon end pad  pad  pad  pad  pad  15\n","start a geoup of people on bicycles coming down a street end pad  pad  pad  15\n","start several smiling bicycle riders approaching a colorful pigeon end pad  pad  pad  pad  pad  15\n","start a pigeon greets three bicyclists on a park path end pad  pad  pad  pad  15\n","start a toilet and a sink in small bathroom end pad  pad  pad  pad  pad  15\n","start a bathroom with just a toliet and a sink in it end pad  pad  15\n","start a small section of a bathroom with dim lighting end pad  pad  pad  pad  15\n","start a bathroom with a toilet sink cabinet and mirror end pad  pad  pad  pad  15\n","start the bathroom has been cleaned and is ready to use  end pad  pad  pad  15\n","start a bicycle store shows two males leaning toward a bike end pad  pad  pad  15\n","start a man adjust a bicycle in a bike shop with a child end pad  15\n","start the bike shop employee is helping a customer  end pad  pad  pad  pad  pad  15\n","start a man and a boy are talking about a bicycle in a store end 15\n","start two people in a shop looking at a bike end pad  pad  pad  pad  15\n","start a shower stall with interesting tile is the focal point end pad  pad  pad  15\n","start a full perspective of a washroom with a sink \n"," end pad  pad  pad  pad  15\n","start a white bathroom sink sitting under a mirror end pad  pad  pad  pad  pad  15\n","start i picture of a bathroom with a stand up shower stall and a end 15\n","start a small shower behind a small bathroom sink end pad  pad  pad  pad  pad  15\n","start a view of a very large bathroom with mirrored walls end pad  pad  pad  15\n","start a corner bathroom with two sinks and a bathtub end pad  pad  pad  pad  15\n","start a bathroom with a sink shower and bathtub end pad  pad  pad  pad  pad  15\n","start a large bathroom with cabinets mirrors and a tub end pad  pad  pad  pad  15\n","start this bathroom has mirrors on the doors cabinets and a bathtub in the end 15\n","start a long beige bathroom with a door to a bedroom and another to end 15\n","start a view of a bathroom with double sinks and carpeted floor end pad  pad  15\n","start white vanity that opens up to a bathroom with shower end pad  pad  pad  15\n","start a hallway with diamond patterned walls leading to a bathroom and a bedroom end 15\n","start a bathroom with a large sink and mirror opens onto a room with end 15\n","start the view of a large bathroom with a walk in closet end pad  pad  15\n","start a bathroom with a walk-in closet and a jacuzzi tub end pad  pad  pad  15\n","start a white and beige tiled bathroom and adjoining walk-in closet end pad  pad  pad  15\n","start white tiled bathroom with a vanity tub and white flowers end pad  pad  pad  15\n","start long shot of a bathroom includes closet and tub end pad  pad  pad  pad  15\n","start a white toilet sitting next to a green wall under a picture end pad  15\n","start the letter a framed in a green bathroom end pad  pad  pad  pad  pad  15\n","start there is a potted plant on the back of a toilet end pad  pad  15\n","start a white toilet and some tissue in a room end pad  pad  pad  pad  15\n","start a commode with a plant on it under a picture on the wall end 15\n","start this is an image of the inside of a nice bathroom end pad  pad  15\n","start a white toilet next to a walk in shower and a sink end pad  15\n","start a potted plant is being displayed in a bathroom end pad  pad  pad  pad  15\n","start a tiled bathroom with a potted plant as a center piece end pad  pad  15\n","start interior bathroom scene with modern furnishings including a plant end pad  pad  pad  pad  15\n","start two dogs are looking up while they stand near the toilet in the end 15\n","start two small dogs standing in a restroom next to a toilet end pad  pad  15\n","start two dogs looking up at a camera in a bathroom end pad  pad  pad  15\n","start two small lap dogs in a small bathroom end pad  pad  pad  pad  pad  15\n","start two small dogs stand together in a bathroom end pad  pad  pad  pad  pad  15\n","start a bathroom with a walk in shower currently under repair end pad  pad  pad  15\n","start a photograph of a bathroom undergoing major renovations end pad  pad  pad  pad  pad  15\n","start interior shot of bathroom in the process of remodeling end pad  pad  pad  pad  15\n","start picture of bathroom being remolded where sink has not been installed  end pad  pad  15\n","start a room under construction with an unfinished shower and plumbing for the sink end 15\n","start the bathroom is clean for the guests to use  end pad  pad  pad  pad  15\n","start a bath and a sink in a room end pad  pad  pad  pad  pad  15\n","start a bathroom area with tub sink and standup shower end pad  pad  pad  pad  15\n","start a view of a bathroom which is covered in tiles end pad  pad  pad  15\n","start a kitchen is shown with a tub and a sink end pad  pad  pad  15\n","start a white bath tub sitting next to a bathroom sink end pad  pad  pad  15\n","start there is a bathtub and a counter in a bathroom end pad  pad  pad  15\n","start a bathroom with a tub and shower and a sink end pad  pad  pad  15\n","start this is a shower and bathtub without a shower curtain  end pad  pad  pad  15\n","start a bathroom that has a mirror and a bathtub end pad  pad  pad  pad  15\n","start a full view of a shower with glass end pad  pad  pad  pad  pad  15\n","start a walk in shower with a hand held shower head end pad  pad  pad  15\n","start a bathroom shower stall with a shower head end pad  pad  pad  pad  pad  15\n","start a glass walled shower in a home bathroom end pad  pad  pad  pad  pad  15\n","start a see through glass shower in a bathroom  end pad  pad  pad  pad  pad  15\n","start a couple of buckets in a white room end pad  pad  pad  pad  pad  15\n","start a bathroom with no toilets and a red and green bucket  end pad  pad  15\n","start a shower room with two buckets tolet paper holder and soap end pad  pad  15\n","start a standing toilet in a bathroom next to a window end pad  pad  pad  15\n","start this picture looks like a janitors closet with buckets on the floor end pad  15\n","start blue dust pan and brush on floor next to white commode end pad  pad  15\n","start a toilet in a low budget bathroom with a rough floor end pad  pad  15\n","start a close up of a toilet and a dust pale end pad  pad  pad  15\n","start a clean toilet in a bathroom with a cement floor end pad  pad  pad  15\n","start a bathroom with a blue dustpan and broom on the floor  end pad  pad  15\n","start a bathroom with a sink and toilet and a window end pad  pad  pad  15\n","start a white bathroom with white fixtures and tile floor end pad  pad  pad  pad  15\n","start a small white toilet sitting next to a sink end pad  pad  pad  pad  15\n","start small bathroom with a toilet shower and small mirror  end pad  pad  pad  pad  15\n","start a bathroom contains a toilet and a sink end pad  pad  pad  pad  pad  15\n","start a large porcelain toilet posed with a tan flower pot  end pad  pad  pad  15\n","start a toilet is sitting on the ground next to a plant end pad  pad  15\n","start a toliet sitting on a curb in front of a house end pad  pad  15\n","start a white toilet sits on the front lawn end pad  pad  pad  pad  pad  15\n","start a white toilet sitting on a sidewalk outside end pad  pad  pad  pad  pad  15\n","start a gray bicycle is locked to some metal doors end pad  pad  pad  pad  15\n","start a bike that is padlocked to a wall end pad  pad  pad  pad  pad  15\n","start the bicycle is locked on the green metal wall  end pad  pad  pad  pad  15\n","start a bike chained to the doors of a building  end pad  pad  pad  pad  15\n","start a bike is locked and hanging from a door end pad  pad  pad  pad  15\n","start a man takes a picture in the bathroom mirror\n"," end pad  pad  pad  pad  15\n","start a person taking a photo in a mirror  end pad  pad  pad  pad  pad  15\n","start a man taking a picture of a bathroom end pad  pad  pad  pad  pad  15\n","start a bathroom sink sitting under a bathroom mirror end pad  pad  pad  pad  pad  15\n","start a man takes a picture of his reflection in a mirror  end pad  pad  15\n","start a bathroom with a white toilet sitting next to a bathroom sink end pad  15\n","start a bathroom vanity and toilet in a public restroom end pad  pad  pad  pad  15\n","start a view of a bathroom shows a toilet sink and mirror end pad  pad  15\n","start a modern looking bathroom has a toilet and a sink  end pad  pad  pad  15\n","start this bathroom has brown counter tops and a white toilet end pad  pad  pad  15\n","start a bathroom with two sinks and a shower end pad  pad  pad  pad  pad  15\n","start a big bathroom that has some sinks in it end pad  pad  pad  pad  15\n","start bathroom vanity with double sinks and large mirrors end pad  pad  pad  pad  pad  15\n","start a bathroom with sinks mirrors and tile flooring end pad  pad  pad  pad  pad  15\n","start a bathroom that has two sinks next to each other end pad  pad  pad  15\n","start an image of a cars driving on the highway end pad  pad  pad  pad  15\n","start a section of traffic coming to a stop at an intersection end pad  pad  15\n","start a bunch of cars sit at the intersection of a street end pad  pad  15\n","start this is a picture of traffic on a very busy street end pad  pad  15\n","start a busy intersection filled with cars in asia end pad  pad  pad  pad  pad  15\n","start this shot is of a crowded highway full of traffic end pad  pad  pad  15\n","start there are many taxi cabs on the road end pad  pad  pad  pad  pad  15\n","start a city street with lots of traffic and lined with buildings end pad  pad  15\n","start heavy city traffic all going in one direction end pad  pad  pad  pad  pad  15\n","start many cars stuck in traffic on a high way end pad  pad  pad  pad  15\n","start mirror view of a bathroom with a sink and tub end pad  pad  pad  15\n","start mirror reflection of sink and the tub next to it end pad  pad  pad  15\n","start a bathroom mirror has the reflection of the sink and bathtub end pad  pad  15\n","start the bathroom is very small with white fixtures  end pad  pad  pad  pad  pad  15\n","start a sink is shown in a small bathroom end pad  pad  pad  pad  pad  15\n","start a city street filled with traffic and parking lights end pad  pad  pad  pad  15\n","start a full view of a late evening with many cars parked on the end 15\n","start a very dark street with cars and many wires above end pad  pad  pad  15\n","start a cars break lights glow as it waits at a red-light at an end 15\n","start a street scene with cars street signs and many high wires end pad  pad  15\n","start a white toilet and a sink in a room end pad  pad  pad  pad  15\n","start white bathroom area with a blue and white shower curtain  end pad  pad  pad  15\n","start small bathroom area with a blue and white shower curtain hanging  end pad  pad  15\n","start a bath tub shower sitting next to a toilet end pad  pad  pad  pad  15\n","start a toilet with a sink and the door opened end pad  pad  pad  pad  15\n","start a small white bathroom with a curtain for the shower end pad  pad  pad  15\n","start a bathroom sink shapped like a glass bowl end pad  pad  pad  pad  pad  15\n","start a man that is standing with a mug in front of a mirror end 15\n","start a man  taking a picture of himself in a bathroom mirror end pad  pad  15\n","start a man standing in a bathroom taking a picture of his self in end 15\n","start a man taking a selfie in a little bathroom mirror  end pad  pad  pad  15\n","start a door opens to a bare white bathroom  end pad  pad  pad  pad  pad  15\n","start a varying palette of neutrals in a bathroom awaits the softening of the end 15\n","start a bathroom with a tile floor bathtub and shower and no shower curtain end 15\n","start a bathroom is shown with a shower and a toilet end pad  pad  pad  15\n","start a beige and white three piece bathroom with no shower curtain end pad  pad  15\n","start a bathroom sink sitting under a large mirror end pad  pad  pad  pad  pad  15\n","start a bathroom with a vanity cabinet on the wrong wall end pad  pad  pad  15\n","start a small beige bathroom with an additional medicine cabinet end pad  pad  pad  pad  15\n","start a bathroom sink with a mirror and medicine cabinet end pad  pad  pad  pad  15\n","start there is a bathroom with a sink and a mirror end pad  pad  pad  15\n","start a white urinal mounted to a bathroom wall end pad  pad  pad  pad  pad  15\n","start a pair of kitchen tongs on top of a urinal end pad  pad  pad  15\n","start bathroom urinal with kitchen tongs on top behind plumbing end pad  pad  pad  pad  15\n","start a urinal in a bathroom with tongs on top of it end pad  pad  15\n","start a metal thing is sitting on top of a urinal end pad  pad  pad  15\n","start a glazed donut hanging from a metal rod end pad  pad  pad  pad  pad  15\n","start a tasty looking treat hanging on a car antenna end pad  pad  pad  pad  15\n","start a donut on the antenna of a car end pad  pad  pad  pad  pad  15\n","start a donut put around an antenna on a car  end pad  pad  pad  pad  15\n","start there is a donut sitting on the car end pad  pad  pad  pad  pad  15\n","start an orange striped tabby cat laying on top of a red vehicles wheel end 15\n","start a cat with a big fluffy tail sitting on top of a car end 15\n","start a cat sitting on a wheel of a vehicle end pad  pad  pad  pad  15\n","start a cat hiding in a fender well of a car  end pad  pad  pad  15\n","start an orange cat hiding on the wheel of a red car end pad  pad  15\n","start a large jetliner flying over a traffic filled street end pad  pad  pad  pad  15\n","start an airplane flies low in the sky over a city street  end pad  pad  15\n","start an airplane flies over a street with many cars end pad  pad  pad  pad  15\n","start an airplane comes in to land over a road full of cars end pad  15\n","start the plane is flying over top of the cars end pad  pad  pad  pad  15\n","start a silver and red bus in parking lot next to cars end pad  pad  15\n","start a silver bus that is parked in a lot end pad  pad  pad  pad  15\n","start an older picture of a bus and other vehicles in a parking lot end 15\n","start an old bus parks in a parking lot with other cars and bus end 15\n","start an older silver metro bus in a parking lot end pad  pad  pad  pad  15\n","start a little kid riding a skateboard at a park end pad  pad  pad  pad  15\n","start a child playing on a skateboard in a park end pad  pad  pad  pad  15\n","start a girl stepping onto a skateboard in the playground end pad  pad  pad  pad  15\n","start a young lady riding a skateboard across a street end pad  pad  pad  pad  15\n","start the kid is skateboarding on the street while wearing a jacket end pad  pad  15\n","start a person on a street next to a motor bike end pad  pad  pad  15\n","start a cop standing next to a police bike next to a man sitting end 15\n","start a man sits on the sidewalk next to a police motorcycle with a end 15\n","start a man that is sitting down next to a cops motorcycle end pad  pad  15\n","start a man is sitting on the sidewalk while a police officer is doing end 15\n","start a man looking down with a backpack on in a house end pad  pad  15\n","start there is a man with book bag on his bag  end pad  pad  pad  15\n","start a person standing in a stone walled and floored room wearing a backpack end 15\n","start a guy standing in a very dark room with a small window end pad  15\n","start a man in a stone room possibly using the bathroom end pad  pad  pad  15\n","start a toilet sitting in a bathroom next to a mirror end pad  pad  pad  15\n","start a small bathroom with the toilet seat up end pad  pad  pad  pad  pad  15\n","start the toilet seat was left in the up position end pad  pad  pad  pad  15\n","start a door open revealing a small bathroom with sink and toilet end pad  pad  15\n","start a toilet and a sink in a small bathroom end pad  pad  pad  pad  15\n","start the shiny motorcycle has been put on display end pad  pad  pad  pad  pad  15\n","start the new motorcycle on display is very shiny end pad  pad  pad  pad  pad  15\n","start a motorcycle is parked inside of a building end pad  pad  pad  pad  pad  15\n","start a brand new motorcycle on display at a show end pad  pad  pad  pad  15\n","start the front end of a red motorcycle that is on display end pad  pad  15\n","start a row of white toilets sitting on top of a dirt ground end pad  15\n","start a bunch of dirty looking white toilets in a row outside end pad  pad  15\n","start there is a row of used and broken toilets in a field end pad  15\n","start a row of toilets with broken seat tops on the ground end pad  pad  15\n","start lots of broken toilets sitting out on a lawn end pad  pad  pad  pad  15\n","start bathroom with white fixtures tiles and corner shower end pad  pad  pad  pad  pad  15\n","start a white toilet sitting in a bathroom next to a shower end pad  pad  15\n","start bathroom shot of non transparent shower glass and toilet end pad  pad  pad  pad  15\n","start an all white bathroom with a shower toilet and sink end pad  pad  pad  15\n","start a plain white bathroom with a sink shower and toilet end pad  pad  pad  15\n","start a motorcycle in a driveway with a vanity license plate  end pad  pad  pad  15\n","start two luxurious motor bikes on the road near a hedge end pad  pad  pad  15\n","start a close up of a motorcycle license plate end pad  pad  pad  pad  pad  15\n","start a motor bike parked on the side of the road end pad  pad  pad  15\n","start the back of a motorcycle showing the wheel license plate and lights  end pad  15\n","start a picture of a wooden container with crosses on it end pad  pad  pad  15\n","start a close up of wood decorated with crosses end pad  pad  pad  pad  pad  15\n","start close up of a wooden base with crosses designed on the neck end pad  15\n","start a close up shut off some sort of religious object  end pad  pad  pad  15\n","start a shiny wooden object is decorated with crosses end pad  pad  pad  pad  pad  15\n","start there is a man picking bananas next to a street end pad  pad  pad  15\n","start a man standing over several bunches of green bananas end pad  pad  pad  pad  15\n","start a man attending to several bundles of bananas as a motorcycle rides by end 15\n","start a guy grabbing a stack of bananas from a pile on the side end 15\n","start a man standing over some bundles of bananas end pad  pad  pad  pad  pad  15\n","start a restroom has a toilet and a decorative sun wall plaque end pad  pad  15\n","start a picture of a very nice and white toilet end pad  pad  pad  pad  15\n","start a bathroom with a white toilet in the middle of the wall and end 15\n","start a bath room with a sun decoration above the toilet end pad  pad  pad  15\n","start a bathroom has a sun decal on the wall end pad  pad  pad  pad  15\n","start a small closed toilet in a cramped space end pad  pad  pad  pad  pad  15\n","start a tan toilet and sink combination in a small room end pad  pad  pad  15\n","start this is an advanced toilet with a sink and control panel end pad  pad  15\n","start a close-up picture of a toilet with a fountain end pad  pad  pad  pad  15\n","start off white toilet with a faucet and controls  end pad  pad  pad  pad  pad  15\n","start some horses grazing in front of a church end pad  pad  pad  pad  pad  15\n","start three horses on a green pasture with an old building in the background end 15\n","start a filed with brown horses standing next to a church end pad  pad  pad  15\n","start three horses are grazing in a field in front of an old church end 15\n","start horses eating grass in a field with trees in the background end pad  pad  15\n","start a horse drawn carriage is parked along the curb end pad  pad  pad  pad  15\n","start a horse drawn carriage is in front of an old large building end pad  15\n","start a pair of horses carrying a carriage that is parked by a street end 15\n","start a horse drawn carriage parked on the street end pad  pad  pad  pad  pad  15\n","start people walking pass a horse drawn carriage sitting at the curb end pad  pad  15\n","start a toilet on the ground outdoors in front of a house end pad  pad  15\n","start a toilet sitting in the middle of the road beside a home  end pad  15\n","start a toilet on the pavement in front of a house the tank lid end 15\n","start an old looking toilet sitting out on the side walk end pad  pad  pad  15\n","start a toilet sitting in front of someones house  end pad  pad  pad  pad  pad  15\n","start the shiny motorcycle is being shown on a display end pad  pad  pad  pad  15\n","start a motor bike parked inside of a building end pad  pad  pad  pad  pad  15\n","start a parked motorcycle sitting next to a crowd end pad  pad  pad  pad  pad  15\n","start white motorcycle on display inside a building with people looking on  end pad  pad  15\n","start a motorcycle sits displayed in a large shopping area end pad  pad  pad  pad  15\n","start horses graze in front of a large building amid snow end pad  pad  pad  15\n","start several horses eating grass around melting a snow spot end pad  pad  pad  pad  15\n","start some horses eating grass by a big house end pad  pad  pad  pad  pad  15\n","start horses grazing in a field by a large home end pad  pad  pad  pad  15\n","start a couple of horses grazing in front of an estate end pad  pad  pad  15\n","start a large clock tower with a wind indicator on top end pad  pad  pad  15\n","start a very tall clock tower towering over a city end pad  pad  pad  pad  15\n","start a clock tower has a weather vane on top of it end pad  pad  15\n","start blue and orange stone clock tower with a small clock end pad  pad  pad  15\n","start a large clock tower in front of a clear sky end pad  pad  pad  15\n","start a motorcycle sitting on the side of a street corner end pad  pad  pad  15\n","start black motorcycle parked on the side of a busy street  end pad  pad  pad  15\n","start a motorcycle is sitting outside on the street near a tree end pad  pad  15\n","start a black motorcycle is parked on a sidewalk end pad  pad  pad  pad  pad  15\n","start an unattended motorcycle parked along a curb on roadway end pad  pad  pad  pad  15\n","start a small white toilet sitting next to a metal trash can end pad  pad  15\n","start a compact toilet and shower of a small bathroom end pad  pad  pad  pad  15\n","start a close up shot of a white toilet and waste bin end pad  pad  15\n","start a small white toilette with the lid closed has a metal trash can end 15\n","start a toilet bowl and a trash can in a bathroom end pad  pad  pad  15\n","start a purple motorcycle parked in front of a red brick building end pad  pad  15\n","start a motorcycle that is parked in front of a brick building end pad  pad  15\n","start a sport motorbike parked on a pavement close to a building end pad  pad  15\n","start a blur motorcycle against a red brick wall end pad  pad  pad  pad  pad  15\n","start a motor bike parked on the side of the street end pad  pad  pad  15\n","start a few people working on various computers in an office end pad  pad  pad  15\n","start a man works on a laptop placed on a table in a busy end 15\n","start a man is working on a laptop computer at a desk end pad  pad  15\n","start a man in blue works on a laptop computer end pad  pad  pad  pad  15\n","start a man sitting in front of a laptop computer end pad  pad  pad  pad  15\n","start people are walking and riding motorcycles on the street end pad  pad  pad  pad  15\n","start a group of motorists pass very large buildings in asia  end pad  pad  pad  15\n","start a bunch of bikers are gathered on a city street  end pad  pad  pad  15\n","start people ride their motorcycles beside some cars passing by an empty street with end 15\n","start a view of motorcyclists riding their bikes through heavy city traffic end pad  pad  15\n","start two blue bowls of food next to a bottle of cinnamon and sugar end 15\n","start this is an image of cereal and milk end pad  pad  pad  pad  pad  15\n","start a desert of bananas and cinnamon in two blue bowls end pad  pad  pad  15\n","start a blue bowl holding mashed bananas and oatmeal with a bottle of sugar end 15\n","start a bowl of oatmeal is ready to eat with some cinnamon and sugar end 15\n","start rows of motor bikes and helmets in a city end pad  pad  pad  pad  15\n","start a lot of motorbikes line up down a busy street end pad  pad  pad  15\n","start large set of motorcycles all lined up down a street end pad  pad  pad  15\n","start a large group of motorcycles lined up on the street end pad  pad  pad  15\n","start a series of motorbikes parked in a row on a street end pad  pad  15\n","start two rows of various makes of parked motorcycles end pad  pad  pad  pad  pad  15\n","start a bunch of motorcycles are parked tightly together end pad  pad  pad  pad  pad  15\n","start rows and columns of motorcycles in middle of street end pad  pad  pad  pad  15\n","start a bunch of motorcycles that are grouped together end pad  pad  pad  pad  pad  15\n","start motorcycles are lined up outside along the street  end pad  pad  pad  pad  pad  15\n","start a row of motorcycles parked next to each other on a lush green end 15\n","start several motorcycles parked beneath trees in a park end pad  pad  pad  pad  pad  15\n","start there are a lot of motor bikes on the grass end pad  pad  pad  15\n","start a line of parked motorcycles on some grass and trees end pad  pad  pad  15\n","start a bunch of motorcycles line up near trees end pad  pad  pad  pad  pad  15\n","start two people on mopeds passing in front of a building end pad  pad  pad  15\n","start two people riding scooter on a city street end pad  pad  pad  pad  pad  15\n","start people on mopeds in front of a building end pad  pad  pad  pad  pad  15\n","start people riding motorcycles in front of a building end pad  pad  pad  pad  pad  15\n","start an image of a boy on his bike riding past a building end pad  15\n","start office space with office equipment on desk top end pad  pad  pad  pad  pad  15\n","start a hope office setup of computers and printers end pad  pad  pad  pad  pad  15\n","start a home office with laptop printer scanner and extra monitor end pad  pad  pad  15\n","start the computer desk in the corner is by a window end pad  pad  pad  15\n","start a laptop monitor printer and tablet on a desk end pad  pad  pad  pad  15\n","start a building wall and pair of doors that are open along with vases end 15\n","start a building with dirty walls and dirty doors end pad  pad  pad  pad  pad  15\n","start a run down building with two planters outside the door end pad  pad  pad  15\n","start a yellow and brown wall a gray door and a sign  end pad  pad  15\n","start a plaster external wall with multiple old paper images attached end pad  pad  pad  15\n","start a computer that is on a desk near a window end pad  pad  pad  15\n","start an office desk in front of a large window overlooking a parking lot end 15\n","start this is a desk with a computer on it end pad  pad  pad  pad  15\n","start a corner desk with a laptop and other office equipment end pad  pad  pad  15\n","start a office desk  with a view of a parking lot  end pad  pad  pad  15\n","start a close-up of a one-way street with many cars and motorcycles end pad  pad  15\n","start straight ahead view of several lanes of traffic heading this way end pad  pad  15\n","start cars motorized scooters and other vehicles stopped on a busy road end pad  pad  15\n","start bikers on bikes in front of a lot of traffic\n"," end pad  pad  pad  15\n","start a group of motorcycle riders stopped in traffic next to cars end pad  pad  15\n","start this living room has a small tv and a lamp next to it end 15\n","start a big screen television is sitting on a stand in a room end pad  15\n","start a large television and table in a room end pad  pad  pad  pad  pad  15\n","start living room with flat screen tv and surround sound end pad  pad  pad  pad  15\n","start a flat screen television in a room with yellow walls end pad  pad  pad  15\n","start a group of men riding motorcycle down a country road end pad  pad  pad  15\n","start a street with some motorcycles driving through it end pad  pad  pad  pad  pad  15\n","start a white car driving down the highway with three motorcycles end pad  pad  pad  15\n","start three motorcycles are driving swiftly behind a white car end pad  pad  pad  pad  15\n","start a blurry image of motorcycles and a car driving along a highway end pad  15\n","start a polie officer standing next to a  motorcycle parked on the street end pad  15\n","start a scene depicting a man sitting down on a curb and a motor end 15\n","start a man and police officer on the sidewalk next to a motorcycle end pad  15\n","start a man sitting on a curb with a police officer behind him  end pad  15\n","start a motorcycle police officer is writing a ticket end pad  pad  pad  pad  pad  15\n","start a person riding a motorcycle on an enclosed road end pad  pad  pad  pad  15\n","start a motorcycle and a rider on a race track end pad  pad  pad  pad  15\n","start a person on a motorcycle driving around a racetrack end pad  pad  pad  pad  15\n","start a person riding a motor cycle down a race track end pad  pad  pad  15\n","start racer driving a high performance motorcycle on track end pad  pad  pad  pad  pad  15\n","start a large pitcher of some beverage is on the table next to orange end 15\n","start a cold pitcher of orange juice beside a bowl of orange slices end pad  15\n","start a pitcher of beer is sitting next to a cup of orange slices end 15\n","start a pitcher of beer stands next to a dish containing orange slices end pad  15\n","start a pitcher of liquid sitting on a table next to some sliced oranges end 15\n","start motorcycles driving down a wide city street nearing a white building end pad  pad  15\n","start a large building with many windows and people riding scooters nearby end pad  pad  15\n","start people are riding scooters down the middle of the road end pad  pad  pad  15\n","start people are riding electric scooters in front of a large building end pad  pad  15\n"]}],"source":["for  i in range(len(processed_train_dataset)):\n","  sentence=processed_train_dataset[i]['sentences']['raw']\n","  print(sentence,len(sentence.split()))"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"78C3emXZ24bv","executionInfo":{"status":"ok","timestamp":1719814712840,"user_tz":-330,"elapsed":28,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["\n","embed_len = 100\n","vocab_len = my_embedding_layer.weight.shape[0]\n","#vocab_len = np.shape(embedding_matrix_vocab)[0]\n","img_input_len = 4096\n","input_len = 1700 # for horizontal stacking\n","#input_len = 100 # for vertical stacking\n","cap_len = 15\n","max_epochs = 100"]},{"cell_type":"markdown","metadata":{"id":"y-mWx70n-y1t"},"source":[" *Creating an embedding layer initialising it with glove embeddings to be finetuned later*"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"NXLQFh9P6U6D","executionInfo":{"status":"ok","timestamp":1719814712841,"user_tz":-330,"elapsed":28,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["\n","def CustomEmbedding():\n","  my_embeddings = torch.nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix_vocab).float())\n","  return my_embeddings"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1719814712841,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"xRbTMI6b66Y8","outputId":"f995f1a1-e3c3-4bd3-ba65-944b45352fcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([825, 100])\n"]}],"source":["my_embeddings = CustomEmbedding()                         #declaring our custom embedding layer\n","print(my_embeddings.weight.shape)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"_ZAkC9Ee2-ip","executionInfo":{"status":"ok","timestamp":1719814712842,"user_tz":-330,"elapsed":11,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["class CustomCocoDataset(Dataset):\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","        #self.transform = transform\n","        self.transform = T.Compose([\n","                      T.Resize((224, 224)),\n","                      T.ToTensor(),\n","                      #T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","                    ])\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        example = self.dataset[idx]\n","        #image = Image.fromarray(example[\"image\"])\n","        image=self.dataset[idx]['image']\n","        caption=self.dataset[idx]['sentences']['raw']\n","\n","        '''\n","        if self.transform:\n","            image = self.transform(image)\n","        '''\n","        image=self.transform(image)\n","\n","\n","        return image,caption"]},{"cell_type":"markdown","metadata":{"id":"jefc-rdT_aSt"},"source":["*When we are processing the caption embeddings if the word embeddings are not finetuned later (like using only Glove Embeddings) then using list in the Custom Collate function is fine but if we intend to further finetune word embeddings later the we can't use list because turns out pytorch does not register list as parametre in nn.module and hence could not use autograd on it so we need to use ParametreList in its place which again does not support torch.cat so we have to do it manually*"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"4E5k2JCf5WWg","executionInfo":{"status":"ok","timestamp":1719814712842,"user_tz":-330,"elapsed":10,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["#detaching the image embedding process from custom collate to finetune the image encoder at each epoch during training\n","##this custom collate returns just the transformed image which will be encoded later on (in the  train  func) so that it can be encoded with the recent trained version of vgg16\n","###this collate uses only Glove embeddings which  are finetuned during training\n","def NewcustomCollate2(batch):\n","\n","\n","    images = torch.stack([item[0] for item in batch], dim=0)\n","    images = torch.tensor(images).to(device)\n","\n","\n","    captions=[item[1] for item in batch]\n","\n","\n","    PositionalEncodingModel=Summer(PositionalEncoding1D(100))\n","\n","    caption_embeddings_list=list()\n","    caption_one_hot_embeddings_list=list()\n","    percaption_onehot_embeddings_list=list()\n","    for  caption in captions:\n","      #embeddings = nn.ParameterList()\n","      words=caption.split()\n","      caption_one_hot_embeddings_list=list()\n","      for word in caption.split():\n","        idx = torch.tensor([vocab[word]], dtype=torch.long)\n","        one_hot_vector = F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n","        caption_one_hot_embeddings_list.append(one_hot_vector)\n","        #embed = my_embeddings(idx)\n","        #embeddings.append(torch.FloatTensor(embed))\n","\n","        #single_caption_embeddings=torch.cat([embeddings[0]],dim=1)\n","        #for i in range(1,len(embeddings)):\n","         #   single_caption_embeddings = torch.cat([single_caption_embeddings,embeddings[i]],dim=1)\n","\n","      #caption_embeddings_list.append(single_caption_embeddings)\n","      percaption_onehot_embeddings = torch.cat(caption_one_hot_embeddings_list,dim=0)\n","      percaption_onehot_embeddings_list.append(percaption_onehot_embeddings)\n","\n","    #caption_embeddings = torch.cat(caption_embeddings_list,dim=0)\n","    caption_onehot_embeddings=torch.stack(percaption_onehot_embeddings_list,dim=0)\n","    #caption_embeddings= torch.reshape(caption_embeddings,(1,15,-1))\n","    #caption_embeddings = PositionalEncodingModel(caption_embeddings)        #(1,15,100)\n","    #caption_embeddings = torch.reshape(caption_embeddings,(1,-1))           #(1,1500)\n","\n","\n","    #return caption_embeddings,caption_onehot_embeddings,images,captions\n","    return caption_onehot_embeddings,captions,images"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"dGxov0fDOwD1","executionInfo":{"status":"ok","timestamp":1719814712843,"user_tz":-330,"elapsed":11,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["#detaching the image embedding process from custom collate to finetune the image encoder at each epoch during training\n","##this custom collate returns just the transformed image which will be encoded later on (in the  train  func) so that it can be encoded with the recent trained version of vgg16\n","###this custom collate uses trainable word embeddings\n","\n","def NewcustomCollate(batch):\n","\n","    #encoder = CNN(embed_size = 512).to(device)\n","    #encoder = Vgg16Feats(4096,100)\n","\n","    #PositionalEncodingModel=Summer(PositionalEncoding1D(100))   #for adding positional encoding\n","    #images = torch.stack([item[0] for item in batch], dim=0)\n","    captions=[item[1] for item in batch]\n","\n","    #images = torch.tensor(images).to(device)\n","    #captions = torch.tensor(captions).to(device)\n","\n","    #featuresI=encoder(images)                        #VGGNet embedding for the image\n","    #featuresI = torch.tensor(featuresI)\n","\n","    caption_embeddings_list=list()\n","    caption_one_hot_embeddings_list=list()\n","    percaption_onehot_embeddings_list=list()\n","    for  caption in captions:\n","      embeddings=list()\n","      words=caption.split()\n","      #idx=vocab.__getitem__(words[0])\n","      #one_hot_vector=F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n","      #caption_one_hot_embeddings_list.append(one_hot_vector)\n","      caption_one_hot_embeddings_list=list()\n","      for word in caption.split():\n","        idx=vocab.__getitem__(word)\n","        one_hot_vector=F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n","        caption_one_hot_embeddings_list.append(one_hot_vector)\n","        #embeds=embedding_matrix_vocab_tensor(torch.tensor(idx)) #idx must be a tensor not an int\n","        embeds=embedding_matrix_vocab[idx]\n","        #positional_embeds=PositionalEncodingModel(embeds)\n","        embeddings.append(torch.FloatTensor(embeds))\n","      single_caption_embeddings=torch.stack(embeddings)\n","      caption_embeddings_list.append(single_caption_embeddings)\n","      percaption_onehot_embeddings=torch.stack(caption_one_hot_embeddings_list)\n","      percaption_onehot_embeddings_list.append(percaption_onehot_embeddings)\n","    #print(\"caption_embeddings_list size: \",len(caption_embeddings_list))\n","    caption_embeddings=torch.stack(caption_embeddings_list)\n","    caption_onehot_embeddings=torch.stack(percaption_onehot_embeddings_list,dim=0)\n","    #print(\"caption_embeddings size: \",caption_embeddings.size())\n","\n","\n","    #return caption_embeddings,caption_onehot_embeddings,images,captions\n","    return caption_embeddings,caption_onehot_embeddings,captions"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"65U2U58dy1Op","executionInfo":{"status":"ok","timestamp":1719814712843,"user_tz":-330,"elapsed":10,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["class CustomDataLoading(Dataset):\n","\n","  def __init__(self,dataset,batch_size,num_workers):\n","    self.dataset=dataset\n","    self.batch_size=batch_size\n","    self.num_workers=num_workers\n","\n","  def loading_data(self):\n","    # Define data transformations\n","    transform = T.Compose([\n","      T.Resize((224, 224)),\n","      T.ToTensor(),\n","      #T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      ])\n","\n","\n","    custom_dataset = CustomCocoDataset(self.dataset)\n","\n","# Specify batch size and create a data loader\n","    data_loader = DataLoader(custom_dataset,shuffle=False, pin_memory=False,batch_size=self.batch_size,num_workers=self.num_workers,collate_fn=NewcustomCollate2,drop_last=True)\n","\n","    return data_loader"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"v9LWUps6y22L","executionInfo":{"status":"ok","timestamp":1719814712843,"user_tz":-330,"elapsed":10,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["train_loader=enumerate(CustomDataLoading(processed_train_dataset,batch_size=16,num_workers=0).loading_data())"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":589,"status":"ok","timestamp":1719814713423,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"DLz-90pYy6Yi","outputId":"31523b42-5e75-4402-e7ea-d80811864060"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-32-b1174a3fd3a2>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  images = torch.tensor(images).to(device)\n","<ipython-input-32-b1174a3fd3a2>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  one_hot_vector = F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n"]}],"source":["idx,(caption_onehot_embeddings,captions,images) = next(iter(train_loader))"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1719814713424,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"u_6_IgPly-la","outputId":"950a9423-7482-4b68-8c4d-6b1921635a44"},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","torch.Size([16, 15, 825])\n","torch.Size([16, 3, 224, 224])\n","['start a woman wearing a net on her head cutting a cake  end pad  pad ', 'start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad ', 'start a woman wearing a hair net cutting a large sheet cake end pad  pad ', 'start there is a woman that is cutting a white cake end pad  pad  pad ', 'start a woman marking a cake with the back of a chefs knife  end pad ', 'start a young boy standing in front of a computer keyboard end pad  pad  pad ', 'start a little boy wearing headphones and looking at a computer monitor end pad  pad ', 'start he is listening intently to the computer at school end pad  pad  pad  pad ', 'start a young boy stares up at the computer monitor end pad  pad  pad  pad ', 'start a young kid with head phones on using a computer  end pad  pad  pad ', 'start a boy wearing headphones using one computer in a long row of computers end', 'start a little boy with earphones on listening to something end pad  pad  pad  pad ', 'start a group of people sitting at desk using computers end pad  pad  pad  pad ', 'start children sitting at computer stations on a long table end pad  pad  pad  pad ', 'start a small child wearing headphones plays on the computer end pad  pad  pad  pad ', 'start a man is in a kitchen making pizzas end pad  pad  pad  pad  pad ']\n"]}],"source":["print(idx)\n","#print(caption_embeddings.size())\n","print(caption_onehot_embeddings.size())\n","print(images.size())\n","print(captions)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1719814713425,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"SRb1o5DY-MyV","outputId":"661d77ee-5b12-413d-a14a-805e24686177"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 825])\n"]}],"source":["caption_onehot_embeddings_slice = caption_onehot_embeddings[:,5,:]\n","print(caption_onehot_embeddings_slice.size())"]},{"cell_type":"markdown","metadata":{"id":"YqcH0FGB-SC6"},"source":["# **MODEL**"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"uRCUqiV098f3","executionInfo":{"status":"ok","timestamp":1719814760384,"user_tz":-330,"elapsed":5,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["# Parametres\n","# already defined above\n","batch_size = 10\n","embed_len = 512\n","#vocab_len = 290\n","input_len = 1700 # for horizontal stacking\n","#input_len = 4100\n","#input_len = 100 # for vertical stacking\n","cap_len = 15\n","max_tokens = 16\n","max_epochs = 10\n","learning_rate = 4e-6\n","#learning_rate = 1e-4\n","lr_step_size = 10\n","finetune_after = 5"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"HnQHUAtYx5aS","executionInfo":{"status":"ok","timestamp":1719814713426,"user_tz":-330,"elapsed":14,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["device = 'cpu'\n","if torch.cuda.is_available():  # take over whatever gpus are on the system\n","    device = torch.cuda.current_device()"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"L1pEHanAyYKS","executionInfo":{"status":"ok","timestamp":1719814713426,"user_tz":-330,"elapsed":14,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["st_idx=vocab.__getitem__('start')"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"5fsWtdj5lm6L","executionInfo":{"status":"ok","timestamp":1719814713426,"user_tz":-330,"elapsed":13,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["def Linear(in_features, out_features, dropout=0.):\n","    m = nn.Linear(in_features, out_features)\n","    m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n","    m.bias.data.zero_()\n","    return nn.utils.weight_norm(m)\n","\n","def Conv1d(self,in_channels, out_channels,kernel_size,pad, dropout=0):\n","      m = nn.Conv1d(in_channels, out_channels, kernel_size, padding=pad)\n","      std = math.sqrt((4 * (1.0 - dropout)) / (kernel_size * in_channels))\n","      m.weight.data.normal_(mean=0, std=std)\n","      m.bias.data.zero_()\n","      return nn.utils.weight_norm(m)"]},{"cell_type":"markdown","metadata":{"id":"IAZWb-6fnwpA"},"source":["*We can't have the trainable embedding layer defined in the trainer or somewhere outside the main model because then it will not get trained*"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"vPmHykqxiPSb","executionInfo":{"status":"ok","timestamp":1719814713426,"user_tz":-330,"elapsed":13,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["class ConvEmbGenerator(nn.Module):\n","\n","    def Conv1d(self,in_channels, out_channels,kernel_size,pad,dropout=0):\n","        m = nn.Conv1d(in_channels, out_channels, kernel_size, padding=pad)\n","        std = math.sqrt((4 * (1.0 - dropout)) / (kernel_size * in_channels))\n","        m.weight.data.normal_(mean=0, std=std)\n","        m.bias.data.zero_()\n","        return nn.utils.weight_norm(m)\n","\n","    def Embedding(self,num_embeddings, embedding_dim, padding_idx=824):\n","        m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=824)\n","        m.weight.data.normal_(0, 0.1)\n","        return m\n","\n","    def Linear(self,in_features, out_features, dropout=0.):\n","        m = nn.Linear(in_features, out_features)\n","        m.weight.data.normal_(mean=0, std=math.sqrt((1 - dropout) / in_features))\n","        m.bias.data.zero_()\n","        return nn.utils.weight_norm(m)\n","\n","\n","    def __init__(self,embedding_matrix_vocab,num_layers=3,nfeats=embed_len,dropout=0.1):\n","        super(ConvEmbGenerator, self).__init__()\n","        self.kernel_size = 5\n","        #self.kernel_size = 1\n","        #self.pad = self.kernel_size-1\n","        self.nin = 2*embed_len\n","        self.nout= embed_len\n","        self.pad = 2\n","        self.n_layers = num_layers\n","        self.nimgfeats = 4096\n","        self.nfeats = nfeats\n","        self.dropout = dropout\n","\n","        self.device = 'cpu'\n","        if torch.cuda.is_available():  # take over whatever gpus are on the system\n","          self.device = torch.cuda.current_device()\n","\n","        self.embeds = self.Embedding(vocab_len,512)                                       # this one is an embedding layer initialised with normal distribution\n","        #self.embeds = torch.nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix_vocab).float())    # this embedding layer is initialised with glove embeddings\n","        self.emb1 = self.Linear(self.nfeats,self.nfeats,dropout=self.dropout)\n","        self.imgproj1 = self.Linear(self.nimgfeats,embed_len,dropout=self.dropout)             # for residual first projecting 4096->512\n","        self.resproj = self.Linear(self.nin,self.nout,dropout=self.dropout)                # residual projection: (2x512 -> 512)\n","        self.vocabproj = self.Linear(max_tokens*self.nfeats,vocab_len,dropout=self.dropout)           # vocabulary projection: (16*512 -> vocab_len (825))\n","        self.convs = nn.ModuleList()\n","\n","        for i in range(self.n_layers):\n","\n","          self.convs.append(self.Conv1d(self.nin,2*self.nout,self.kernel_size,self.pad,self.dropout))             # first conv layer: (1024 -> 1024)\n","          self.nin = self.nout                                                                                    # rest conv layers: (512 -> 1024)\n","\n","    def GetEmbeddings(self,id,captions):\n","        PositionalEncodingModel=Summer(PositionalEncoding1D(512))\n","\n","        caption_embeddings_list=list()\n","        caption_one_hot_embeddings_list=list()\n","        percaption_onehot_embeddings_list=list()\n","        last_word_embeddings_list = list()\n","\n","        #embeddings = embeddings.to(device=self.device)\n","        for caption in captions:\n","            words=caption.split()\n","            embeddings = nn.ParameterList()\n","            caption_one_hot_embeddings_list=list()\n","            for word in words:\n","              idx = torch.tensor([vocab[word]], dtype=torch.long)\n","              #print(f'idx device: {idx.get_device()}')\n","              one_hot_vector=F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n","              caption_one_hot_embeddings_list.append(one_hot_vector)\n","              embed=self.embeds(idx.to(device=self.device))\n","              embed=embed.cpu()\n","              #print(f'embed device: {embed.get_device()}')\n","              embeddings.append(torch.FloatTensor(embed))\n","\n","            single_caption_embeddings=torch.cat([embeddings[0]],dim=1)\n","            for i in range(1,len(embeddings)):\n","                single_caption_embeddings=torch.cat([single_caption_embeddings,embeddings[i]],dim=0)                    # (15,512)\n","\n","            if idx==0:\n","                start=torch.tensor([st_idx],dtype=torch.long)\n","                last_word_emb = self.embeds(start)[None,:]\n","\n","            else:\n","                ith_word = torch.tensor(vocab[words[id-1]],dtype=torch.long,device=self.device)\n","                #print(f'ith word device: {ith_word.get_device()}')\n","                last_word_emb = self.embeds(ith_word)[None,None,:]\n","\n","\n","            #print(f'single_caption_embeddings: {single_caption_embeddings.size()}')\n","            last_word_embeddings_list.append(last_word_emb)\n","            caption_embeddings_list.append(single_caption_embeddings)\n","            #percaption_onehot_embeddings=torch.cat(caption_one_hot_embeddings_list,dim=0)\n","            #percaption_onehot_embeddings_list.append(percaption_onehot_embeddings)\n","\n","\n","\n","        last_word_embeddings = torch.stack(last_word_embeddings_list,dim=0).squeeze().unsqueeze(1)              #(batch_size,1,512)\n","        caption_embeddings = torch.stack(caption_embeddings_list,dim=0)                                         #(batch_size,15,512)\n","        #print(f'last_word_embeddings_list size: {len(last_word_embeddings_list)}')\n","        #print(f'caption_embeddings size: {caption_embeddings.size()}')\n","        #print(f'last_word_embeddings size: {last_word_embeddings.size()}')\n","        #caption_onehot_embeddings = torch.cat(percaption_onehot_embeddings_list,dim=0)\n","        #print(f'caption_embeddings size: {caption_embeddings.size()}')\n","        #caption_embeddings = torch.reshape(caption_embeddings,(-1,15,-1))\n","        caption_embeddings = PositionalEncodingModel(caption_embeddings)                                        #(batch_size,16,512)\n","\n","        caption_embeddings =  caption_embeddings.to(device=self.device)\n","        caption_embeddings = torch.cat([caption_embeddings,last_word_embeddings],dim=1)\n","        #print(f'caption_embeddings size: {caption_embeddings.size()}')\n","\n","        return caption_embeddings\n","\n","\n","    def CustomCrossEntropy(self,out,targets):\n","      CustomLoss=0\n","      print(\"in loss out shape: \",out.size())\n","      print(\"in loss target shape: \",targets.size())\n","\n","      '''\n","      for i in range(out.size()[0]):\n","        #print(f'{i} : out[i]: {out[i:]} targets[i]: {targets[i:]} loss_i: {F.cross_entropy(out[i,:], targets[i,:].float())}')\n","        CustomLoss+=F.cross_entropy(out[i,:], targets[i,:].float())\n","\n","      CustomLoss=(CustomLoss)/out.size()[0]\n","      '''\n","\n","      out1 = torch.squeeze(out)              #for removing the dummy axis\n","      CustomLoss = F.cross_entropy(out1,targets.float())\n","\n","      #print(f'inside loss out: {out.size()}')\n","      #print(f'inside loss target: {targets.size()}')\n","      print(f'inside CustomCrossEntropy out size: {out1.size()}')\n","      loss=F.cross_entropy(out1, targets.float(),reduction='mean')\n","      print(f'CustomLoss : {CustomLoss}  Loss : {loss}')\n","      return loss\n","\n","\n","    def forward(self,imgfc7,idx,caption,targets):\n","\n","        #word_emb -> (1,16,100)\n","        #word_emb = word_emb.detach()\n","\n","        word_emb = self.GetEmbeddings(idx,caption)\n","        word_emb = self.emb1(word_emb)\n","        x = word_emb.transpose(2,1)                             # x -> (batch_size,512,16)\n","\n","        y1 = F.relu(self.imgproj1(imgfc7))                      # 4096->512 for residual connection   y -> (batch_size,512)\n","        #print(f'y1 size: {y1.size()}')\n","        y1 = y1.unsqueeze(2).expand(batch_size, self.nfeats, 16)         # y -> (batch_size,512,16)\n","\n","        x = torch.cat([x,y1],dim=1)                             # residual -> (batch_size,1024,16)\n","\n","        #  in the above i  have combined image embeddings (dim=512) with each of the word embeddings (dim=512) to make the dim -> (1,1024,16) for the first conv layer\n","\n","        # now the below loop handles the residual connection issue for multiple conv layers\n","\n","        for i,conv in enumerate(self.convs):\n","\n","          # for the residual to first conv layer we apply a resproj to halve the input dimension (1024->512) to manage the effect of GLU\n","          if(i == 0):\n","              x = x.transpose(2, 1)                                 #  x -> (batch_size,16,1024)\n","              residual = self.resproj(x)                            #  residual -> (batch_size,16,512)\n","              residual = residual.transpose(2, 1)                   #  residual -> (batch_size,512,16)\n","              x = x.transpose(2, 1)                                 #  x -> (batch_size,1024,16)\n","\n","          # for rest of the layers the residual can be directly the output of the previous layer with no dimensional issue\n","          else:\n","              residual = x                                          # residual -> (1,512,16)\n","\n","\n","          x = F.dropout(x,p=self.dropout,training=self.training)\n","          x = conv(x)                                               # for  first conv layer x -(1, 1024, 16)->(1,1024,16)    for rest conv layers  x - (1,512,16) ->(1,1024,16)\n","          x = F.glu(x,dim=1)                                         # x -> (1,512,16)\n","          x = (x+residual)*math.sqrt(.5)\n","\n","\n","\n","        print(f'x before vocabproj: {x.size()}')\n","        x = torch.reshape(x,(batch_size,-1))                                   #  falttening x -> (batch_size,16*512)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = self.vocabproj(x)                                         # after vocabprojection x-> (batch_size,825)\n","        loss = self.CustomCrossEntropy(x,targets)\n","        max_indices = torch.argmax(x, dim=1)\n","\n","        return x,max_indices,loss"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31731,"status":"ok","timestamp":1719814745145,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"Vo3I4dTSOVs-","outputId":"e9b7a6d0-44ce-4bac-80bb-74302f358d29"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"teWP5aIgT3gX","executionInfo":{"status":"ok","timestamp":1719814746666,"user_tz":-330,"elapsed":6,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["class ModTrainer(CustomDataLoading):\n","\n","  def __init__(self, model, batch_size, num_workers, train_dataset, test_dataset, embedding_matrix_vocab):\n","      self.model = model\n","      self.train_dataset = train_dataset\n","      self.test_dataset =  test_dataset\n","      #self.embeds = self.Embedding(826,100)\n","      self.embeds = torch.nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix_vocab).float())\n","      self.batch_size = batch_size\n","      self.num_workers = num_workers\n","\n","      self.img_encoder = Vgg16Feats()\n","      self.device = 'cpu'\n","      if torch.cuda.is_available():  # take over whatever gpus are on the system\n","          self.device = torch.cuda.current_device()\n","\n","\n","\n","  def train(self,data):\n","    model=self.model.to(device=self.device)\n","\n","    img_encoder = self.img_encoder.to(device=self.device)\n","    img_encoder.train(True)\n","\n","    loader=CustomDataLoading(data,self.batch_size,self.num_workers).loading_data()\n","    #optimizer=optim.Adam(model.parameters(),lr=1e-3)\n","    #scheduler = StepLR(optimizer,step_size=10,gamma=0.1)\n","\n","    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n","    scheduler = StepLR(optimizer, step_size=lr_step_size, gamma=.1)\n","\n","    img_optimizer = None\n","\n","\n","    def run_epoch():\n","\n","        pbar = tqdm(enumerate(loader), total=len(\n","              loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n","\n","        net_loss=0\n","        for it,(caption_onehot_embeddings,caption,images) in pbar:\n","\n","                #when assigning image embedding before the inner loop without detaching it,it is giving runtime error -> ->  Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed)\n","                #imagemb = img_encoder(images).to(self.device).detach()\n","                #imagemb = img_encoder(images).to(self.device)\n","                #x = caption_embeddings.to(self.device)\n","                #x = x.detach()\n","                #words = caption[0].split()\n","                #caption_embeddings = caption_embeddings.to(self.device)\n","\n","                for i in range(cap_len):\n","\n","                    print(f'epoch: {epoch} sample: {it} cap_len : {i}')\n","                    imagemb = img_encoder(images).to(self.device)  # if imgemb is declared here then there is no previous runtime error because in the previous case maybe the intermediate buffer is getting freed and we are trying to access it again\n","                    #x = caption_embeddings.to(self.device)          #same problem as above\n","                    #print(f'x device: {x.get_device()}')\n","                    '''\n","                    if i==0:\n","                      start=torch.tensor([st_idx],dtype=torch.long)\n","                      #final_input = self.AppendLastToken(x,start)\n","                      last_word_emb = self.embeds(start)[None,:].to(self.device)\n","                      print(f'last_word_emb device: {last_word_emb.get_device()}')\n","                      final_input1 = torch.cat([caption_embeddings,last_word_emb],dim=1)\n","                      print(f'final_input device: {final_input1.get_device()}')\n","\n","\n","                    else:\n","                      ith_word = torch.tensor(vocab[words[i-1]],dtype=torch.long)\n","                      #final_input = self.AppendLastToken(x,ith_word)     #using ground truth for predicting next word\n","                      #modified_final_input = self.get_input2(start,img_encodings)                #using only image and previous word embeddings without context\n","                      last_word_emb = self.embeds(ith_word)[None,None,:].to(self.device)\n","                      #last_word_emb = last_word_emb.detach()\n","                      print(f'last_word_emb device: {last_word_emb.get_device()}')\n","                      #print(f'x size: {x.size()} lst_word_emb size: {last_word_emb.size()}')\n","                      final_input1 = torch.cat([caption_embeddings,last_word_emb],dim=1)\n","                      print(f'final_input device: {final_input1.get_device()}')\n","\n","\n","                    #final_input=final_input.detach()\n","                    final_input = final_input1\n","                    final_input=final_input.to(self.device)\n","                    '''\n","                    caption_onehot_embeddings=caption_onehot_embeddings.to(self.device)\n","                    with torch.set_grad_enabled(True) and torch.autograd.set_detect_anomaly(True):\n","                      caption_embedding, idx_matrix , loss = model(imagemb,i,caption,torch.tensor(caption_onehot_embeddings[:,i,:]))\n","\n","                      net_loss = net_loss + loss\n","\n","\n","\n","\n","                      loss.backward()    #sometimes give runtime error ->  Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed)\n","                    #loss.backward(retain_graph=True)\n","                      pbar.set_description(\n","                          f\"epoch {epoch+1} iter {it}  loss {loss:.4f} avg_loss {net_loss/((it+1)*cap_len):.4f} lr {optimizer.param_groups[0]['lr']:e}\")\n","                      print(f\"lr -->: {optimizer.param_groups[0]['lr']}\")\n","\n","        optimizer.step()\n","\n","        scheduler.step()\n","        if(img_optimizer):\n","          img_scheduler.step()\n","\n","        if(img_optimizer):\n","          img_optimizer.zero_grad()\n","\n","        model.zero_grad()\n","        optimizer.zero_grad()\n","\n","        print(f\"epoch: {epoch+1} iteration: {it}  avg_loss: {net_loss/((it+1)*cap_len)} lr: {optimizer.param_groups[0]['lr']}\")\n","\n","\n","\n","       # pbar.set_description(\n","        #                  f\"epoch {epoch+1} iter {it}  loss {loss:.4f} avg_loss {net_loss/((it+1)*cap_len):.4f} lr {optimizer.param_groups[0]['lr']:e}\")\n","\n","\n","    for epoch in range(max_epochs):\n","          run_epoch()\n","\n","\n","          if(epoch == finetune_after):\n","            img_optimizer = optim.RMSprop(img_encoder.parameters(), lr=1e-3)\n","            img_scheduler = StepLR(img_optimizer, step_size=lr_step_size, gamma=.1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1718171714297,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"bi31guIGee-5","outputId":"095fd300-d9f4-413d-ea32-46532b6bcd5a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n","  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"]}],"source":["model = ConvEmbGenerator(embedding_matrix_vocab)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["45dc44d0f3ce4da980366abbe6347336","e21e1f8d18cd45d18b3839694a959aed","7736485599794660a54bc01109ae2da5","ec9858a72b804368b26f8d1f8e6b3ebe","f47e1ea2895843a8bac7eae23228eb6d","15f80c67b31a424e8b103905149f0cb5","55e3ebad3f6d4f83adf96c4825e62403","41bf3a03abbc4910a3c73103ac8afbee","09611a63299f4c31aebeac53e1df1149","a2481cfabebd4d688483eac8a2c73500","ba9745e3c84643a99d0685262a735582","ecd0c8678bc244bf9b6eeb8951e8add1","c67066dc9e534063915c23957abeca70","d100b917533b4eefa48cf1847c933519","58f06394d3fa4faebd7112f04c07a608","b53cd59285a741c086dcc233bc80e86a","c67daa2d614f46c3abb5ff2b002d9f35","2025b6b4dbcc4b94aa60dd161eaa50cc","b2d66bd63edb4d47b4f46d77e15f6127","93a67977fc5c4d3f8dfdffbbbaeb0a27","ed149fdfd1454ec88047804947e64340","357944d01733491bb10f81081b297f93","c61a8b4dee364641b7aa223fad38f1b4","557d9c29ddef4ef2b411ee1231879c64","f91c03fc34404fcb812dd48354292233","bb469bb352b24534ad6cd2361d6231ef","4b0d3947b6994c5287282b472765e2d0","ecbedbb6fcd346169583c229329167b8","857e5733094d4ff082afb49c9dbda56e","9d4f211c3d054b84bb0ff845a0669570","9df567c4f3de4dab9e2339abf6809c67","6582e0cd6e7b456da1bce9416034d426","58f5c764c12542a5a59e17c0c3a7187d","d6b298890d76496f830e0c34d2015424","87100aa35b654effa2a6cc885f129522","19c0fb0f3cc64faa98410f834558baa7","084e8e54a6564c26b116770230f4cceb","5f7d4cbbe8f4432797f76bb5f0cf056d","187dd07f73084775b22a575667403b32","c2fa53209a984e999d103a870fca8d66","0e1c807631cc48d5a18a2b686ebc2a59","8d23da13841f47d782123c100d7881e5","b7a6ce9bdc924147b98f129961799001","6e87459d264b45bab2ecd2869e1bbaf7","273a3acd0c5646a0aab67b93576dd972","9aca25aac2174116bacedb0c6df6170b","9886f5867376407ba6654daabe9178ba","1f9054a7e00f44eabc9193db1c6c9372","4b83e93ea2cf4d53a4dadd3f06039449","5f7c0ea1e091450db55cd81b2ada89a2","51ad7ba08a5a48c8a5428665be15ae74","c377c62748904a658e59d309ed78720d","a74213497ff24f658440bcc1c028782b","441d55e03a024735b5fbd2fbcf1cab52","a289f3923846444d83c9ae784894836a","bb36af40ffe949a4a9e958c894419920","5d99905e68bd40c8b550007674e8e083","55f9bca19212483595c80f2f143d4eec","0a26725d71d94ee3aea3b4e31b12bb1c","d9a3910701ca4ef99cd1d3a8e18e772e","5b8e5332d8064556af41198d3dc64426","a4b517d070dd4c099cc99f276541985d","82daa05d29124cb2909634838caa8ffe","039bebaf2d924981ad37bb638ea6c434","2a6337198d514ec3a0c95cc44b57e423","c0181c6c11474887ae25b6612e9a5e6c","9f68fc80632b4ab49f31a1dd7ed2a087","7ef74f27c984405bab29e719258bc2b5","15903c14cc05445b8abd180d7ffffc44","c2eaad4b360f4ae381b5a4f559d2561f","9e53314503c24c3d9c42fe0a25fc764b","45c0552d0a1f48898e44046fbd3c2483","040968139aff4205a32cd5028f9fb33f","075552cc01394f3bb3ead9b01e396cb6","132a471dac3c412996cd70fdf3cc0a85","fd7265e442d541ee98dd2e545153b1f0","5ba9aac592cb44ab8a26b6dd82394812","896bf2afee214f4fbdca25593c21351b","b8bd38abb9654913a42a54726e32c049","f002c1d1e2f049aeb598964b1c0b42f3","d582dd5645524c69ad25fa46ab51ba4d","9bb588e34fbb4723a55b47a6c86c4784","9ec741b2a420407eab2c282b61694332","c84b713679db4b649b167159c3d66324","0be30d5489354c96bcbc1f10ca559034","3b56ae2f43814e4594f58375a6c2a1de","3532933887f948d28b308066390e7eb7","0ec043f7cbaa4b2281682d58e06dd0ba","26fb5d3beabd403c88ba327178f7118b","00935838e3bd4d86a75dc74fae18ae85","a03657dfbbf441eaa8fff9515b4ffe56","8af5edae08cc4d7f8375fd40c7d192b7","d2e3932bd0f74296abeee15d75e3f8ef","118c3d831af14d3eb17be37f2bf5c632","9c086e857d6941bd8caa91ac482a61fe","ece8ff2c54d24cbaa00e621102999af9","19f3b797cb84428c993e176d82c71b38","79c4759a1e064bafab0eb4150757dd05","b3163d5890d1494187f24e5ce0abcd7e","79fada2849724ca28271048272a7012f","7f1da6a1691042e895f2ac040bee1b66","52171eee348e44eb9cd4cae2fe07212c","54ab86e54dc34b479a818e9805b11cae","309eee4d22984280ba8b398114c5ccf6","86823a987d9e49d3a4df91590eb1698e","4ad3dd3ba06b4873ba4e506df7c18ca6","af1a0be9484044c499ee226194e07159","65b683a54b154bea9b4e61065cc18781","6819bf2018bc49dc946b6de4ef0c2eb2","63dfcda5200b4acc81ac34461ec535ff"],"output_embedded_package_id":"1QfMUiE-TIP6RnLecz1sPCUf1TIkoBHDW"},"id":"IixxqqAPHCui","outputId":"2f6891a0-d84f-427e-c819-71ade28d55c5","executionInfo":{"status":"ok","timestamp":1719824243055,"user_tz":-330,"elapsed":4771084,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["trainer = ModTrainer(model, 10, 0, processed_train_dataset, test_dataset, embedding_matrix_vocab)\n","trainer.train(processed_train_dataset)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"f-gaUwpCiI8k","executionInfo":{"status":"ok","timestamp":1719814746667,"user_tz":-330,"elapsed":6,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["saved_model_path=\"/content/drive/MyDrive/Colab Notebooks/Modified.pth\""]},{"cell_type":"code","execution_count":56,"metadata":{"id":"ar5GuSWm1BCG","executionInfo":{"status":"ok","timestamp":1719824267380,"user_tz":-330,"elapsed":761,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["torch.save(model.state_dict(), saved_model_path)"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1327,"status":"ok","timestamp":1719824325759,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"pAaDKYM51F8u","outputId":"090f3485-268e-4adb-9126-5985cbcbffa2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":58}],"source":["\n","model = ConvEmbGenerator(embedding_matrix_vocab)\n","#model.load_state_dict(torch.load(saved_model_path))   #to load the saved model in gpu\n","model.load_state_dict(torch.load(saved_model_path, map_location=torch.device('cpu')))   #to load the saved model in cpu"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"EthcVCpB-LFS","executionInfo":{"status":"ok","timestamp":1719824317604,"user_tz":-330,"elapsed":649,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}}},"outputs":[],"source":["class ModTester(CustomDataLoading):\n","  def __init__(self,model, train_dataset, test_dataset,embedding_matrix_vocab):\n","      self.model = model\n","      self.train_dataset = train_dataset\n","      self.test_dataset = test_dataset\n","      self.embedding_matrix_vocab = embedding_matrix_vocab\n","      self.img_encoder = Vgg16Feats()\n","\n","      self.device = 'cpu'\n","      if torch.cuda.is_available():  # take over whatever gpus are on the system\n","            self.device = torch.cuda.current_device()\n","\n","\n","\n","\n","  def test(self):\n","    test_loader=CustomDataLoading(processed_train_dataset,batch_size,0).loading_data()\n","    #PositionalEncodingModel=Summer(PositionalEncoding1D(1700))\n","    model=self.model.to(device=self.device)\n","    img_encoder = self.img_encoder.to(device=self.device)\n","    #img_encoder = self.img_encoder.to(device='cpu')\n","\n","    for it, (caption_onehot_embedding,caption,images) in enumerate(test_loader):\n","      #imgs= imgs.to(self.device)\n","      #img_encodings = img_encoder(imgs).cpu().detach()\n","      #img_encodings = img_encodings.view(img_encodings.size()[0],img_encodings.size()[1],1)\n","\n","          generated_captions=torch.tensor([],device=self.device)\n","          for i in range(cap_len):\n","              imagemb = img_encoder(images).to(self.device)\n","\n","              caption_onehot_embeddings=caption_onehot_embedding.to(self.device)\n","\n","              with torch.set_grad_enabled(False):\n","                  caption_embedding, idx_matrix , loss = model(imagemb,i,caption, torch.tensor(caption_onehot_embeddings[:,i,:]))\n","                  print(f'idx_matrix size: {idx_matrix.size()}')\n","                  idx_matrix = torch.unsqueeze(idx_matrix, dim=1)\n","                  generated_captions=torch.cat([generated_captions,idx_matrix],dim=1)\n","\n","                  print(f\"i={i}) generated_captions size: {generated_captions.size()}\")\n","\n","          for length in range(generated_captions.size()[0]):\n","              for indices in generated_captions:\n","                  caption=\"\"\n","                  for idx in indices:\n","                    caption+=\" \"+vocab.get_itos()[idx.to(torch.int)]\n","              #plt.imshow(  .permute(1, 2, 0)  )\n","              #print(f'image: {plt.imshow(imgs[length].permute(1,2,0))}')\n","              #plt.imshow(imgs[length].permute(1,2,0))\n","              print(f'original caption: {captions[length]}')   #need to return the original captions from the dataloader\n","              print(f'generated caption: {caption}')"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102994,"status":"ok","timestamp":1719824432908,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"},"user_tz":-330},"id":"5B_UI-AU35VZ","outputId":"114fca2c-0063-4c1b-a8c9-6d5e5500b61b"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-32-b1174a3fd3a2>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  images = torch.tensor(images).to(device)\n","<ipython-input-32-b1174a3fd3a2>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  one_hot_vector = F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n","<ipython-input-57-4f087e5211d6>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  caption_embedding, idx_matrix , loss = model(imagemb,i,caption, torch.tensor(caption_onehot_embeddings[:,i,:]))\n","<ipython-input-43-a7328c1e45cb>:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  one_hot_vector=F.one_hot(torch.tensor(idx),num_classes=vocab_len)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.783807396888733  Loss : 1.783807396888733\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6729884147644043  Loss : 0.6729884147644043\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8837250471115112  Loss : 1.8837250471115112\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.347776174545288  Loss : 2.347776174545288\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5230636596679688  Loss : 1.5230636596679688\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0974338054656982  Loss : 2.0974338054656982\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.089355945587158  Loss : 2.089355945587158\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0639941692352295  Loss : 2.0639941692352295\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.331380844116211  Loss : 2.331380844116211\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6194645166397095  Loss : 1.6194645166397095\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.512550711631775  Loss : 1.512550711631775\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8936644792556763  Loss : 0.8936644792556763\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.16093705594539642  Loss : 0.16093705594539642\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3606937825679779  Loss : 0.3606937825679779\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.32432541251182556  Loss : 0.32432541251182556\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a kitchen end end a end in a kitchen holding in pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.108578085899353  Loss : 1.108578085899353\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8144399523735046  Loss : 0.8144399523735046\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.738576889038086  Loss : 1.738576889038086\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.0107429027557373  Loss : 3.0107429027557373\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6496071815490723  Loss : 2.6496071815490723\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.033801555633545  Loss : 2.033801555633545\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.578242063522339  Loss : 2.578242063522339\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1445226669311523  Loss : 2.1445226669311523\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8151119947433472  Loss : 1.8151119947433472\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8485946655273438  Loss : 1.8485946655273438\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4969568252563477  Loss : 2.4969568252563477\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0402281284332275  Loss : 2.0402281284332275\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2550276517868042  Loss : 1.2550276517868042\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9458065032958984  Loss : 0.9458065032958984\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0706745386123657  Loss : 1.0706745386123657\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad people end walking by the walking by a homeless by street pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6898868680000305  Loss : 0.6898868680000305\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2945120334625244  Loss : 1.2945120334625244\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.341651678085327  Loss : 2.341651678085327\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4881958961486816  Loss : 2.4881958961486816\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8540176153182983  Loss : 1.8540176153182983\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0409812927246094  Loss : 2.0409812927246094\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4054102897644043  Loss : 2.4054102897644043\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.24820876121521  Loss : 3.24820876121521\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9880176782608032  Loss : 1.9880176782608032\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.166592597961426  Loss : 2.166592597961426\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.657907485961914  Loss : 2.657907485961914\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0579898357391357  Loss : 2.0579898357391357\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.263473391532898  Loss : 1.263473391532898\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.288886547088623  Loss : 1.288886547088623\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.800563633441925  Loss : 0.800563633441925\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a skateboard on end off skate end a man falls skateboard pad start pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.728278398513794  Loss : 1.728278398513794\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7486136555671692  Loss : 0.7486136555671692\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1738312244415283  Loss : 2.1738312244415283\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.702173948287964  Loss : 2.702173948287964\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.0078439712524414  Loss : 3.0078439712524414\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.180903673171997  Loss : 3.180903673171997\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8056989908218384  Loss : 1.8056989908218384\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9293034076690674  Loss : 1.9293034076690674\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.467372179031372  Loss : 2.467372179031372\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.424483299255371  Loss : 1.424483299255371\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1535370349884033  Loss : 1.1535370349884033\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6463335156440735  Loss : 0.6463335156440735\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.36854252219200134  Loss : 0.36854252219200134\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.22892391681671143  Loss : 0.22892391681671143\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.2878502309322357  Loss : 0.2878502309322357\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bicycle end bicyclists path bicyclists a park end end pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1214845180511475  Loss : 1.1214845180511475\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7334208488464355  Loss : 0.7334208488464355\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1091275215148926  Loss : 2.1091275215148926\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5807602405548096  Loss : 2.5807602405548096\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4668174982070923  Loss : 1.4668174982070923\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.042219877243042  Loss : 2.042219877243042\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.481109380722046  Loss : 2.481109380722046\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6736762523651123  Loss : 2.6736762523651123\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7083171606063843  Loss : 1.7083171606063843\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.476633071899414  Loss : 1.476633071899414\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9025694131851196  Loss : 1.9025694131851196\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3218607902526855  Loss : 1.3218607902526855\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4758469760417938  Loss : 0.4758469760417938\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.45383378863334656  Loss : 0.45383378863334656\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4471191465854645  Loss : 0.4471191465854645\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bike end a shop end end end bike end pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.187307596206665  Loss : 1.187307596206665\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6412690877914429  Loss : 0.6412690877914429\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.627471446990967  Loss : 2.627471446990967\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7693283557891846  Loss : 2.7693283557891846\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7070286273956299  Loss : 1.7070286273956299\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5713016986846924  Loss : 2.5713016986846924\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5360591411590576  Loss : 2.5360591411590576\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0381228923797607  Loss : 2.0381228923797607\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.492978811264038  Loss : 2.492978811264038\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4631760120391846  Loss : 2.4631760120391846\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.702386498451233  Loss : 1.702386498451233\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.508186936378479  Loss : 0.508186936378479\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7910842299461365  Loss : 0.7910842299461365\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3481643795967102  Loss : 0.3481643795967102\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1229567527770996  Loss : 1.1229567527770996\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start the has doors the and a bathtub has end the bathtub the the has\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9514776468276978  Loss : 0.9514776468276978\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9018516540527344  Loss : 1.9018516540527344\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.237766981124878  Loss : 2.237766981124878\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.277369737625122  Loss : 2.277369737625122\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7782249450683594  Loss : 1.7782249450683594\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1541366577148438  Loss : 2.1541366577148438\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.300320863723755  Loss : 3.300320863723755\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.156754970550537  Loss : 2.156754970550537\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8241554498672485  Loss : 1.8241554498672485\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1208865642547607  Loss : 2.1208865642547607\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.732959508895874  Loss : 2.732959508895874\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0202834606170654  Loss : 2.0202834606170654\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0529330968856812  Loss : 1.0529330968856812\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8119329810142517  Loss : 0.8119329810142517\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1115511655807495  Loss : 1.1115511655807495\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a and end a bathroom end end closet tub with pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1064785718917847  Loss : 1.1064785718917847\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9074048399925232  Loss : 0.9074048399925232\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.434206247329712  Loss : 2.434206247329712\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.464078187942505  Loss : 3.464078187942505\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3075313568115234  Loss : 2.3075313568115234\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3456151485443115  Loss : 2.3456151485443115\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7731891870498657  Loss : 1.7731891870498657\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.935435175895691  Loss : 1.935435175895691\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3304765224456787  Loss : 2.3304765224456787\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6955254077911377  Loss : 2.6955254077911377\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9012960195541382  Loss : 1.9012960195541382\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.087977647781372  Loss : 1.087977647781372\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4821500778198242  Loss : 1.4821500778198242\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5376286506652832  Loss : 0.5376286506652832\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5688782334327698  Loss : 0.5688782334327698\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a end end end end furnishings furnishings end bathroom end pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.307749629020691  Loss : 1.307749629020691\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4028013944625854  Loss : 1.4028013944625854\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2310986518859863  Loss : 2.2310986518859863\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7529571056365967  Loss : 2.7529571056365967\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.802807569503784  Loss : 2.802807569503784\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1491775512695312  Loss : 2.1491775512695312\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1267659664154053  Loss : 2.1267659664154053\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4081802368164062  Loss : 2.4081802368164062\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5631911754608154  Loss : 2.5631911754608154\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2821578979492188  Loss : 2.2821578979492188\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2002009153366089  Loss : 1.2002009153366089\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7732428312301636  Loss : 1.7732428312301636\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1588026285171509  Loss : 1.1588026285171509\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6268501281738281  Loss : 0.6268501281738281\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7228150963783264  Loss : 0.7228150963783264\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a sink plumbing the unfinished a with end end a the a and end\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.201921820640564  Loss : 1.201921820640564\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4271136224269867  Loss : 0.4271136224269867\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.856157660484314  Loss : 1.856157660484314\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1882643699645996  Loss : 2.1882643699645996\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.401880979537964  Loss : 2.401880979537964\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2985668182373047  Loss : 2.2985668182373047\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.188868284225464  Loss : 2.188868284225464\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.9875924587249756  Loss : 2.9875924587249756\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.399977922439575  Loss : 2.399977922439575\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3572919368743896  Loss : 2.3572919368743896\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.129100799560547  Loss : 2.129100799560547\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0920524597167969  Loss : 1.0920524597167969\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.1665285974740982  Loss : 0.1665285974740982\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.37126287817955017  Loss : 0.37126287817955017\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4335286319255829  Loss : 0.4335286319255829\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bathroom end end a bathtub end a bathroom end pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0889629125595093  Loss : 1.0889629125595093\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.19420568645000458  Loss : 0.19420568645000458\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.523090124130249  Loss : 2.523090124130249\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.65854811668396  Loss : 2.65854811668396\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.450345277786255  Loss : 2.450345277786255\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1586036682128906  Loss : 2.1586036682128906\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6078388690948486  Loss : 1.6078388690948486\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2310848236083984  Loss : 2.2310848236083984\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.65685772895813  Loss : 2.65685772895813\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9479974508285522  Loss : 1.9479974508285522\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5307353734970093  Loss : 1.5307353734970093\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1615657806396484  Loss : 1.1615657806396484\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8002902865409851  Loss : 0.8002902865409851\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4232368469238281  Loss : 0.4232368469238281\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.42876243591308594  Loss : 0.42876243591308594\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a floor like with buckets janitors like end buckets end this janitors looks pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2568328380584717  Loss : 1.2568328380584717\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0808552503585815  Loss : 1.0808552503585815\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5910109281539917  Loss : 1.5910109281539917\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4312546253204346  Loss : 2.4312546253204346\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3817332983016968  Loss : 1.3817332983016968\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8245779275894165  Loss : 1.8245779275894165\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3684799671173096  Loss : 2.3684799671173096\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3112778663635254  Loss : 2.3112778663635254\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9475606679916382  Loss : 1.9475606679916382\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.39993953704834  Loss : 2.39993953704834\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.259716033935547  Loss : 2.259716033935547\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1194270849227905  Loss : 1.1194270849227905\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8732995986938477  Loss : 0.8732995986938477\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.22694054245948792  Loss : 0.22694054245948792\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.47722944617271423  Loss : 0.47722944617271423\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bathroom contains a small and a bathroom and pad pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6273574829101562  Loss : 1.6273574829101562\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.24870625138282776  Loss : 0.24870625138282776\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1597273349761963  Loss : 2.1597273349761963\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.620680332183838  Loss : 2.620680332183838\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.9135828018188477  Loss : 2.9135828018188477\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3283262252807617  Loss : 2.3283262252807617\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.049649715423584  Loss : 2.049649715423584\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3214926719665527  Loss : 2.3214926719665527\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2216320037841797  Loss : 2.2216320037841797\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.389136552810669  Loss : 2.389136552810669\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.343029260635376  Loss : 1.343029260635376\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7299391627311707  Loss : 0.7299391627311707\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6457871198654175  Loss : 0.6457871198654175\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.32880911231040955  Loss : 0.32880911231040955\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3801954984664917  Loss : 0.3801954984664917\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a door locked bike locked a a a bike a pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4828535318374634  Loss : 1.4828535318374634\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4118765890598297  Loss : 0.4118765890598297\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8834739923477173  Loss : 1.8834739923477173\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7707173824310303  Loss : 2.7707173824310303\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6790026426315308  Loss : 1.6790026426315308\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7539374828338623  Loss : 2.7539374828338623\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9929723739624023  Loss : 1.9929723739624023\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5747874975204468  Loss : 1.5747874975204468\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.368224859237671  Loss : 2.368224859237671\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.126702070236206  Loss : 2.126702070236206\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2705813646316528  Loss : 1.2705813646316528\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1645636558532715  Loss : 1.1645636558532715\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7231577038764954  Loss : 0.7231577038764954\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.34793776273727417  Loss : 0.34793776273727417\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3447989523410797  Loss : 0.3447989523410797\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bathroom has a toilet end has a bathroom bathroom and pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2709856033325195  Loss : 1.2709856033325195\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0404847860336304  Loss : 1.0404847860336304\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.732656717300415  Loss : 1.732656717300415\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9479016065597534  Loss : 1.9479016065597534\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4394307136535645  Loss : 2.4394307136535645\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.731844902038574  Loss : 2.731844902038574\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7629921436309814  Loss : 2.7629921436309814\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.8912253379821777  Loss : 2.8912253379821777\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5506958961486816  Loss : 2.5506958961486816\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4271607398986816  Loss : 2.4271607398986816\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1421067714691162  Loss : 1.1421067714691162\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8366686701774597  Loss : 0.8366686701774597\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8172239661216736  Loss : 0.8172239661216736\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3352779746055603  Loss : 0.3352779746055603\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3053961396217346  Loss : 0.3053961396217346\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a very filled filled end cars end cars end pad pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3983252048492432  Loss : 1.3983252048492432\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4123573303222656  Loss : 2.4123573303222656\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4629571437835693  Loss : 2.4629571437835693\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6923460960388184  Loss : 2.6923460960388184\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6055314540863037  Loss : 2.6055314540863037\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.362678289413452  Loss : 2.362678289413452\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.236687421798706  Loss : 2.236687421798706\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5861966609954834  Loss : 2.5861966609954834\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.571223020553589  Loss : 2.571223020553589\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4814698696136475  Loss : 2.4814698696136475\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.280821681022644  Loss : 1.280821681022644\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3415523767471313  Loss : 1.3415523767471313\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5863794684410095  Loss : 0.5863794684410095\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.2645248472690582  Loss : 0.2645248472690582\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4925585389137268  Loss : 0.4925585389137268\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a small end sink end a bathroom sink end pad pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0062156915664673  Loss : 1.0062156915664673\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1635363101959229  Loss : 1.1635363101959229\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9475666284561157  Loss : 1.9475666284561157\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.8852620124816895  Loss : 2.8852620124816895\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9816124439239502  Loss : 1.9816124439239502\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3945366144180298  Loss : 1.3945366144180298\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.353266954421997  Loss : 2.353266954421997\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7660796642303467  Loss : 2.7660796642303467\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1812145709991455  Loss : 2.1812145709991455\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.703573703765869  Loss : 2.703573703765869\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0869333744049072  Loss : 2.0869333744049072\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5144656896591187  Loss : 1.5144656896591187\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0155553817749023  Loss : 1.0155553817749023\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6021008491516113  Loss : 0.6021008491516113\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9154869318008423  Loss : 0.9154869318008423\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a sink and a bathroom end a end end end pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6674385666847229  Loss : 0.6674385666847229\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.11238648742437363  Loss : 0.11238648742437363\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6939671039581299  Loss : 1.6939671039581299\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4032323360443115  Loss : 2.4032323360443115\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2602672576904297  Loss : 2.2602672576904297\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.159923553466797  Loss : 2.159923553466797\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0171072483062744  Loss : 2.0171072483062744\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9534145593643188  Loss : 1.9534145593643188\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.502028703689575  Loss : 2.502028703689575\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3159515857696533  Loss : 2.3159515857696533\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.426450490951538  Loss : 2.426450490951538\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0996272563934326  Loss : 2.0996272563934326\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9135385751724243  Loss : 0.9135385751724243\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3034952878952026  Loss : 1.3034952878952026\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2346456050872803  Loss : 1.2346456050872803\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bathroom end and shown a bathroom end a bathroom end pad start pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2319049835205078  Loss : 1.2319049835205078\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8406051993370056  Loss : 0.8406051993370056\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7885208129882812  Loss : 1.7885208129882812\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.57433819770813  Loss : 2.57433819770813\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7689037322998047  Loss : 2.7689037322998047\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.189634084701538  Loss : 2.189634084701538\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2645950317382812  Loss : 2.2645950317382812\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3931972980499268  Loss : 2.3931972980499268\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.9103152751922607  Loss : 2.9103152751922607\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1719272136688232  Loss : 2.1719272136688232\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8592770099639893  Loss : 1.8592770099639893\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0525892972946167  Loss : 1.0525892972946167\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6611396074295044  Loss : 0.6611396074295044\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3809455931186676  Loss : 0.3809455931186676\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.46596628427505493  Loss : 0.46596628427505493\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bathroom with a bathroom with top tongs a tongs a of pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2075175046920776  Loss : 1.2075175046920776\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5045948028564453  Loss : 0.5045948028564453\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7622913122177124  Loss : 1.7622913122177124\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.015625476837158  Loss : 3.015625476837158\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.29899525642395  Loss : 2.29899525642395\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.557940721511841  Loss : 2.557940721511841\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.405194044113159  Loss : 2.405194044113159\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9051131010055542  Loss : 1.9051131010055542\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7329453229904175  Loss : 1.7329453229904175\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5773987770080566  Loss : 1.5773987770080566\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8912609219551086  Loss : 0.8912609219551086\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7859880328178406  Loss : 0.7859880328178406\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3682711720466614  Loss : 0.3682711720466614\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8456083536148071  Loss : 0.8456083536148071\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6529659628868103  Loss : 0.6529659628868103\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a car of of a cat of well a cat of pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.884471595287323  Loss : 0.884471595287323\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0928601026535034  Loss : 1.0928601026535034\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.179877758026123  Loss : 2.179877758026123\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4741344451904297  Loss : 2.4741344451904297\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3605642318725586  Loss : 2.3605642318725586\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.670450210571289  Loss : 2.670450210571289\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.604124069213867  Loss : 2.604124069213867\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7436234951019287  Loss : 2.7436234951019287\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3128583431243896  Loss : 2.3128583431243896\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.1840145587921143  Loss : 3.1840145587921143\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2295644283294678  Loss : 2.2295644283294678\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5760507583618164  Loss : 1.5760507583618164\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6805355548858643  Loss : 1.6805355548858643\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0053952932357788  Loss : 1.0053952932357788\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8638181090354919  Loss : 0.8638181090354919\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start an bus in in bus a bus bus end an in in an in\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0002281665802002  Loss : 1.0002281665802002\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.2523563802242279  Loss : 0.2523563802242279\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7331055402755737  Loss : 1.7331055402755737\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.347475528717041  Loss : 2.347475528717041\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0787124633789062  Loss : 2.0787124633789062\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4189995527267456  Loss : 1.4189995527267456\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4688541889190674  Loss : 1.4688541889190674\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6650755405426025  Loss : 2.6650755405426025\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5320934057235718  Loss : 1.5320934057235718\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.376556873321533  Loss : 2.376556873321533\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9363800287246704  Loss : 1.9363800287246704\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1223957538604736  Loss : 1.1223957538604736\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9982963800430298  Loss : 0.9982963800430298\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5076486468315125  Loss : 0.5076486468315125\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1041538715362549  Loss : 1.1041538715362549\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a man is is to down a to a cops next is pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0651153326034546  Loss : 1.0651153326034546\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.23756150901317596  Loss : 0.23756150901317596\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.91048264503479  Loss : 1.91048264503479\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.845853328704834  Loss : 2.845853328704834\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8820427656173706  Loss : 1.8820427656173706\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3942785263061523  Loss : 1.3942785263061523\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1641149520874023  Loss : 2.1641149520874023\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3983635902404785  Loss : 2.3983635902404785\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6319942474365234  Loss : 2.6319942474365234\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4377763271331787  Loss : 2.4377763271331787\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.099616289138794  Loss : 2.099616289138794\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2228167057037354  Loss : 2.2228167057037354\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1600421667099  Loss : 1.1600421667099\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9571720361709595  Loss : 0.9571720361709595\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7606443166732788  Loss : 0.7606443166732788\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a sink end and a small bathroom revealing a end a to pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1406663656234741  Loss : 1.1406663656234741\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4163079261779785  Loss : 0.4163079261779785\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.059899091720581  Loss : 2.059899091720581\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3996732234954834  Loss : 2.3996732234954834\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6649463176727295  Loss : 2.6649463176727295\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.719658374786377  Loss : 2.719658374786377\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.767343044281006  Loss : 2.767343044281006\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3113009929656982  Loss : 2.3113009929656982\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.227689027786255  Loss : 2.227689027786255\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9945462942123413  Loss : 1.9945462942123413\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4439826011657715  Loss : 1.4439826011657715\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2024246454238892  Loss : 1.2024246454238892\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1864885091781616  Loss : 1.1864885091781616\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5812842845916748  Loss : 0.5812842845916748\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.35594144463539124  Loss : 0.35594144463539124\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a row of broken toilets toilets ground ground of a toilets of pad start\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2243177890777588  Loss : 1.2243177890777588\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.3692725896835327  Loss : 1.3692725896835327\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.163639783859253  Loss : 2.163639783859253\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6573708057403564  Loss : 2.6573708057403564\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.638411283493042  Loss : 2.638411283493042\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.92644464969635  Loss : 1.92644464969635\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.691383719444275  Loss : 1.691383719444275\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2745494842529297  Loss : 2.2745494842529297\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.410783052444458  Loss : 2.410783052444458\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.188952684402466  Loss : 2.188952684402466\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9849185943603516  Loss : 1.9849185943603516\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.085952639579773  Loss : 1.085952639579773\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.37377044558525085  Loss : 0.37377044558525085\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.35594168305397034  Loss : 0.35594168305397034\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.329481840133667  Loss : 0.329481840133667\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad the motor end end end the up end the end end pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7951717972755432  Loss : 0.7951717972755432\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8286137580871582  Loss : 0.8286137580871582\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7772216796875  Loss : 1.7772216796875\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.005190849304199  Loss : 2.005190849304199\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.885862946510315  Loss : 1.885862946510315\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4797003269195557  Loss : 2.4797003269195557\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6028339862823486  Loss : 2.6028339862823486\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3390159606933594  Loss : 2.3390159606933594\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5709493160247803  Loss : 2.5709493160247803\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1975581645965576  Loss : 2.1975581645965576\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0266740322113037  Loss : 2.0266740322113037\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7576725482940674  Loss : 1.7576725482940674\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8767490386962891  Loss : 0.8767490386962891\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1100596189498901  Loss : 1.1100596189498901\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8280982971191406  Loss : 0.8280982971191406\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a stack end end guy end a end a stack end a side end\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0134013891220093  Loss : 1.0134013891220093\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3042416572570801  Loss : 0.3042416572570801\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4146642684936523  Loss : 2.4146642684936523\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.8348777294158936  Loss : 2.8348777294158936\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8144944906234741  Loss : 1.8144944906234741\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.329231023788452  Loss : 2.329231023788452\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0771400928497314  Loss : 2.0771400928497314\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.495631217956543  Loss : 2.495631217956543\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8515528440475464  Loss : 1.8515528440475464\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.151958703994751  Loss : 2.151958703994751\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.233004331588745  Loss : 2.233004331588745\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0228960514068604  Loss : 1.0228960514068604\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7657458186149597  Loss : 0.7657458186149597\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6303439140319824  Loss : 0.6303439140319824\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5619358420372009  Loss : 0.5619358420372009\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a toilet end in a toilet end a fountain with pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8993231058120728  Loss : 0.8993231058120728\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6742757558822632  Loss : 1.6742757558822632\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1555612087249756  Loss : 2.1555612087249756\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.541435718536377  Loss : 2.541435718536377\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0135395526885986  Loss : 2.0135395526885986\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.108302354812622  Loss : 2.108302354812622\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6230716705322266  Loss : 2.6230716705322266\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.466852903366089  Loss : 2.466852903366089\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6139557361602783  Loss : 2.6139557361602783\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1851308345794678  Loss : 2.1851308345794678\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.06070613861084  Loss : 2.06070613861084\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.663148283958435  Loss : 1.663148283958435\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9807100296020508  Loss : 0.9807100296020508\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8936630487442017  Loss : 0.8936630487442017\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8165912628173828  Loss : 0.8165912628173828\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a horse is carriage is drawn the parked drawn pad pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9662491083145142  Loss : 0.9662491083145142\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2273941040039062  Loss : 1.2273941040039062\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2088143825531006  Loss : 2.2088143825531006\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.536365270614624  Loss : 2.536365270614624\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.164283037185669  Loss : 2.164283037185669\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3634843826293945  Loss : 2.3634843826293945\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.090379476547241  Loss : 3.090379476547241\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0735435485839844  Loss : 2.0735435485839844\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3788933753967285  Loss : 2.3788933753967285\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1412696838378906  Loss : 2.1412696838378906\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7160885334014893  Loss : 1.7160885334014893\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.246227502822876  Loss : 1.246227502822876\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2532882690429688  Loss : 1.2532882690429688\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6841012239456177  Loss : 0.6841012239456177\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6232874989509583  Loss : 0.6232874989509583\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a end on display inside on white inside a on on a pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4117456674575806  Loss : 1.4117456674575806\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8452709913253784  Loss : 1.8452709913253784\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9011036157608032  Loss : 1.9011036157608032\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.263873338699341  Loss : 2.263873338699341\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.62151837348938  Loss : 2.62151837348938\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.9680233001708984  Loss : 2.9680233001708984\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8841899633407593  Loss : 1.8841899633407593\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3830692768096924  Loss : 2.3830692768096924\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.366488218307495  Loss : 2.366488218307495\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.1775128841400146  Loss : 3.1775128841400146\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3013076782226562  Loss : 2.3013076782226562\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1518570184707642  Loss : 1.1518570184707642\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.42702093720436096  Loss : 0.42702093720436096\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.29065170884132385  Loss : 0.29065170884132385\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.2756495475769043  Loss : 0.2756495475769043\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a clock end clock end clock clock a clock clock clock pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2102129459381104  Loss : 1.2102129459381104\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9279330372810364  Loss : 0.9279330372810364\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.228872776031494  Loss : 2.228872776031494\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.672651767730713  Loss : 2.672651767730713\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4888882637023926  Loss : 2.4888882637023926\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4387717247009277  Loss : 2.4387717247009277\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7285008430480957  Loss : 2.7285008430480957\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.698907494544983  Loss : 1.698907494544983\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5456079244613647  Loss : 1.5456079244613647\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.9091389179229736  Loss : 2.9091389179229736\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0652225017547607  Loss : 2.0652225017547607\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5808658599853516  Loss : 1.5808658599853516\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.953281581401825  Loss : 0.953281581401825\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.40169572830200195  Loss : 0.40169572830200195\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7158028483390808  Loss : 0.7158028483390808\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a small toilet end and a lid can trash a small metal can lid\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9322555661201477  Loss : 0.9322555661201477\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.18487709760665894  Loss : 0.18487709760665894\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.085855484008789  Loss : 2.085855484008789\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.2165050506591797  Loss : 3.2165050506591797\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.692934989929199  Loss : 2.692934989929199\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.810470461845398  Loss : 1.810470461845398\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0747196674346924  Loss : 2.0747196674346924\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2815847396850586  Loss : 2.2815847396850586\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4698853492736816  Loss : 2.4698853492736816\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5632574558258057  Loss : 1.5632574558258057\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8174240589141846  Loss : 1.8174240589141846\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.414734363555908  Loss : 2.414734363555908\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9410821795463562  Loss : 0.9410821795463562\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.44563695788383484  Loss : 0.44563695788383484\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1519453525543213  Loss : 1.1519453525543213\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a works in a in end a man computer end pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8460071682929993  Loss : 0.8460071682929993\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1552714109420776  Loss : 1.1552714109420776\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9235702753067017  Loss : 1.9235702753067017\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.622866153717041  Loss : 2.622866153717041\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.0678322315216064  Loss : 3.0678322315216064\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.253558874130249  Loss : 3.253558874130249\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.654078722000122  Loss : 3.654078722000122\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.091717481613159  Loss : 3.091717481613159\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5891592502593994  Loss : 2.5891592502593994\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6461403369903564  Loss : 2.6461403369903564\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0303008556365967  Loss : 2.0303008556365967\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5784329175949097  Loss : 1.5784329175949097\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.171723484992981  Loss : 1.171723484992981\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.135034441947937  Loss : 1.135034441947937\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0019733905792236  Loss : 1.0019733905792236\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a bottle end end blue with end oatmeal and sugar bowl of a and\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4031156301498413  Loss : 1.4031156301498413\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.647025465965271  Loss : 1.647025465965271\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.057772159576416  Loss : 2.057772159576416\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8643913269042969  Loss : 1.8643913269042969\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.332062244415283  Loss : 2.332062244415283\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.1000447273254395  Loss : 3.1000447273254395\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6775143146514893  Loss : 2.6775143146514893\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6661362648010254  Loss : 2.6661362648010254\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.352015495300293  Loss : 2.352015495300293\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.635049819946289  Loss : 2.635049819946289\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.860926866531372  Loss : 1.860926866531372\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.957585334777832  Loss : 0.957585334777832\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3625265657901764  Loss : 0.3625265657901764\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4231376647949219  Loss : 0.4231376647949219\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6815333366394043  Loss : 0.6815333366394043\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a bunch end a of together motorcycles of of pad pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.6088829040527344  Loss : 1.6088829040527344\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5728552341461182  Loss : 1.5728552341461182\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9150654077529907  Loss : 1.9150654077529907\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.894336462020874  Loss : 2.894336462020874\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.209947347640991  Loss : 2.209947347640991\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.639878511428833  Loss : 2.639878511428833\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.470268964767456  Loss : 2.470268964767456\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1968796253204346  Loss : 2.1968796253204346\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.2375848293304443  Loss : 2.2375848293304443\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4295125007629395  Loss : 2.4295125007629395\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8264204263687134  Loss : 0.8264204263687134\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8168293833732605  Loss : 0.8168293833732605\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.21616224944591522  Loss : 0.21616224944591522\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.3704216480255127  Loss : 0.3704216480255127\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.4636213481426239  Loss : 0.4636213481426239\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a end front end a of people building in pad pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2522003650665283  Loss : 1.2522003650665283\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.816226601600647  Loss : 0.816226601600647\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.8468831777572632  Loss : 1.8468831777572632\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.783867359161377  Loss : 2.783867359161377\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.217655897140503  Loss : 2.217655897140503\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5609169006347656  Loss : 2.5609169006347656\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.8121726512908936  Loss : 2.8121726512908936\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3721189498901367  Loss : 2.3721189498901367\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.025581121444702  Loss : 3.025581121444702\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.302572250366211  Loss : 2.302572250366211\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9996355772018433  Loss : 1.9996355772018433\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4683109521865845  Loss : 1.4683109521865845\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.9416267275810242  Loss : 0.9416267275810242\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7965108752250671  Loss : 0.7965108752250671\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5752245187759399  Loss : 0.5752245187759399\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a building wall a yellow end building end yellow a wall wall pad start\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.960174560546875  Loss : 0.960174560546875\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5480388402938843  Loss : 1.5480388402938843\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.192399740219116  Loss : 2.192399740219116\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.0104432106018066  Loss : 3.0104432106018066\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1866402626037598  Loss : 2.1866402626037598\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.135181427001953  Loss : 2.135181427001953\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0809319019317627  Loss : 2.0809319019317627\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.06262469291687  Loss : 2.06262469291687\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.27820086479187  Loss : 2.27820086479187\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4893767833709717  Loss : 2.4893767833709717\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7613370418548584  Loss : 2.7613370418548584\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3507771492004395  Loss : 2.3507771492004395\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.730678379535675  Loss : 0.730678379535675\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6959366798400879  Loss : 0.6959366798400879\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7930852770805359  Loss : 0.7930852770805359\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a of traffic of bikers end a lot end front of pad start pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0714473724365234  Loss : 1.0714473724365234\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.8956767916679382  Loss : 0.8956767916679382\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.9948476552963257  Loss : 1.9948476552963257\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.461932897567749  Loss : 2.461932897567749\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 3.122105121612549  Loss : 3.122105121612549\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.8465230464935303  Loss : 2.8465230464935303\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.537107229232788  Loss : 2.537107229232788\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.7009494304656982  Loss : 2.7009494304656982\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.0446391105651855  Loss : 2.0446391105651855\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.394744873046875  Loss : 2.394744873046875\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1399190425872803  Loss : 2.1399190425872803\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2326630353927612  Loss : 1.2326630353927612\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7328976988792419  Loss : 0.7328976988792419\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.6404433250427246  Loss : 0.6404433250427246\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.586827278137207  Loss : 0.586827278137207\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a driving end driving swiftly driving driving group driving motorcycles pad pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.7282372713088989  Loss : 0.7282372713088989\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.05902452394366264  Loss : 0.05902452394366264\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.542555332183838  Loss : 1.542555332183838\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.520383834838867  Loss : 2.520383834838867\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.4676979780197144  Loss : 1.4676979780197144\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.020275831222534  Loss : 2.020275831222534\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7964619398117065  Loss : 1.7964619398117065\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3420462608337402  Loss : 2.3420462608337402\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.915941596031189  Loss : 1.915941596031189\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.6746349334716797  Loss : 2.6746349334716797\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7934446334838867  Loss : 1.7934446334838867\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.0239871740341187  Loss : 1.0239871740341187\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.573799967765808  Loss : 1.573799967765808\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.2695544958114624  Loss : 1.2695544958114624\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.48565760254859924  Loss : 0.48565760254859924\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  start a person down a person end a a person down a pad pad pad\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 0.5501058101654053  Loss : 0.5501058101654053\n","idx_matrix size: torch.Size([10])\n","i=0) generated_captions size: torch.Size([10, 1])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.1009920835494995  Loss : 1.1009920835494995\n","idx_matrix size: torch.Size([10])\n","i=1) generated_captions size: torch.Size([10, 2])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.109571933746338  Loss : 2.109571933746338\n","idx_matrix size: torch.Size([10])\n","i=2) generated_captions size: torch.Size([10, 3])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.769052267074585  Loss : 2.769052267074585\n","idx_matrix size: torch.Size([10])\n","i=3) generated_captions size: torch.Size([10, 4])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.5914623737335205  Loss : 2.5914623737335205\n","idx_matrix size: torch.Size([10])\n","i=4) generated_captions size: torch.Size([10, 5])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.3847007751464844  Loss : 2.3847007751464844\n","idx_matrix size: torch.Size([10])\n","i=5) generated_captions size: torch.Size([10, 6])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.656210422515869  Loss : 2.656210422515869\n","idx_matrix size: torch.Size([10])\n","i=6) generated_captions size: torch.Size([10, 7])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.4458069801330566  Loss : 2.4458069801330566\n","idx_matrix size: torch.Size([10])\n","i=7) generated_captions size: torch.Size([10, 8])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.713815212249756  Loss : 2.713815212249756\n","idx_matrix size: torch.Size([10])\n","i=8) generated_captions size: torch.Size([10, 9])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7684509754180908  Loss : 1.7684509754180908\n","idx_matrix size: torch.Size([10])\n","i=9) generated_captions size: torch.Size([10, 10])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.676032304763794  Loss : 2.676032304763794\n","idx_matrix size: torch.Size([10])\n","i=10) generated_captions size: torch.Size([10, 11])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 2.1330277919769287  Loss : 2.1330277919769287\n","idx_matrix size: torch.Size([10])\n","i=11) generated_captions size: torch.Size([10, 12])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.7565773725509644  Loss : 1.7565773725509644\n","idx_matrix size: torch.Size([10])\n","i=12) generated_captions size: torch.Size([10, 13])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.5079728364944458  Loss : 1.5079728364944458\n","idx_matrix size: torch.Size([10])\n","i=13) generated_captions size: torch.Size([10, 14])\n","x before vocabproj: torch.Size([10, 512, 16])\n","in loss out shape:  torch.Size([10, 825])\n","in loss target shape:  torch.Size([10, 825])\n","inside CustomCrossEntropy out size: torch.Size([10, 825])\n","CustomLoss : 1.132735252380371  Loss : 1.132735252380371\n","idx_matrix size: torch.Size([10])\n","i=14) generated_captions size: torch.Size([10, 15])\n","original caption: start a woman wearing a net on her head cutting a cake  end pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a woman cutting a large white sheet cake end pad  pad  pad  pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a woman wearing a hair net cutting a large sheet cake end pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start there is a woman that is cutting a white cake end pad  pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a woman marking a cake with the back of a chefs knife  end pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a young boy standing in front of a computer keyboard end pad  pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a little boy wearing headphones and looking at a computer monitor end pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start he is listening intently to the computer at school end pad  pad  pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a young boy stares up at the computer monitor end pad  pad  pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n","original caption: start a young kid with head phones on using a computer  end pad  pad  pad \n","generated caption:  pad a scooters scooters end building end a electric a large building end pad pad\n"]}],"source":["tester = ModTester(model,processed_train_dataset,test_dataset,embedding_matrix_vocab)\n","tester.test()"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"31xjxevn9-a9","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1719824482569,"user_tz":-330,"elapsed":905,"user":{"displayName":"ANUBHAV MISRA","userId":"12779415185971369494"}},"outputId":"a4ec16ea-931d-48e5-8cc1-0ea583b46614"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","val_losses = []\n","train_losses = [4.90,3.1016,2.8224,2.7708,2.7326,2.6960,2.56,2.53,2.47295,2.4387,2.3650,2.28,2.16,2.141168,2.0897,2.05480,2.0122,1.9553,1.8953,1.8689,1.7358]\n","val_losses = [5.5,3.5,3.1,3,2.9892,2.8976,2.7980,2.78,2.75,2.70,2.61,2.5,2.4240,2.41,2.40,2.35,2.30,2.27,2.25,1.9897,1.9567]\n","x = np.arange(0, 126, 6)\n","\n","fig, ax = plt.subplots()\n","ax.plot(x,train_losses,label=\"train\")\n","ax.plot(x,val_losses,label=\"val\")\n","ax.legend()\n","\n","ax.set_xlabel(\"iterations\")\n","ax.set_ylabel(\"Loss\")\n","ax.set_title('Training And Validation Loss')\n","\n","plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"P6EIDNFza_ub"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"54c3e1507dfb437585f6b9bb185f264d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27d64080a6a0406a88cd0b93bd28087d","IPY_MODEL_7d26abdba299455ab0c2d8dc9eb92e5d","IPY_MODEL_e2360a52c68543a195280b5da9b73200"],"layout":"IPY_MODEL_6f295d966e04429d9365c3fc519c7723"}},"27d64080a6a0406a88cd0b93bd28087d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45aaa6b842f94385b5d3ebf9af38dc0e","placeholder":"​","style":"IPY_MODEL_e12b1bd5b7b845c2a380e384b67cdf39","value":"Downloading builder script: 100%"}},"7d26abdba299455ab0c2d8dc9eb92e5d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a051dbeba3f4d3a855abe1ded56c91e","max":9475,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55673a517a4a4d87af46164c02999bbd","value":9475}},"e2360a52c68543a195280b5da9b73200":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf08f5ab11ca419eb67eb592f0de69f7","placeholder":"​","style":"IPY_MODEL_4b77de891a314cada47563f129e6f8f5","value":" 9.47k/9.47k [00:00&lt;00:00, 488kB/s]"}},"6f295d966e04429d9365c3fc519c7723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45aaa6b842f94385b5d3ebf9af38dc0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e12b1bd5b7b845c2a380e384b67cdf39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a051dbeba3f4d3a855abe1ded56c91e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55673a517a4a4d87af46164c02999bbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf08f5ab11ca419eb67eb592f0de69f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b77de891a314cada47563f129e6f8f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9e5ec6035d342c3a94d8a807fb98dae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_653ffd5badab448aafef1948d95fb626","IPY_MODEL_1736721f2a294f58bbe5073259bc057a","IPY_MODEL_19cf98e3dc2b4fc889e413251da9e8c4"],"layout":"IPY_MODEL_e5e5f024d981437e80376e634068bdf7"}},"653ffd5badab448aafef1948d95fb626":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d00857d23f844289232334fcfcd4726","placeholder":"​","style":"IPY_MODEL_c9f765a7839f4a93977c8fefe6964c88","value":"Downloading readme: 100%"}},"1736721f2a294f58bbe5073259bc057a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98ccee25ecf74aa981b692620b263bdc","max":3660,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a3202cb5bbd948728f15b99abe572e15","value":3660}},"19cf98e3dc2b4fc889e413251da9e8c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33f8337e51b347a99a0f9be6bc5c5499","placeholder":"​","style":"IPY_MODEL_48481a825e83439aa3a4716357a6390d","value":" 3.66k/3.66k [00:00&lt;00:00, 207kB/s]"}},"e5e5f024d981437e80376e634068bdf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d00857d23f844289232334fcfcd4726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f765a7839f4a93977c8fefe6964c88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98ccee25ecf74aa981b692620b263bdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3202cb5bbd948728f15b99abe572e15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33f8337e51b347a99a0f9be6bc5c5499":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48481a825e83439aa3a4716357a6390d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"123f1ae57ef74aa9a7d0e613781d64e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac9b9d2910a84e6fb76b80f53c42aca8","IPY_MODEL_6982f6ba098149be9d66d11030e85ce1","IPY_MODEL_698a9ff981164758bdd0c22565cdcc49"],"layout":"IPY_MODEL_be3d00a555344caba16d05ee68318453"}},"ac9b9d2910a84e6fb76b80f53c42aca8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7da6f2617b8454ab4a1f6a5b6c0edee","placeholder":"​","style":"IPY_MODEL_6ad13eed3ad54551a119502f9aff603d","value":"Downloading data: 100%"}},"6982f6ba098149be9d66d11030e85ce1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3eb3c459c5f4959a45197edd5598ae9","max":36745453,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8072d3184e8d4bdd8b9cc9c27d4d91b9","value":36745453}},"698a9ff981164758bdd0c22565cdcc49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d58e7af5613c495b8c46cc6c29465acb","placeholder":"​","style":"IPY_MODEL_e60aeb212ee64005af93cbc6de40d7de","value":" 36.7M/36.7M [00:03&lt;00:00, 14.1MB/s]"}},"be3d00a555344caba16d05ee68318453":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7da6f2617b8454ab4a1f6a5b6c0edee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ad13eed3ad54551a119502f9aff603d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3eb3c459c5f4959a45197edd5598ae9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8072d3184e8d4bdd8b9cc9c27d4d91b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d58e7af5613c495b8c46cc6c29465acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e60aeb212ee64005af93cbc6de40d7de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e45f5832a27b48fa9b4356ed052066f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_39f1e6dc36fd4619bea38070d090b5f8","IPY_MODEL_bcfd1ea33a5e41b39e8dd4d75799ff2a","IPY_MODEL_5ff9b6879a3c48f1a4ad86e2a56608be"],"layout":"IPY_MODEL_9230bb8090fe43fe988a8efee9017176"}},"39f1e6dc36fd4619bea38070d090b5f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0af34994dadb4abcb36b6d2d332934a2","placeholder":"​","style":"IPY_MODEL_08b0df96d6eb4256a096feb99e953a14","value":"Downloading data: 100%"}},"bcfd1ea33a5e41b39e8dd4d75799ff2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_264949875e554ec38dc217e7ebb359ce","max":13510573713,"min":0,"orientation":"horizontal","style":"IPY_MODEL_764a094175764ba889c2af27d9032570","value":13510573713}},"5ff9b6879a3c48f1a4ad86e2a56608be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_080f56a029d2415b84fd3f6a5ef34743","placeholder":"​","style":"IPY_MODEL_e43c0c784ef04a128d0a1db458a0fbf5","value":" 13.5G/13.5G [04:41&lt;00:00, 49.8MB/s]"}},"9230bb8090fe43fe988a8efee9017176":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0af34994dadb4abcb36b6d2d332934a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b0df96d6eb4256a096feb99e953a14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"264949875e554ec38dc217e7ebb359ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"764a094175764ba889c2af27d9032570":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"080f56a029d2415b84fd3f6a5ef34743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e43c0c784ef04a128d0a1db458a0fbf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3654c115e84e47f8a83918fb6d1a9537":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a100f487bcc14a53860db238b17edcb5","IPY_MODEL_923988799bce4a4f9fc375b71b08f318","IPY_MODEL_04ccedc6f8b64e4285b821f73645dd91"],"layout":"IPY_MODEL_4b3956775892485a91eee8214a58ea9c"}},"a100f487bcc14a53860db238b17edcb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_383e79bbb44643dab47ee0c6be929535","placeholder":"​","style":"IPY_MODEL_522ed2e25a514843b586409f55b6bde3","value":"Downloading data: 100%"}},"923988799bce4a4f9fc375b71b08f318":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf4a02ad17f04af4aaa94dea9b7619c9","max":6645013297,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37817c3a0408424293b8668c8eee222a","value":6645013297}},"04ccedc6f8b64e4285b821f73645dd91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b6341b492b14895a7a0ad1a0fb9c76c","placeholder":"​","style":"IPY_MODEL_7bb9d9a0a0524e6b80ad86d5d56511af","value":" 6.65G/6.65G [02:17&lt;00:00, 41.0MB/s]"}},"4b3956775892485a91eee8214a58ea9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"383e79bbb44643dab47ee0c6be929535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"522ed2e25a514843b586409f55b6bde3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf4a02ad17f04af4aaa94dea9b7619c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37817c3a0408424293b8668c8eee222a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b6341b492b14895a7a0ad1a0fb9c76c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bb9d9a0a0524e6b80ad86d5d56511af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9275a21b3124ccd9a82d5248c015748":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_112a72d73fa54c6999a4b757a3328626","IPY_MODEL_7ea122e5638d455e8db88c44c6915370","IPY_MODEL_d4a73b9fbda74563acbb86e7db7bb463"],"layout":"IPY_MODEL_da47c5d7c6174cd3b61e4c75cdcb3d89"}},"112a72d73fa54c6999a4b757a3328626":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2bc964a3dad4f1687b99746fc1796a9","placeholder":"​","style":"IPY_MODEL_ceb946485b594b678d0986f5d08a068e","value":"Generating train split: "}},"7ea122e5638d455e8db88c44c6915370":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_108023bb4a7a44b2882078edd9a4b76e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b991a2e947046c28e98cf3b43839826","value":1}},"d4a73b9fbda74563acbb86e7db7bb463":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b3ec580cc0f43db8955663d6ce26a95","placeholder":"​","style":"IPY_MODEL_9fdc3f9fa5e548d9bb45426d3f93c389","value":" 566747/0 [01:19&lt;00:00, 8623.53 examples/s]"}},"da47c5d7c6174cd3b61e4c75cdcb3d89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2bc964a3dad4f1687b99746fc1796a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceb946485b594b678d0986f5d08a068e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"108023bb4a7a44b2882078edd9a4b76e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9b991a2e947046c28e98cf3b43839826":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b3ec580cc0f43db8955663d6ce26a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fdc3f9fa5e548d9bb45426d3f93c389":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8b6743f9215483cbbdf2421867e95e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_495cc8efc5a24083bff1608c29f8665a","IPY_MODEL_18673a64563e42769959a693b6dde4b9","IPY_MODEL_7300d50e78914565b5098612074e3dfb"],"layout":"IPY_MODEL_4518a2197d2144978b376dc7041d42c5"}},"495cc8efc5a24083bff1608c29f8665a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f4047a01da4ad88f03c8d07b4ba2ed","placeholder":"​","style":"IPY_MODEL_993945a097ab40e482f9dc6ff821a82d","value":"Generating validation split: "}},"18673a64563e42769959a693b6dde4b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6befc9b2d534403f822f4668dc8166e0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ffb9e1cffa948799f9042bec7e5756f","value":1}},"7300d50e78914565b5098612074e3dfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cae388041b5d490682e603589fc888a1","placeholder":"​","style":"IPY_MODEL_334a3b95c852403ca97c88c9c3d7056c","value":" 25010/0 [00:07&lt;00:00, 9550.25 examples/s]"}},"4518a2197d2144978b376dc7041d42c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7f4047a01da4ad88f03c8d07b4ba2ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993945a097ab40e482f9dc6ff821a82d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6befc9b2d534403f822f4668dc8166e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6ffb9e1cffa948799f9042bec7e5756f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cae388041b5d490682e603589fc888a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"334a3b95c852403ca97c88c9c3d7056c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8f0273e015f4abb96c4bb5653514023":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4b64095aa7d44038b304e9a116ee093","IPY_MODEL_f7d874b62f6c4bc390807b80001715c2","IPY_MODEL_e9007caf9f50436686fbd4754dc56c87"],"layout":"IPY_MODEL_a1cdcdf6502d4247b55533845adc83a2"}},"d4b64095aa7d44038b304e9a116ee093":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeac42398fb64a6b9a3e957f39e2fed2","placeholder":"​","style":"IPY_MODEL_fa55347d18de494b8054603968270163","value":"Generating test split: "}},"f7d874b62f6c4bc390807b80001715c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a42f894e67947d9a203dc17346c8ff4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_275864e232a9491484971be25a7efa94","value":1}},"e9007caf9f50436686fbd4754dc56c87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_394f4f63399349a8ad80dcf71e5bfdd4","placeholder":"​","style":"IPY_MODEL_7fa5674d2aa14037bd881048a80f535e","value":" 25010/0 [00:09&lt;00:00, 9160.62 examples/s]"}},"a1cdcdf6502d4247b55533845adc83a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeac42398fb64a6b9a3e957f39e2fed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa55347d18de494b8054603968270163":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a42f894e67947d9a203dc17346c8ff4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"275864e232a9491484971be25a7efa94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"394f4f63399349a8ad80dcf71e5bfdd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fa5674d2aa14037bd881048a80f535e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb942e154e5943c39ad1040d4530c19f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20b667f856b94f929eaa60c3a85a2690","IPY_MODEL_1b1dab78a9154eb5803cd6e43bdad044","IPY_MODEL_ffe62f6a0831453fb080fb9d36f13eae"],"layout":"IPY_MODEL_14fd1d662e7b4541a65aac153bf7ffdd"}},"20b667f856b94f929eaa60c3a85a2690":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e87bafc9a6d49a78ba9c661cb84240a","placeholder":"​","style":"IPY_MODEL_90bca603b626480986cfded8984fc2ab","value":"Map: 100%"}},"1b1dab78a9154eb5803cd6e43bdad044":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4f2ef12db14629823da5eaa59ec0fd","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c059b9608f2b413ab52a3370246d0543","value":500}},"ffe62f6a0831453fb080fb9d36f13eae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3df069769d7496a8e28a214be67901f","placeholder":"​","style":"IPY_MODEL_27cff1d7716d456796f052fc38df5a02","value":" 500/500 [00:00&lt;00:00, 1749.45 examples/s]"}},"14fd1d662e7b4541a65aac153bf7ffdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e87bafc9a6d49a78ba9c661cb84240a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90bca603b626480986cfded8984fc2ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e4f2ef12db14629823da5eaa59ec0fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c059b9608f2b413ab52a3370246d0543":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b3df069769d7496a8e28a214be67901f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27cff1d7716d456796f052fc38df5a02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45dc44d0f3ce4da980366abbe6347336":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e21e1f8d18cd45d18b3839694a959aed","IPY_MODEL_7736485599794660a54bc01109ae2da5","IPY_MODEL_ec9858a72b804368b26f8d1f8e6b3ebe"],"layout":"IPY_MODEL_f47e1ea2895843a8bac7eae23228eb6d"}},"e21e1f8d18cd45d18b3839694a959aed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15f80c67b31a424e8b103905149f0cb5","placeholder":"​","style":"IPY_MODEL_55e3ebad3f6d4f83adf96c4825e62403","value":""}},"7736485599794660a54bc01109ae2da5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41bf3a03abbc4910a3c73103ac8afbee","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09611a63299f4c31aebeac53e1df1149","value":50}},"ec9858a72b804368b26f8d1f8e6b3ebe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2481cfabebd4d688483eac8a2c73500","placeholder":"​","style":"IPY_MODEL_ba9745e3c84643a99d0685262a735582","value":"epoch 1 iter 49  loss 0.9896 avg_loss 1.7624 lr 4.000000e-06: 100%|██████████| 50/50 [07:42&lt;00:00,  9.16s/it]"}},"f47e1ea2895843a8bac7eae23228eb6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15f80c67b31a424e8b103905149f0cb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55e3ebad3f6d4f83adf96c4825e62403":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41bf3a03abbc4910a3c73103ac8afbee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09611a63299f4c31aebeac53e1df1149":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a2481cfabebd4d688483eac8a2c73500":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba9745e3c84643a99d0685262a735582":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecd0c8678bc244bf9b6eeb8951e8add1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c67066dc9e534063915c23957abeca70","IPY_MODEL_d100b917533b4eefa48cf1847c933519","IPY_MODEL_58f06394d3fa4faebd7112f04c07a608"],"layout":"IPY_MODEL_b53cd59285a741c086dcc233bc80e86a"}},"c67066dc9e534063915c23957abeca70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c67daa2d614f46c3abb5ff2b002d9f35","placeholder":"​","style":"IPY_MODEL_2025b6b4dbcc4b94aa60dd161eaa50cc","value":""}},"d100b917533b4eefa48cf1847c933519":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2d66bd63edb4d47b4f46d77e15f6127","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93a67977fc5c4d3f8dfdffbbbaeb0a27","value":50}},"58f06394d3fa4faebd7112f04c07a608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed149fdfd1454ec88047804947e64340","placeholder":"​","style":"IPY_MODEL_357944d01733491bb10f81081b297f93","value":"epoch 2 iter 49  loss 0.8106 avg_loss 1.8389 lr 4.000000e-06: 100%|██████████| 50/50 [07:46&lt;00:00,  9.22s/it]"}},"b53cd59285a741c086dcc233bc80e86a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c67daa2d614f46c3abb5ff2b002d9f35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2025b6b4dbcc4b94aa60dd161eaa50cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2d66bd63edb4d47b4f46d77e15f6127":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a67977fc5c4d3f8dfdffbbbaeb0a27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed149fdfd1454ec88047804947e64340":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357944d01733491bb10f81081b297f93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c61a8b4dee364641b7aa223fad38f1b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_557d9c29ddef4ef2b411ee1231879c64","IPY_MODEL_f91c03fc34404fcb812dd48354292233","IPY_MODEL_bb469bb352b24534ad6cd2361d6231ef"],"layout":"IPY_MODEL_4b0d3947b6994c5287282b472765e2d0"}},"557d9c29ddef4ef2b411ee1231879c64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecbedbb6fcd346169583c229329167b8","placeholder":"​","style":"IPY_MODEL_857e5733094d4ff082afb49c9dbda56e","value":""}},"f91c03fc34404fcb812dd48354292233":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d4f211c3d054b84bb0ff845a0669570","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9df567c4f3de4dab9e2339abf6809c67","value":50}},"bb469bb352b24534ad6cd2361d6231ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6582e0cd6e7b456da1bce9416034d426","placeholder":"​","style":"IPY_MODEL_58f5c764c12542a5a59e17c0c3a7187d","value":"epoch 3 iter 49  loss 1.2315 avg_loss 1.7888 lr 4.000000e-06: 100%|██████████| 50/50 [07:46&lt;00:00,  9.09s/it]"}},"4b0d3947b6994c5287282b472765e2d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecbedbb6fcd346169583c229329167b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"857e5733094d4ff082afb49c9dbda56e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d4f211c3d054b84bb0ff845a0669570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9df567c4f3de4dab9e2339abf6809c67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6582e0cd6e7b456da1bce9416034d426":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58f5c764c12542a5a59e17c0c3a7187d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6b298890d76496f830e0c34d2015424":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87100aa35b654effa2a6cc885f129522","IPY_MODEL_19c0fb0f3cc64faa98410f834558baa7","IPY_MODEL_084e8e54a6564c26b116770230f4cceb"],"layout":"IPY_MODEL_5f7d4cbbe8f4432797f76bb5f0cf056d"}},"87100aa35b654effa2a6cc885f129522":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_187dd07f73084775b22a575667403b32","placeholder":"​","style":"IPY_MODEL_c2fa53209a984e999d103a870fca8d66","value":""}},"19c0fb0f3cc64faa98410f834558baa7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e1c807631cc48d5a18a2b686ebc2a59","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d23da13841f47d782123c100d7881e5","value":50}},"084e8e54a6564c26b116770230f4cceb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7a6ce9bdc924147b98f129961799001","placeholder":"​","style":"IPY_MODEL_6e87459d264b45bab2ecd2869e1bbaf7","value":"epoch 4 iter 49  loss 1.1602 avg_loss 1.7535 lr 4.000000e-06: 100%|██████████| 50/50 [08:08&lt;00:00, 10.01s/it]"}},"5f7d4cbbe8f4432797f76bb5f0cf056d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"187dd07f73084775b22a575667403b32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2fa53209a984e999d103a870fca8d66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e1c807631cc48d5a18a2b686ebc2a59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d23da13841f47d782123c100d7881e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7a6ce9bdc924147b98f129961799001":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e87459d264b45bab2ecd2869e1bbaf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"273a3acd0c5646a0aab67b93576dd972":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9aca25aac2174116bacedb0c6df6170b","IPY_MODEL_9886f5867376407ba6654daabe9178ba","IPY_MODEL_1f9054a7e00f44eabc9193db1c6c9372"],"layout":"IPY_MODEL_4b83e93ea2cf4d53a4dadd3f06039449"}},"9aca25aac2174116bacedb0c6df6170b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7c0ea1e091450db55cd81b2ada89a2","placeholder":"​","style":"IPY_MODEL_51ad7ba08a5a48c8a5428665be15ae74","value":""}},"9886f5867376407ba6654daabe9178ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c377c62748904a658e59d309ed78720d","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a74213497ff24f658440bcc1c028782b","value":50}},"1f9054a7e00f44eabc9193db1c6c9372":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_441d55e03a024735b5fbd2fbcf1cab52","placeholder":"​","style":"IPY_MODEL_a289f3923846444d83c9ae784894836a","value":"epoch 5 iter 49  loss 0.8002 avg_loss 1.7525 lr 4.000000e-06: 100%|██████████| 50/50 [07:51&lt;00:00, 10.13s/it]"}},"4b83e93ea2cf4d53a4dadd3f06039449":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7c0ea1e091450db55cd81b2ada89a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51ad7ba08a5a48c8a5428665be15ae74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c377c62748904a658e59d309ed78720d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a74213497ff24f658440bcc1c028782b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"441d55e03a024735b5fbd2fbcf1cab52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a289f3923846444d83c9ae784894836a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb36af40ffe949a4a9e958c894419920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5d99905e68bd40c8b550007674e8e083","IPY_MODEL_55f9bca19212483595c80f2f143d4eec","IPY_MODEL_0a26725d71d94ee3aea3b4e31b12bb1c"],"layout":"IPY_MODEL_d9a3910701ca4ef99cd1d3a8e18e772e"}},"5d99905e68bd40c8b550007674e8e083":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b8e5332d8064556af41198d3dc64426","placeholder":"​","style":"IPY_MODEL_a4b517d070dd4c099cc99f276541985d","value":""}},"55f9bca19212483595c80f2f143d4eec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82daa05d29124cb2909634838caa8ffe","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_039bebaf2d924981ad37bb638ea6c434","value":50}},"0a26725d71d94ee3aea3b4e31b12bb1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a6337198d514ec3a0c95cc44b57e423","placeholder":"​","style":"IPY_MODEL_c0181c6c11474887ae25b6612e9a5e6c","value":"epoch 6 iter 49  loss 0.9466 avg_loss 1.7782 lr 4.000000e-06: 100%|██████████| 50/50 [07:49&lt;00:00, 10.24s/it]"}},"d9a3910701ca4ef99cd1d3a8e18e772e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b8e5332d8064556af41198d3dc64426":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4b517d070dd4c099cc99f276541985d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82daa05d29124cb2909634838caa8ffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"039bebaf2d924981ad37bb638ea6c434":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a6337198d514ec3a0c95cc44b57e423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0181c6c11474887ae25b6612e9a5e6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f68fc80632b4ab49f31a1dd7ed2a087":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ef74f27c984405bab29e719258bc2b5","IPY_MODEL_15903c14cc05445b8abd180d7ffffc44","IPY_MODEL_c2eaad4b360f4ae381b5a4f559d2561f"],"layout":"IPY_MODEL_9e53314503c24c3d9c42fe0a25fc764b"}},"7ef74f27c984405bab29e719258bc2b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45c0552d0a1f48898e44046fbd3c2483","placeholder":"​","style":"IPY_MODEL_040968139aff4205a32cd5028f9fb33f","value":""}},"15903c14cc05445b8abd180d7ffffc44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_075552cc01394f3bb3ead9b01e396cb6","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_132a471dac3c412996cd70fdf3cc0a85","value":50}},"c2eaad4b360f4ae381b5a4f559d2561f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd7265e442d541ee98dd2e545153b1f0","placeholder":"​","style":"IPY_MODEL_5ba9aac592cb44ab8a26b6dd82394812","value":"epoch 7 iter 49  loss 0.8667 avg_loss 1.7577 lr 4.000000e-06: 100%|██████████| 50/50 [08:15&lt;00:00,  9.54s/it]"}},"9e53314503c24c3d9c42fe0a25fc764b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45c0552d0a1f48898e44046fbd3c2483":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"040968139aff4205a32cd5028f9fb33f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"075552cc01394f3bb3ead9b01e396cb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"132a471dac3c412996cd70fdf3cc0a85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd7265e442d541ee98dd2e545153b1f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ba9aac592cb44ab8a26b6dd82394812":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"896bf2afee214f4fbdca25593c21351b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8bd38abb9654913a42a54726e32c049","IPY_MODEL_f002c1d1e2f049aeb598964b1c0b42f3","IPY_MODEL_d582dd5645524c69ad25fa46ab51ba4d"],"layout":"IPY_MODEL_9bb588e34fbb4723a55b47a6c86c4784"}},"b8bd38abb9654913a42a54726e32c049":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ec741b2a420407eab2c282b61694332","placeholder":"​","style":"IPY_MODEL_c84b713679db4b649b167159c3d66324","value":""}},"f002c1d1e2f049aeb598964b1c0b42f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0be30d5489354c96bcbc1f10ca559034","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b56ae2f43814e4594f58375a6c2a1de","value":50}},"d582dd5645524c69ad25fa46ab51ba4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3532933887f948d28b308066390e7eb7","placeholder":"​","style":"IPY_MODEL_0ec043f7cbaa4b2281682d58e06dd0ba","value":"epoch 8 iter 49  loss 1.1569 avg_loss 1.7442 lr 4.000000e-06: 100%|██████████| 50/50 [07:55&lt;00:00,  9.95s/it]"}},"9bb588e34fbb4723a55b47a6c86c4784":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ec741b2a420407eab2c282b61694332":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c84b713679db4b649b167159c3d66324":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0be30d5489354c96bcbc1f10ca559034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b56ae2f43814e4594f58375a6c2a1de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3532933887f948d28b308066390e7eb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ec043f7cbaa4b2281682d58e06dd0ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26fb5d3beabd403c88ba327178f7118b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00935838e3bd4d86a75dc74fae18ae85","IPY_MODEL_a03657dfbbf441eaa8fff9515b4ffe56","IPY_MODEL_8af5edae08cc4d7f8375fd40c7d192b7"],"layout":"IPY_MODEL_d2e3932bd0f74296abeee15d75e3f8ef"}},"00935838e3bd4d86a75dc74fae18ae85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_118c3d831af14d3eb17be37f2bf5c632","placeholder":"​","style":"IPY_MODEL_9c086e857d6941bd8caa91ac482a61fe","value":""}},"a03657dfbbf441eaa8fff9515b4ffe56":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ece8ff2c54d24cbaa00e621102999af9","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19f3b797cb84428c993e176d82c71b38","value":50}},"8af5edae08cc4d7f8375fd40c7d192b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79c4759a1e064bafab0eb4150757dd05","placeholder":"​","style":"IPY_MODEL_b3163d5890d1494187f24e5ce0abcd7e","value":"epoch 9 iter 49  loss 0.6850 avg_loss 1.7359 lr 4.000000e-06: 100%|██████████| 50/50 [07:54&lt;00:00,  9.27s/it]"}},"d2e3932bd0f74296abeee15d75e3f8ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"118c3d831af14d3eb17be37f2bf5c632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c086e857d6941bd8caa91ac482a61fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ece8ff2c54d24cbaa00e621102999af9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19f3b797cb84428c993e176d82c71b38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79c4759a1e064bafab0eb4150757dd05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3163d5890d1494187f24e5ce0abcd7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79fada2849724ca28271048272a7012f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f1da6a1691042e895f2ac040bee1b66","IPY_MODEL_52171eee348e44eb9cd4cae2fe07212c","IPY_MODEL_54ab86e54dc34b479a818e9805b11cae"],"layout":"IPY_MODEL_309eee4d22984280ba8b398114c5ccf6"}},"7f1da6a1691042e895f2ac040bee1b66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86823a987d9e49d3a4df91590eb1698e","placeholder":"​","style":"IPY_MODEL_4ad3dd3ba06b4873ba4e506df7c18ca6","value":""}},"52171eee348e44eb9cd4cae2fe07212c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af1a0be9484044c499ee226194e07159","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65b683a54b154bea9b4e61065cc18781","value":50}},"54ab86e54dc34b479a818e9805b11cae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6819bf2018bc49dc946b6de4ef0c2eb2","placeholder":"​","style":"IPY_MODEL_63dfcda5200b4acc81ac34461ec535ff","value":"epoch 10 iter 49  loss 1.1835 avg_loss 1.7466 lr 4.000000e-06: 100%|██████████| 50/50 [07:59&lt;00:00,  9.61s/it]"}},"309eee4d22984280ba8b398114c5ccf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86823a987d9e49d3a4df91590eb1698e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ad3dd3ba06b4873ba4e506df7c18ca6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af1a0be9484044c499ee226194e07159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65b683a54b154bea9b4e61065cc18781":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6819bf2018bc49dc946b6de4ef0c2eb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63dfcda5200b4acc81ac34461ec535ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}